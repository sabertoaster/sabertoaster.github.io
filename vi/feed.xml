<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sabertoaster.github.io/vi/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sabertoaster.github.io/vi/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-31T19:22:03+00:00</updated><id>https://sabertoaster.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry xml:lang="en"><title type="html">Learning Resources</title><link href="https://sabertoaster.github.io/vi/blog/2025/awesome-learning-path/" rel="alternate" type="text/html" title="Learning Resources"/><published>2025-07-01T12:00:00+00:00</published><updated>2025-07-01T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2025/awesome-learning-path</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2025/awesome-learning-path/"><![CDATA[<blockquote> <p><em>An attempt of an undergrad to create a curated repository of computational neuroscience and brain-inspired artificial intelligence resources, which can also including Data Science, pure Math, Machine Learning, Deep Learning, and not constrictively limiting beyond those topics. Leave a star if you believe I can make it, or follow me if you don’t.</em></p> </blockquote> <div style="float: right"> SaberToaster, 2025 </div> <p><br/></p> <h2 id="learning-resources">Learning Resources</h2> <details><summary>MOOCs &amp; Online Courses</summary> <table> <thead> <tr> <th>Course</th> <th>Provider/Author</th> <th>Focus</th> <th>Best For</th> <th>Access</th> </tr> </thead> <tbody> <tr> <td><strong>Computational Neuroscience</strong></td> <td>Coursera</td> <td>Mathematical foundations of neural computation</td> <td>CS students transitioning to neuro</td> <td><a href="https://www.coursera.org/learn/computational-neuroscience">Course</a></td> </tr> <tr> <td><strong>Neuroscience for Machine Learners</strong></td> <td>Neuromatch Academy</td> <td>CS-friendly intro to neuroscience</td> <td>ML practitioners</td> <td><a href="https://neuro4ml.github.io/">Course</a> • <a href="https://youtube.com/playlist?list=PL09WqqDbQWHErc8xOyWdKpNEk78Jjk0EL">Videos</a></td> </tr> <tr> <td><strong>Machine Learning Specialization</strong></td> <td>Coursera</td> <td>Standard ML foundations</td> <td>Beginners</td> <td><a href="https://www.coursera.org/specializations/machine-learning-introduction">Course</a></td> </tr> <tr> <td><strong>Deep Learning Specialization</strong></td> <td>Coursera</td> <td>Neural networks and architectures</td> <td>Intermediate practitioners</td> <td><a href="https://www.coursera.org/specializations/deep-learning">Course</a></td> </tr> <tr> <td><strong>Predictive Brain Lab Resources</strong></td> <td>MIT</td> <td>Heavy NeuroAI focus</td> <td>Advanced researchers</td> <td><a href="https://predictive-brain-lab.github.io/Lab-Wiki/documents/neuro-ai/neuroai-resources.html">Resources</a></td> </tr> <tr> <td><strong>Computational Neuroscience 2020</strong></td> <td>Michelle R. Greene, Ph.D</td> <td>Self-paced neuroscience curriculum</td> <td>Self-directed learners</td> <td><a href="https://mrgreene09.github.io/compNeuro2020/">Course</a></td> </tr> </tbody> </table> </details> <details><summary>Essential Books &amp; Textbooks</summary> <h5 id="mathematical-foundations">Mathematical Foundations</h5> <table> <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>Focus</th> <th>Best For</th> <th>Access</th> </tr> </thead> <tbody> <tr> <td><strong>Neuronal Dynamics</strong></td> <td>Wulfram Gerstner et al</td> <td>2014</td> <td>Mathematical models of neural dynamics</td> <td>Physics background</td> <td><a href="https://neuronaldynamics.epfl.ch/">Free Online</a></td> </tr> <tr> <td><strong>Theoretical Neuroscience</strong></td> <td>Dayan &amp; Abbott</td> <td>2001</td> <td>Mathematical framework for computation</td> <td>CS + Physics students</td> <td>Essential textbook</td> </tr> <tr> <td><strong>Dynamical Systems in Neuroscience</strong></td> <td>Izhikevich</td> <td>2007</td> <td>Mathematical models of neuronal behavior</td> <td>Advanced mathematical focus</td> <td>Standard reference</td> </tr> <tr> <td><strong>Neural Engineering</strong></td> <td>Eliasmith &amp; Anderson</td> <td>2003</td> <td>Neural representation principles (NEF)</td> <td>Engineering approach</td> <td>NEF methodology foundation</td> </tr> </tbody> </table> <h5 id="aiml-references">AI/ML References</h5> <table> <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>Focus</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><strong>Deep Learning</strong></td> <td>Goodfellow et al.</td> <td>2016</td> <td>Comprehensive DL textbook</td> <td>Standard reference</td> </tr> <tr> <td><strong>Attention Is All You Need</strong></td> <td>Vaswani et al.</td> <td>2017</td> <td>Transformer architecture</td> <td>Revolutionary paper</td> </tr> </tbody> </table> </details> <details><summary>Blogs &amp; Personal Resources</summary> <table> <thead> <tr> <th>Author</th> <th>Affiliation</th> <th>Focus</th> <th>Why Follow</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>Christopher Olah</strong></td> <td>Anthropic AI co-founder</td> <td>Neural network interpretability</td> <td>Clear explanations of complex concepts</td> <td><a href="https://colah.github.io/">Blog</a></td> </tr> <tr> <td><strong>Aman Chadha</strong></td> <td>AWS GenAI Chief Research Scientist</td> <td>AI research and applications</td> <td>Industry perspective on cutting-edge research</td> <td><a href="https://aman.ai/">Homepage</a></td> </tr> <tr> <td><strong>Charles Frye</strong></td> <td>Helen Wills Neuroscience Institute</td> <td>Computational neuroscience</td> <td>Academic insights bridging theory and practice</td> <td><a href="https://charlesfrye.github.io/about/">Homepage</a> • <a href="https://charlesfrye.github.io/FoundationalNeuroscience/">CNS</a></td> </tr> </tbody> </table> </details> <details><summary>YouTube Channels</summary> <table> <thead> <tr> <th>Channel</th> <th>Creator</th> <th>Focus</th> <th>Why Watch</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>3Blue1Brown</strong></td> <td>Grant Sanderson</td> <td>Mathematical visualizations</td> <td>Best math/ML concept explanations</td> <td><a href="https://www.youtube.com/@3blue1brown">Channel</a></td> </tr> <tr> <td><strong>Artem Kirsanov</strong></td> <td>Artem Kirsanov</td> <td>Computational neuroscience animations</td> <td>Covers most CNS concepts with clear animations</td> <td><a href="https://www.youtube.com/@ArtemKirsanov">Channel</a></td> </tr> <tr> <td><strong>Deepia</strong></td> <td>Various</td> <td>AI/ML visualizations</td> <td>Cool visualization techniques</td> <td><a href="https://www.youtube.com/@Deepia-ls2fo">Channel</a></td> </tr> <tr> <td><strong>Computerphile</strong></td> <td>University team</td> <td>Computer science concepts</td> <td>CS fundamentals relevant to neurocomputation</td> <td><a href="https://www.youtube.com/@Computerphile">Channel</a></td> </tr> </tbody> </table> </details> <hr/> <h2 id="research-literature">Research Literature</h2> <details><summary>Mathematical &amp; Computational Foundations</summary> <table> <thead> <tr> <th>Paper/Book</th> <th>Authors</th> <th>Year</th> <th>Key Contribution</th> <th>Impact</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>What the Frog’s Eye Tells the Frog’s Brain</strong></td> <td>Lettvin et al.</td> <td>1959</td> <td>Feature detection in visual system</td> <td>Classic computational neuro</td> <td> </td> </tr> <tr> <td><strong>Computational neuroscience: a frontier</strong></td> <td>Multiple</td> <td>2020</td> <td>State of the field overview</td> <td>Recent comprehensive review</td> <td><a href="https://academic.oup.com/nsr/article/7/9/1418/5856589">PDF</a></td> </tr> </tbody> </table> <p><em>Note: Mathematical foundations table to be expanded with specific papers on neural dynamics, information theory, and computational methods.</em></p> </details> <details><summary>NeuroAI Integration Papers</summary> <table> <thead> <tr> <th>Paper</th> <th>Authors</th> <th>Year</th> <th>Key Contribution</th> <th>Research Area</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><strong>The Genomic Bottleneck</strong></td> <td>Zador</td> <td>2019</td> <td>Constraints on innate vs learned computation</td> <td>Evolutionary computation</td> <td>Influential perspective paper</td> </tr> <tr> <td><strong>Brain-Inspired AI</strong></td> <td>Hassabis et al.</td> <td>2017</td> <td>Neuroscience-AI bidirectional influence</td> <td>AI methodology</td> <td>DeepMind perspective</td> </tr> <tr> <td><strong>Surrogate Gradient Learning in SNN</strong></td> <td>Neftci et al.</td> <td>2019</td> <td>Training spiking neural networks</td> <td>SNN algorithms</td> <td>Key training methodology</td> </tr> <tr> <td><strong>Deep Learning with Spiking Neurons</strong></td> <td>Pfeiffer &amp; Pfeil</td> <td>2018</td> <td>Neuromorphic deep learning approaches</td> <td>Hardware-software co-design</td> <td>Practical implementation focus</td> </tr> <tr> <td><strong>Towards Spike-Based Machine Intelligence</strong></td> <td>Roy et al.</td> <td>2019</td> <td>Comprehensive SNN survey</td> <td>SNN overview</td> <td>State-of-the-art review</td> </tr> </tbody> </table> </details> <details><summary>Specialized Topics</summary> <h5 id="spiking-neural-networks">Spiking Neural Networks</h5> <p><em>Papers on temporal dynamics, energy efficiency, and biological realism</em></p> <h5 id="hopfield-networks">Hopfield Networks</h5> <p><em>Classic associative memory models and modern variants</em></p> <h5 id="attention-mechanisms">Attention Mechanisms</h5> <p><em>Biological inspiration and computational implementations</em></p> <p><em>These sections to be populated with specific paper recommendations</em></p> </details> <hr/> <h2 id="academic--research-community">Academic &amp; Research Community</h2> <details><summary>Research Institutions &amp; Labs</summary> <h5 id="north-america">North America</h5> <table> <thead> <tr> <th>Institution</th> <th>Key Researchers</th> <th>Research Focus</th> <th>Location</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>MIT McGovern Institute</strong></td> <td>Multiple PIs</td> <td>Computational neuroscience</td> <td>Cambridge, MA</td> <td> </td> </tr> <tr> <td><strong>Stanford Wu Tsai</strong></td> <td>Multiple PIs</td> <td>Neural computation and AI</td> <td>Stanford, CA</td> <td> </td> </tr> <tr> <td><strong>Allen Institute</strong></td> <td>Christof Koch</td> <td>Large-scale brain mapping</td> <td>Seattle, WA</td> <td> </td> </tr> <tr> <td><strong>HHMI Janelia</strong></td> <td>Multiple PIs</td> <td>Neural circuits and computation</td> <td>Ashburn, VA</td> <td> </td> </tr> <tr> <td><strong>Cold Spring Harbor Laboratory</strong></td> <td>Multiple PIs</td> <td>NeuroAI internships + postdocs</td> <td>NYC, NY</td> <td><a href="https://www.cshl.edu/research/neuroscience/neuroai/">NeuroAI Program</a></td> </tr> </tbody> </table> <h5 id="europe">Europe</h5> <table> <thead> <tr> <th>Institution</th> <th>Key Researchers</th> <th>Research Focus</th> <th>Location</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>EPFL (Computational Neuroscience)</strong></td> <td>Wulfram Gerstner</td> <td>Mathematical neuroscience</td> <td>Switzerland</td> <td><a href="https://www.epfl.ch/labs/lcn/">LCN Lab</a></td> </tr> <tr> <td><strong>EPFL (NeuroAI)</strong></td> <td>Martin Schrimpf</td> <td>Brain-AI alignment</td> <td>Switzerland</td> <td><a href="https://www.epfl.ch/labs/schrimpflab/">Schrimpf Lab</a></td> </tr> <tr> <td><strong>ETH Zurich INI</strong></td> <td>Multiple PIs</td> <td>Neuromorphic engineering</td> <td>Switzerland</td> <td> </td> </tr> <tr> <td><strong>DeepMind Neuroscience</strong></td> <td>Multiple researchers</td> <td>Brain-inspired AI</td> <td>London, UK</td> <td> </td> </tr> <tr> <td><strong>IT:U</strong></td> <td>Jie Mei</td> <td>Neuromodulatory mechanisms</td> <td>Austria</td> <td><a href="https://it-u.at/en/research/research-groups/computational-neuroscience/">Computational Neuroscience</a></td> </tr> </tbody> </table> <h5 id="asia">Asia</h5> <table> <thead> <tr> <th>Institution</th> <th>Key Researchers</th> <th>Research Focus</th> <th>Location</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>KWANGWOON University</strong></td> <td>Young-Seok Choi</td> <td>Neuroengineering and AI</td> <td>Korea</td> <td><a href="https://sites.google.com/view/neuroailab/home">NeuroAI Lab</a></td> </tr> <tr> <td><strong>Shanghai Jiao Tong University</strong></td> <td>Ru-Yuan Zhang</td> <td>Cognitive computational neuroscience</td> <td>Shanghai, China</td> <td><a href="https://ruyuanzhang.github.io/index.html">Zhang Lab</a></td> </tr> </tbody> </table> </details> <details><summary>Journals &amp; Conferences</summary> <h5 id="high-impact-journals">High-Impact Journals</h5> <table> <thead> <tr> <th>Journal</th> <th>Type</th> <th>Focus</th> <th>Impact Factor</th> <th>Submission Focus</th> </tr> </thead> <tbody> <tr> <td><strong>Nature Neuroscience</strong></td> <td>Journal</td> <td>High-impact neuroscience research</td> <td>&gt;20</td> <td>Breakthrough discoveries</td> </tr> <tr> <td><strong>Neuron</strong></td> <td>Journal</td> <td>Cellular and systems neuroscience</td> <td>&gt;15</td> <td>Mechanistic insights</td> </tr> <tr> <td><strong>Neural Computation</strong></td> <td>Journal</td> <td>Computational theory</td> <td>~3</td> <td>Mathematical models</td> </tr> </tbody> </table> <h5 id="open-access-venues">Open-Access Venues</h5> <table> <thead> <tr> <th>Venue</th> <th>Type</th> <th>Focus</th> <th>Impact Factor</th> <th>Why Submit Here</th> </tr> </thead> <tbody> <tr> <td><strong>PLoS Computational Biology</strong></td> <td>Journal</td> <td>Computational biology</td> <td>~4</td> <td>Open access, broad reach</td> </tr> <tr> <td><strong>Frontiers in Computational Neuroscience</strong></td> <td>Journal</td> <td>Computational approaches</td> <td>~3</td> <td>Fast review, open access</td> </tr> </tbody> </table> <h5 id="key-conferences">Key Conferences</h5> <table> <thead> <tr> <th>Conference</th> <th>Full Name</th> <th>Focus</th> <th>Venue Type</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><strong>NeurIPS</strong></td> <td>Neural Information Processing Systems</td> <td>ML/AI with neuroscience connections</td> <td>Annual</td> <td>Premier ML conference</td> </tr> <tr> <td><strong>COSYNE</strong></td> <td>Computational and Systems Neuroscience</td> <td>Computational neuroscience</td> <td>Annual</td> <td>Theory-focused</td> </tr> <tr> <td><strong>ICLR</strong></td> <td>International Conference on Learning Representations</td> <td>Representation learning</td> <td>Annual</td> <td>Rising importance in AI</td> </tr> <tr> <td><strong>CNS</strong></td> <td>Cognitive Neuroscience Society</td> <td>Cognitive neuroscience</td> <td>Annual</td> <td>Experimental focus</td> </tr> <tr> <td><strong>ICMNAI</strong></td> <td>International Conference on Mathematics of Neuroscience and AI</td> <td>Mathematics intersection</td> <td>Annual</td> <td><a href="https://www.neuromonster.org/contact">Website</a></td> </tr> </tbody> </table> </details> <hr/> <h2 id="computational-tools--software">Computational Tools &amp; Software</h2> <details><summary>Neural Simulation Platforms</summary> <table> <thead> <tr> <th>Tool</th> <th>Category</th> <th>Description</th> <th>Language</th> <th>Best For</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>Brian2</strong></td> <td>SNN Simulation</td> <td>Clock-driven neural network simulator</td> <td>Python</td> <td>Research, education</td> <td> </td> </tr> <tr> <td><strong>NEURON</strong></td> <td>Detailed Modeling</td> <td>Biophysically detailed neuron models</td> <td>Python/C++</td> <td>Detailed biophysical models</td> <td> </td> </tr> <tr> <td><strong>STEPS</strong></td> <td>Spatial Modeling</td> <td>Spatial stochastic simulation</td> <td>Python/C++</td> <td>Subcellular processes</td> <td> </td> </tr> </tbody> </table> </details> <details><summary>Analysis &amp; Visualization Tools</summary> <table> <thead> <tr> <th>Tool</th> <th>Category</th> <th>Description</th> <th>Language</th> <th>Best For</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>DeepLabCut</strong></td> <td>Behavioral Analysis</td> <td>Markerless pose estimation</td> <td>Python</td> <td>Animal behavior tracking</td> <td> </td> </tr> <tr> <td><strong>PyTorch</strong></td> <td>Deep Learning</td> <td>Research-focused ML framework</td> <td>Python</td> <td>NeuroAI model development</td> <td> </td> </tr> <tr> <td><strong>TensorFlow</strong></td> <td>Deep Learning</td> <td>Production-focused ML framework</td> <td>Python</td> <td>Deployment and large-scale apps</td> <td> </td> </tr> </tbody> </table> </details> <hr/> <p><em>This resource hub is community-maintained. Feel free to contribute through PRs or suggestions for additional resources, corrections, or organizational improvements.</em></p>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="comp-neursci"/><category term="math"/><summary type="html"><![CDATA[AI, Math and Neuroscience]]></summary></entry><entry xml:lang="en"><title type="html">Harvard CS197 AI Research Experiences</title><link href="https://sabertoaster.github.io/vi/blog/2025/harvard-cs197/" rel="alternate" type="text/html" title="Harvard CS197 AI Research Experiences"/><published>2025-07-01T12:00:00+00:00</published><updated>2025-07-01T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2025/harvard-cs197</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2025/harvard-cs197/"><![CDATA[<h2 id="notation-convention">Notation Convention:</h2> <blockquote> <p>This is a quote from the course or other references.</p> </blockquote> <p><em><span style="color:red">This is an important highlight.</span></em></p> <h2 id="brief-introduction">Brief introduction</h2> <p>This is a course ran by Harvard, instructed by Professor <a href="https://pranavrajpurkar.com/">Pranav Rajpurkar</a>.</p> <blockquote> <p>Dive into cutting-edge development tools like PyTorch, Lightning, and Hugging Face, and streamline your workflow with VSCode, Git, and Conda. You’ll learn how to harness the power of the cloud with AWS and Colab to train massive deep learning models with lightning-fast GPU acceleration. Plus, you’ll master best practices for managing a large number of experiments with Weights and Biases. And that’s just the beginning! This course will also teach you how to systematically read research papers, generate new ideas, and present them in slides or papers. You’ll even learn valuable project management and team communication techniques used by top AI researchers. Don’t miss out on this opportunity to level up your AI skills.</p> </blockquote> <hr/> <h2 id="chapter-1--2-introduction-to-ai-and-setting-up-environment">Chapter <a href="https://docs.google.com/document/u/0/d/1FHnGGGhTTarovEAVzSfELlNvxhXFJV4DkpuGgMKaEGw/edit">1</a> &amp; <a href="https://docs.google.com/document/u/0/d/1z5ELxpTw_U01jUB6-D6ILqHRPg6SSiLE7VFQryH3LPU/edit">2</a>: Introduction to AI and Setting up environment</h2> <p>Typical, intuitive guides to interact with AI and its research + development. Working environment requires source-code version control (Git), environment control (Conda) and editors (VSCode).</p> <hr/> <h2 id="chapter-3-reading-ai-research-papers"><a href="https://docs.google.com/document/d/1bPhwNdCCKkm1_adD0rx1YV6r2JG98qYmTxutT5gdAdQ/edit?tab=t.0">Chapter 3: Reading AI Research Papers</a></h2> <details><summary>TLDR: Read wide for learning, read deep for improving. Take incremental approach.</summary> <h3 id="ch3-objectives"><u>Objectives</u>:</h3> <ul> <li>Conduct a literature search to identify papers relevant to a topic of interest</li> <li>Read a machine learning research paper and summarize its contributions</li> </ul> <h3 id="ch3-content"><u>Chapter's content</u>:</h3> <blockquote> <p>I’m going to break down the process of reading AI research papers into two pieces: reading wide, and reading deep. <em><span style="color:red">When you start learning about a new topic, you typically get more out of reading wide</span></em>: this means navigating through literature reading small amounts of individual research papers. Our goal when reading wide is to build and improve our mental model of a research topic. <em><span style="color:red">Once you have identified key works that you want to understand well in the first step, you will want to read deep</span></em>: here, you are trying to read individual papers in depth. Both reading wide and deep are necessary and complimentary, especially when you’re getting started.</p> </blockquote> <h4 id="read-wide">Read wide:</h4> <p>Paperswithcode (AI/ML) | Google scholar | ACM Digital Lib &gt; Taking notes &gt; Check benchmark with recent submissions &gt; Check SOTA &gt; Check dataset (in any order) &gt; Read em and take notes. <br/> Results: Notes condensed with information, may take up to 2-3 hours doing this.</p> <blockquote> <p>At this point, I find myself particularly intrigued by the SOTA methods: Why are they achieving high performance? According to the review paper, it looks like “training strategies using pre-training” have been an advance. Maybe that’s worth keeping an eye out for!</p> </blockquote> <h4 id="read-deep">Read deep:</h4> <blockquote> <p>So I would like you to take an incremental approach here. Understand that, in your first pass, you will not understand more than 10% of the research paper. The paper may require us to read another more fundamental paper (which might require reading a third paper and so on; it could be turtles all the way down)! Then, in your second pass, you might understand 20% of the paper. Understanding 100% of a paper might require a significant leap, maybe because it’s poorly written, insufficiently detailed, or simply too technically/mathematically advanced. We thus want to aim to build up to understanding as much of the paper as possible – I’ll bet that 70-80% of the paper is a good target.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec3-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec3-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025/07/harvard-cs197-lec3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Define your own highlight strategy, for example:</p> <ul> <li>The <span style="color:yellow">yellow highlights</span> are the problems/challenges</li> <li>The <span style="color:pink">pink highlights</span> are the solutions to the challenges</li> <li>The <span style="color:orange">orange highlights</span> are the main contributions of the work we’re reading.</li> </ul> <h3 id="ch3-tips"><u>Some useful tips</u>:</h3> <ul> <li> <p>Read deep: do a <a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">three-pass approach</a>. <br/><br/> First, take a quick scan through the title, put some brief thought into it. Read the abstract, introduction, section headings. <strong>Identify the research question, the thematics, the papers’ contribution.</strong> Look through the figures for visual details. Glance through <strong>discussion</strong> if exists. Read the conclusion and see if the research question have been solved (?) <br/><br/> Second, focus on the introduction content, preliminary. Grasp the underlying content in <strong>methodology</strong> section and the <strong>results</strong>. Take notes on key points. Skip math terms if too heavy. The goal is to understand the paper’s content without getting bogged down in details. <br/><br/> This pass is a thorough, line-by-line reading with the aim of understanding every detail and challenging the author’s assumptions and claims. This pass is particularly useful for papers that require a deep understanding or are central. <strong>Re-implement</strong> the paper’s work/ <strong>identify areas for improvement</strong>.</p> </li> <li> <p>Read wide: If <strong>doing a literature review/ search, just do the first pass</strong> then look for referenced sources, mentioned SOTA methods, prominent proposed dataset/ benchmark, …</p> </li> <li> <blockquote> <p>You would have thus created a list of concepts you need to learn about, and the relevant paper for each, if the paper specifies any.</p> </blockquote> </li> </ul> </details> <hr/> <h2 id="chapter-4-fine-tuning-a-language-model-using-hugging-face"><a href="https://docs.google.com/document/d/18mTeEk1zfJDz-oY48oB96ryZRuiAWRbCLOMYKap8eXk/edit?tab=t.0">Chapter 4: Fine-tuning a Language Model using Hugging Face</a></h2> <h3 id="ch4-objectives"><u>Objectives</u>:</h3> <ul> <li>Load up and process a natural language processing dataset using the datasets library.</li> <li>Tokenize a text sequence, and understand the steps used in tokenization.</li> <li>Construct a dataset and training step for causal language modeling.</li> </ul> <h3 id="ch4-content"><u>Chapter's content</u>:</h3> <details><summary>TLDR: HuggingFace’s Dataloader -&gt; Tokenize -&gt; Train/Test split -&gt; Train and Evaluate -&gt; Upload to model hub</summary> <h4 id="huggingface">HuggingFace</h4> <ul> <li>Community/ Data science center for building, training and deploying ML models based on open src software.</li> <li><a href="https://huggingface.co/docs/transformers/tasks/language_modeling">Guide 1: Causal language mdeoling</a> and <a href="https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb">Guide 2: Code </a> on how to fine tune.</li> </ul> <h4 id="load-up-a-dataset">Load up a dataset</h4> <p>HF’s <code class="language-plaintext highlighter-rouge">Datasets library</code>’s 3 main feats:</p> <ul> <li>Load/ process data from CSV/JSON/text or python dict/pandas.DataFrame</li> <li>Access/ share datasets (through HF’s hub)</li> <li>Can be used with DL frameworks (pandas, numpy, pytorch/ tensorflow) (interoperable - new vocab hehe)</li> </ul> <p><a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a> (Stanford Question Answering Dataset) dataset:</p> <ul> <li>Reading comprehension</li> <li>Questions posed by crowdworkers on a set of Wikipedia articles</li> <li>Answer is a segment of text, span from corresponding reading passage; or unanswerable.</li> </ul> <h4 id="tokenize">Tokenize</h4> <p>A tokenization pipeline in huggingface comprises several <a href="https://huggingface.co/course/chapter6/8?fw=pt">steps</a>:</p> <ul> <li>(1) Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)</li> <li>(2) Pre-tokenization (splitting the input into words)</li> <li>(3) Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)</li> <li>(4) Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs).</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>The above steps show how we can go from text into tokens. There are multiple rules that govern the process that are specific to certain models. For tokenization, there are three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others); we won’t go into any of these, but if you’re curious, you can learn about them <a href="https://huggingface.co/course/chapter6/4?fw=pt#algorithm-overview">here</a>.</p> </blockquote> <h4 id="data-processing">Data Processing</h4> <blockquote> <p>For causal language modeling (CLM), one of the data preparation steps often used is to concatenate the different examples together, and then split them into chunks of equal size. This is so that we can have a common length across all examples without needing to pad.</p> </blockquote> <p>From <br/>[<code class="language-plaintext highlighter-rouge">"I went to the yard.&lt;|endoftext|&gt;"</code>,<code class="language-plaintext highlighter-rouge">"You came here a long time ago from the west coast.&lt;|endoftext|&gt;"</code>], <br/>we might change this to: <br/>[<code class="language-plaintext highlighter-rouge">"I went to the yard.&lt;|endoftext|&gt;You came here"</code>, <code class="language-plaintext highlighter-rouge">"a long time ago from the west coast.&lt;|endoftext|&gt;"</code>]</p> <h4 id="finetuning-and-setup-trainer">Finetuning and setup <a href="https://huggingface.co/docs/transformers/v4.21.3/en/main_classes/trainer#transformers.Trainer">Trainer</a></h4> <p>Code sample:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">distilgpt2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">small_train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">small_eval_dataset</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
</code></pre></div></div> <h4 id="model-evaluation">Model Evaluation</h4> <blockquote> <p>Because we want our model to assign high probabilities to sentences that are real, and low probabilities to fake sentences, we seek a model that assigns the highest probability to the test set. The metric we use is <a href="##" title="Investigate this more">‘perplexity’</a>, which we can think of as the inverse probability of the test set normalized by the number of words in the test set. Therefore, a lower perplexity is better.</p> </blockquote> <h4 id="generation-with-our-fine-tuned-model">Generation with our fine-tuned model</h4> <p>Code sample:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rajpurkar/</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rajpurkar/</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">)</span>
<span class="n">start_text</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">A speedrun is a playthrough of a video game, </span><span class="se">\
</span><span class="s">or section of a video game, with the goal of </span><span class="se">\
</span><span class="s">completing it as fast as possible. Speedruns </span><span class="se">\
</span><span class="s">often follow planned routes, which may incorporate sequence </span><span class="se">\
</span><span class="s">breaking, and might exploit glitches that allow sections to </span><span class="se">\
</span><span class="s">be skipped or completed more quickly than intended. </span><span class="sh">"</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is the</span><span class="sh">"</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
     <span class="n">start_text</span> <span class="o">+</span> <span class="n">prompt</span><span class="p">,</span>
     <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
     <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>

<span class="n">prompt_length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
     <span class="n">inputs</span><span class="p">,</span>
     <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
     <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
     <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
     <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
     <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
     <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">prompt_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>

<span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div> <p>Results:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec4-result-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec4-result-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec4-result-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025/07/harvard-cs197-lec4-result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </details> <hr/> <h2 id="chapter-5-fine-tuning-a-vision-transformer-using-lightning"><a href="https://docs.google.com/document/d/1VnNYGEmVgvl5p8w2xzypGySajaRv6qvzqw7E7LEwQKI/edit?tab=t.0">Chapter 5: Fine-tuning a Vision Transformer using Lightning</a></h2> <hr/> <h2 id="chapter-6--7-solidifying-pytorch-fundamentals"><a href="https://docs.google.com/document/d/1dA8KmOTZePMRl3MixxM6Fb0H8IJhIyn_g-LUXbRHeqU/edit?tab=t.0">Chapter 6 &amp; 7: Solidifying PyTorch Fundamentals</a></h2> <hr/> <h2 id="chapter-8--9-organizing-model-training-with-weights--biases-and-hydra"><a href="https://docs.google.com/document/d/1kZCrACh8wHFFAinscHpbaHqMBKeErjOgXMVqKSUZIMU/edit?tab=t.0">Chapter 8 &amp; 9: Organizing Model Training with Weights &amp; Biases and Hydra</a></h2> <hr/> <h2 id="chapter-10--11-a-framework-for-generating-research-ideas">Chapter 10 &amp; 11: A Framework for Generating Research Ideas</h2> <hr/> <h2 id="chapter-12--13-structuring-a-research-paper">Chapter 12 &amp; 13: Structuring a Research Paper</h2> <hr/> <h2 id="chapter-14--15-aws-ec2-for-deep-learning-setup-optimization-and-hands-on-training-with-chexzero">Chapter 14 &amp; 15: AWS EC2 for Deep Learning: Setup, Optimization, and Hands-on Training with CheXzero</h2> <hr/> <h2 id="chapter-16--17-fine-tuning-your-stable-diffusion-model">Chapter 16 &amp; 17: Fine-Tuning Your Stable Diffusion Model</h2> <hr/> <h2 id="chapter-18-tips-to-manage-your-time-and-efforts">Chapter 18: Tips to Manage Your Time and Efforts</h2> <hr/> <h2 id="chapter-19-making-progress-and-impact-in-ai-research">Chapter 19: Making Progress and Impact in AI Research</h2> <hr/> <h2 id="chapter-20-tips-for-creating-high-quality-slides">Chapter 20: Tips for Creating High-Quality Slides</h2> <hr/> <h2 id="chapter-21-statistical-testing-to-compare-model-performances">Chapter 21: Statistical Testing to Compare Model Performances</h2> <hr/> <h2 id="exercises">Exercises:</h2> <h3 id="assignments">Assignments:</h3> <p>Assignment 1: The Language of Code <br/> Assignment 2: First Dive in AI <br/> Assignment 3: Torched <br/> Assignment 4: Spark Joy <br/> Assignment 5: Ideation and Organization <br/> Assignment 6: Stable Diffusion and Research Operations</p> <hr/> <h3 id="course-project">Course Project:</h3> <p>[Redacted]</p> <hr/> <h2 id="references">References:</h2> <p><br/> <a href="https://www.cs197.seas.harvard.edu/">Course Homepage</a> <br/> <a href="https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit?tab=t.0#heading=h.act903jwq03w">Docs Content</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes and commentaries]]></summary></entry><entry xml:lang="vi"><title type="html">Trượt dốc đừng xuống hố.</title><link href="https://sabertoaster.github.io/vi/blog/2025/downfall/" rel="alternate" type="text/html" title="Trượt dốc đừng xuống hố."/><published>2025-01-01T12:00:00+00:00</published><updated>2025-01-01T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2025/downfall</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2025/downfall/"><![CDATA[<h1 id="lời-mở-đầu">Lời mở đầu</h1> <p>Phòng ký túc xá bốc mùi không khí ứ đọng và dang dở. Những hộp đồ ăn mang về còn sót tạo thành một nghĩa địa trên bàn làm việc của anh, phần thức ăn bên trong đã hóa thạch thành những hình dạng không thể nhận ra, phản chiếu ánh sáng nhạt nhòa từ màn hình laptop. Cậu trai—hai mươi tuổi nhưng mang khuôn mặt của một người già hơn nhiều—ngồi khom lưng trước bàn phím, những ngón tay nhảy múa trên các phím với độ chính xác thành thạo của một kẻ nghiện đang nuôi dưỡng thói quen của mình.</p> <p>2:47 AM. Những con số cháy đỏ từ góc nhìn của anh, nhưng thời gian đã mất hết ý nghĩa từ nhiều giờ trước. Bên ngoài cửa sổ, khuôn viên trường nằm im lìm, nhưng bên trong căn phòng chật hẹp này, anh tỉnh táo hoàn toàn, đồng tử giãn rộng vì cảm giác hưng phấn tổng hợp đang chảy trong huyết quản.</p> <p>Chiếc lọ thủy tinh nhỏ nằm rỗng bên cạnh miếng lót chuột, nội dung bên trong đã được hấp thụ vào dòng máu của anh. <strong>Mortis</strong>, họ gọi nó như vậy trên các diễn đàn. Cái chết tạm thời. Sự trốn thoát tuyệt đối khỏi ý thức, khỏi trách nhiệm, khỏi sự nhận thức ăn mòn về sự tầm thường của chính mình.</p> <hr/> <p>Bạn cùng phòng của anh đã chuyển đi ba tuần trước—nói gì đó về “năng lượng độc hại” và “không thể sống chung được.” Thôi tốt rồi. Giờ anh có toàn bộ không gian cho riêng mình, tự do thỏa mãn mà không bị phán xét, không bị gián đoạn.</p> <p>Cảm giác ấm quen thuộc bắt đầu lan tỏa qua ngực anh, dấu hiệu đặc trưng cho thấy Mortis đang phát huy tác dụng. Nhịp tim anh chậm lại thành một nhịp điệu thôi miên, mỗi nhịp đập vang vọng trong tai như một đếm ngược. Màn hình trước mặt anh mờ đi, dòng nội dung cuộn vô tận trở thành một dòng sông vô nghĩa của ánh sáng và bóng tối.</p> <p><em><span style="color:red">“Anh giỏi hơn thế này,”</span></em> anh thì thầm với chính mình, những từ ngữ có vị như tro tàn. Đó là một lời nói dối anh đã không còn tin từ nhiều tháng trước, nhưng nghi lễ vẫn tiếp tục—một lời cầu nguyện cuối cùng trước khi bị đóng đinh bằng hóa chất.</p> <p>Hơi thở anh trở nên nông. Cảm giác hưng phấn ập đến như một cơn sóng thần, cuốn trôi mọi góc cạnh sắc bén của thực tại. Sự thất vọng từ dự án nhóm thất bại, sự nhục nhã khi bị những người anh từng nghĩ là bạn ruồng bỏ, gánh nặng nghiền nát của tiềm năng chưa được khai phá—tất cả đều tan biến trong niềm hạnh phúc hóa học.</p> <p><em><span style="color:red">Dù sao họ cũng không xứng đáng với tôi,</span></em> anh nghĩ khi ý thức bắt đầu phai nhạt. <em><span style="color:red">Không có tham vọng. Yếu đuối. Họ không thể đối phó với một người có tầm nhìn thực sự.</span></em></p> <p>Nhưng ngay cả khi lời nói dối hình thành, một sự thật sâu sắc hơn vẫn cào xé những rìa của nhận thức đang mờ dần. Anh thấy chính mình như cách người khác nhìn anh: <strong>cay độc, đòi hỏi, không thể làm hài lòng</strong>. Anh đã tập hợp đội nhóm không phải như những cộng tác viên mà như những kẻ thuộc hạ, mong đợi họ cúi đầu trước trí tuệ của anh trong khi chỉ đóng góp những lời chỉ trích và khinh thường.</p> <p>Tim anh ngập ngừng, rồi ngừng đập. Máu ngừng cuộc hành trình cổ xưa qua các tĩnh mạch. Xét theo mọi mục đích và ý định, anh đã chết—một xác chết được chống đỡ trên chiếc ghế mà anh vẫn hay ngồi, được bao quanh bởi những mảnh vỡ của những ngày hoang phí.</p> <p>Trong cái chết tạm thời này, những ảo ảnh nhảy múa sau mi mắt khép kín của anh. Anh thấy chính mình như anh có thể trở thành: <span style="color:green">tập trung</span>, <span style="color:green">mang lại động lực</span>, <span style="color:green">sự hợp tác</span>. Anh thấy những dự án anh có thể hoàn thành, những mối quan hệ anh có thể xây dựng, người đàn ông anh có thể trở thành nếu có thể thoát khỏi quan tài hóa học này. Nhưng ngay cả trong cái chết, những ảo ảnh vẫn cảm thấy xa vời, thuộc về một phiên bản khác của chính anh—một người lạ mang khuôn mặt của anh.</p> <hr/> <p>Những phút trôi qua. Hoặc có thể là hàng giờ. Thời gian di chuyển khác đi trong vòng tay của cái chết.</p> <p>Rồi, đột ngột như khi nó rời đi, sự sống ùa về. Tim anh giật mình vào nhịp điệu với những nhịp đập dữ dội, tuyệt vọng. Máu dâng trào qua những mạch máu đang tỉnh dậy, mang oxy đến một bộ não thiếu mục đích. Mắt anh bật mở, và căn phòng quen thuộc ập về trong tầm nhìn—không thay đổi, không tha thứ, đang chờ đợi.</p> <p>Anh sống lại, nhưng thế giới vẫn giữ nguyên như anh đã để lại. Những hộp rỗng vẫn rải rác trên bàn. Những bài tập vẫn nằm chưa hoàn thành trong các thư mục laptop. Sự im lặng vẫn ép sát những bức tường nơi lẽ ra phải có kết nối con người.</p> <p>Anh ây với tay run rẩy vươn tới một lọ khác, đã lập kế hoạch cho cái chết tạm thời tiếp theo. Bên ngoài, bình minh đang ló dạng trên khuôn viên, nhuộm bầu trời những sắc thái của khả năng anh không còn có thể nhìn thấy. <strong>Ở đâu đó bên ngoài những bức tường này, những đồng đội cũ của anh có lẽ đang thức, làm việc trên những dự án anh đã bỏ rơi, xây dựng tương lai mà anh đã thuyết phục bản thân rằng họ không xứng đáng.</strong></p> <p>Nhưng ở đây, trong ngôi mộ do chính anh tạo ra, anh chuẩn bị chết lần nữa thay vì đối mặt với viễn cảnh đáng sợ của việc thực sự sống.</p> <p>Chiếc lọ thủy tinh bắt ánh sáng buổi sáng, nắm giữ những lời hứa trống rỗng như cuộc đời anh đang chọn lựa để bỏ lại đằng sau, từng cái chết tạm thời một.</p> <hr/> <h1 id="chương-0-trải-nghiệm-cận-tử">Chương 0: Trải nghiệm cận tử</h1> <p>Tiếng báo thức the thé xuyên qua màn sương của một đêm mất ngủ khác. 6:30 AM. Điện thoại anh rung bên cạnh giường, rung lên với sự hối hả của một thế giới đòi hỏi gương mặt của anh. Anh rên rỉ, cố gắng ngủ nướng thêm chút trên những tấm ga chưa được thay từ nhiều tuần, cơ thể phản đối mọi chuyển động.</p> <p><em>Chỉ năm phút nữa thôi,</em> anh nghĩ, nhưng bàng quang của anh có kế hoạch khác.</p> <p>Loạng choạng ra khỏi giường, anh lê bước về phía phòng tắm, chân vướng vào quần áo vứt bừa và lon nước tăng lực còn sót lại. Ánh đèn huỳnh quang le lói bật sáng, tạo những bóng tối khắc nghiệt trên hình phản chiếu gầy gò của anh trong gương. Những quầng thâm bao quanh mắt như vết bầm tím, chứng nhân cho một đêm khác đã kéo dài đến tận bình minh.</p> <hr/> <p>Rồi nó ập đến.</p> <p>Cảm giác đó ập xuống anh như một cơn sóng thần của nỗi kinh hoàng thuần khiết. Đầu anh cảm thấy như sắp nổ tung từ bên trong, áp lực tăng lên phía sau mắt cho đến khi thế giới bắt đầu mờ đi và biến dạng. Màu sắc chảy lẫn vào nhau, các cạnh mềm đi, và những viên gạch phòng tắm quen thuộc trở thành một bức tranh trừu tượng của cảm giác buồn nôn và sợ hãi.</p> <p>Anh loạng choạng, vai va mạnh vào bức tường phòng tắm lạnh lẽo. Tầm nhìn hoàn toàn trắng xóa, rồi đen thui, rồi lại trắng—một ánh đèn nhấp nháy của sự bất tỉnh đang đến gần. Tim anh đập thình thịch vào xương sườn với sự dữ dội đến nỗi anh có thể nghe từng nhịp đập riêng lẻ vang vọng trong tai như tiếng súng nổ trong nhà thờ.</p> <p><em><span style="color:red">Thình. Thịch. THÌNH.</span></em></p> <p>Mồ hôi lạnh bùng phát khắp da, thấm qua áo sơ mi trong vài giây. Anh trượt xuống tường, chân mềm nhũn, và thấy mình đang ngồi trên sàn phòng tắm, thở hổn hển như một con cá bị kéo lên khỏi nước.</p> <p><em><span style="color:red">Thế là xong</span></em>, anh nghĩ với sự rõ ràng trong suốt tựa pha lê. <em><span style="color:red">Đây là cách mà mình chết. <b>Hai mươi tuổi, cô đơn trong phòng tắm lúc bình minh</b>, không có gì để tự hào về cả.</span></em></p> <p>Nhận thức này ập đến mạnh hơn cả những triệu chứng thể chất. Tất cả những giấc mơ anh đã bỏ rơi như những bài tập đã trễ deadline—trốn thoát khỏi đất nước ngột ngạt này, xây dựng một cuộc sống đáng sống, trở thành nhà khoa học anh từng mong ước mình có thể là. Chúng lóe lên trước mắt anh không phải như cảm hứng mà như những lời buộc tội, bóng ma của những tương lai anh đã giết chết bằng chính đôi tay mình.</p> <p>Đây có phải là cơn đau tim? Tâm trí anh chạy qua những thuật ngữ y khoa hấp thụ từ những đêm trắng lướt internet. Nhồi máu cơ tim. Thiếu oxy não. Đột quỵ. Tụt huyết áp. Những từ ngữ cảm thấy xa lạ và đáng sợ, những mô tả lâm sàng về cuộc nổi loạn của cơ thể chống lại sự lạm dụng anh đã gây ra cho nó.</p> <p><em><span style="color:red">Hai mươi tuổi</span></em>, anh nghĩ lại, con số cảm thấy vừa trẻ một cách không thể tin được vừa già một cách tàn khốc. <em><span style="color:red">Hai mươi tuổi và chết trong phòng tắm vì mình không thể ngừng đầu độc bản thân bằng chất kích thích và những cái chết tổng hợp.</span></em></p> <p>Trong khoảnh khắc kinh hoàng thuần khiết đó, được lột bỏ mọi vỏ bọc và cái tôi, anh nhìn lại cuộc đời mình trung thực một cách tàn nhẫn. Những đêm muộn không phải sự hiệu quả—chúng là sự phá hoại. Thứ này không phải giải trí—chúng là sự trốn thoát khỏi thực tại anh quá sợ để đối mặt. Sự cô lập không phải cô độc cao thượng—nó là hậu quả tự nhiên của việc đẩy xa bất kỳ ai có thể quan tâm.</p> <p>Nếu anh sống sót qua điều này—<em>nếu</em>—mọi thứ sẽ thay đổi. Chúng phải thay đổi.</p> <p><em><span style="color:green">Tôi sẽ ngủ sớm,</span></em> anh hứa với bất kỳ vị thần hay thế lực siêu nhiên nào có thể đang lắng nghe. <em><span style="color:green">Không còn gì cho đến tận bình minh. Tôi sẽ làm điều gì đó có ý nghĩa, điều gì đó giúp ích cho người khác thay vì chỉ nuôi dưỡng vỏ bọc trống rỗng của chính mình.</span></em></p> <p>Những lời hứa cảm thấy mong manh và tuyệt vọng, như những lời cầu nguyện thì thầm trong hào chiến đấu. Nhưng chúng là thật, được khắc vào ý thức của anh.</p> <hr/> <p>Dần dần, các triệu chứng bắt đầu thuyên giảm. Tầm nhìn anh trong trở lại. Nhịp tim chậm lại từ chạy nước rút thành chạy bộ. Áp lực nghiền nát trong đầu giảm thành một cơn đau âm ỉ. Anh vẫn ở trên sàn phòng tắm thêm vài phút nữa, sợ di chuyển, sợ tin rằng cuộc khủng hoảng đã qua.</p> <p>Khi cuối cùng anh đứng dậy, chân run rẩy, anh bắt gặp hình phản chiếu của mình một lần nữa. Cùng một khuôn mặt gầy gò, cùng những đôi mắt trống rỗng—nhưng có điều gì đó đã thay đổi. Phía sau sự kiệt sức và sợ hãi, có một tia sáng của điều gì đó anh chưa thấy trong nhiều tháng qua.</p> <p><strong><span style="color:yellow">Sự quyết tâm.</span></strong></p> <p>Hoặc có thể chỉ là nỗi kinh hoàng được cải trang thành quyết tâm. Dù thế nào, nó cũng nhiều hơn những gì anh cảm thấy trong một thời gian rất dài.</p> <p>Anh vấy nước lạnh lên mặt, nhìn một người lạ khác trong gương đang nhợt nhạt, sợ hãi trong gương bắt chước chuyển động của mình. Hôm nay sẽ khác. Phải như vậy.</p> <p>Câu hỏi là liệu ký ức của khoảnh khắc này có thể sống sót qua sự tê liệt thoải mái của một cái chết tạm thời khác nữa không, hay nó sẽ phai nhạt như tất cả những lời hứa khác với chính mình, bị mất trong dòng cuộn vô tận của sự chìm đắm trong sự trốn chạy.</p> <p>Chỉ có thời gian mới cho biết liệu anh có đủ mạnh mẽ để chọn sự sống thay vì vòng tay quyến rũ của cái chết nhân tạo này.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Hành trình xây dựng thế giới của tôi]]></summary></entry></feed>