<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sabertoaster.github.io/vi/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sabertoaster.github.io/vi/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-20T19:25:17+00:00</updated><id>https://sabertoaster.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry xml:lang="en"><title type="html">In memorial of David C.Marr</title><link href="https://sabertoaster.github.io/vi/blog/2026/in_memorial_david-marr/" rel="alternate" type="text/html" title="In memorial of David C.Marr"/><published>2026-01-19T12:00:00+00:00</published><updated>2026-01-19T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2026/in_memorial_david-marr</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2026/in_memorial_david-marr/"><![CDATA[<hr/> <h1 id="table-of-contents">Table of contents</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Who is David Marr? A short biography
2. Tri-level of analysis
3. Missing views on `Tri-level of analysis`
4. Mechanistic Interpretability
</code></pre></div></div> <hr/> <h1 id="1-who-is-david-marr">1. Who is David Marr?</h1> <p>David (Courtney) Marr is a person of previous century, a cognitive scientist, a neurologist. In his lifespan, if not accounting for the years of undergrad studies, he would have around 15 years working towards actual scientific rigors, yet made foundational breakthroughs that lead to establishments of Computational Neuroscience, and have an influential impacts on Artificial Intelligence (which most people’s daily routine employ under some form or another: Recommender Systems such as Youtube, TikTok, Netflix; Chatbot, LLMs; etc etc).</p> <p>David Courtnay Marr was born on January 19, 1945 in Essex, England. Unfortunately, despite being a legend in the field, we know very little about David’s childhood and elementary school years. However, as far as we know, he attended one of England’s most established schools, Rugby School, a private middle-high school, on a scholarship. Around that time he grew interest in mathematics, physics and neuroscience; particularly after reading the book “Living Brain” (1953) of Grey Walter’s.</p> <blockquote> <p>“Young David was excited by the idea of developing a “mathematical theory of the brain,” advocating that it was a feasible notion.”</p> </blockquote> <p>In 1963, David began his undergraduate studies in mathematics at Trinity College, Cambridge, on a scholarship. After completing his bachelor’s degree three years later, he contacted Giles Brindley in the physiology department to express his desire to do his Ph.D. with him on the brain-related work he had been contemplating for a long time. <strong>Brindley advised him to spend a year reading extensively in the field to gain knowledge, which David dedicated a year to.</strong> Among his readings at that time were the works of Ramon y Cajal and the 1967 book by Eccles, Ito, and Szentágothai, “The Cerebellum as a Neuronal Machine.” After this intense period of reading, Marr felt ready for his theory, which resulted in three separate papers published in 1969, 1970, and 1971. These papers, in essence, attempted to answer one of the fundamental questions of neurobiology by focusing on three different neuroanatomical structures: How does the brain work?</p> <p>The early 1970s research landscape exhibited a problematic fragmentation: artificial intelligence pursued symbolic rule-based systems largely divorced from neural implementation, while neuroscience focused on circuit-level descriptions without computational interpretations. Upon joining MIT in 1973, Marr recognized this epistemological gap from direct experience. His cerebellar cortex theory (1969) had proposed functional explanations for anatomical structure, yet he lacked a systematic methodology to relate neural mechanisms to computational objectives. His interdisciplinary credentials with formal training in mathematics (BSc, MA), a neuroscience doctorate, and active engagement with AI research, uniquely positioned him to synthesize these perspectives. Mathematics enabled precise formalization of computational-level specifications (the <em>what</em> and <em>why</em>). AI methodologies informed algorithmic-level descriptions (representations and processes). Neuroscience provided implementation-level constraints (physical substrates). This synthesis yielded the levels-of-analysis framework, formally introduced with Poggio in 1976.</p> <hr/> <h1 id="2-tri-level-of-analysis">2. Tri-level of analysis:</h1> <p>To understand how this framework emerged, we must look at the “dynamic duo” of MIT’s AI Lab. In 1973, <strong>Tomaso Poggio</strong>, a physicist and cyberneticist from the Max Planck Institute, visited Boston. He found Marr to be “sharp, opinionated, and possessing clear ideas about almost everything”. Together, they realized that the prevailing reductionist approach, trying to understand the brain by only looking at neurons, was like trying to understand how a bird flies by studying only its feathers, ignoring aerodynamics.</p> <p>The framework was first formally introduced in their joint 1976 paper, <em>“From Understanding Computation to Understanding Neural Circuitry”</em>. They argued against the prevailing reductionist view that understanding the hardware (neurons) was enough to understand the system. They introduced the levels of analysis to bridge the gap between Poggio’s detailed biophysics (flies) and Marr’s high-level computational theory (human vision), and further extended in generality to state that any information-processing system must be understood at three distinct, loosely coupled level:</p> <ol> <li><strong>Computational Theory (The Goal):</strong> What is the system doing, and <em>why</em>? What are the constraints? (e.g., The goal of a cash register is addition).</li> <li><strong>Algorithmic &amp; Representational (The Method):</strong> How is the input transformed into the output? What specific algorithm is used? (e.g., The rules of decimal arithmetic).</li> <li><strong>Implementation (The Hardware):</strong> How is this physically realized? (e.g., Silicon chips, biological neurons, or mechanical gears).</li> </ol> <p></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://apsc450computationalneuroscience.com/wp-content/uploads/2019/01/marr3-480.webp 480w,https://apsc450computationalneuroscience.com/wp-content/uploads/2019/01/marr3-800.webp 800w,https://apsc450computationalneuroscience.com/wp-content/uploads/2019/01/marr3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="https://apsc450computationalneuroscience.com/wp-content/uploads/2019/01/marr3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A figure from APSC450 course, William and Mary refering to different levels in Marr's analysis </div> <p>Marr emphasized that you cannot separate the <em>process</em> from the <em>representation</em>. A representation is simply a formal system for making certain information explicit.</p> <p>Quoting directly from Marr’s saying:</p> <ul> <li> <blockquote> <p>Vision is therefore, first and foremost, an information-processing task, but we cannot think of it just as a process. For if we are capable of knowing what is where in the world, our brains must somehow be capable of <em>representing</em> this information — in all its profusion of color and form, beauty, motion and detail.  The study of vision must therefor include not only the study of how to extract from images the various aspects of the world the are useful to us, but also an inquiry into the nature of the internal representations by which we capture this information and thus make it available as a basis for decisions about our thoughts and actions.  This duality — the representation and the processing of information — lies at the heart of most information-processing tasks and will profoundly shape our investigation of the particular problems posed by vision.</p> </blockquote> </li> <li> <blockquote> <p>Modern representational theories conceive of the mind as having access to systems of internal representations; mental states are characterized by asserting what the internal representations currently specify, and mental processes by how such internal representations are obtained and how they interact.</p> </blockquote> </li> <li> <blockquote> <p>A representation is a formal system for making explicit certain entities or types of information, together with a specification of how the system does this.  … However, the notion that one can capture some aspect of reality by making a description of it using a symbol and that to do so can be useful seems to me a fascinating and powerful idea. … But … the choice of which to use is important and cannot be taken lightly. It determines what information is made explicit and hence what is pushed further into the background, and it has a far-reaching effect on the ease and difficulty with which operations may subsequently be carried out on that information.</p> </blockquote> </li> </ul> <p>In short, the representation is the state of the system, portraited by symbols used to make specific information <em>explicit</em>. However, information processing is the transformation of the system. It is the algorithmic rules that manipulate the representation. The ease or difficulty of a process depends entirely on the representation chosen.</p> <p>Take an example for the counting system. The number “21” (decimal) and “10101” (binary) are different representations of the same abstract quantity. They make different things explicit (decimal makes magnitude clear to humans; binary makes on/off states clear to circuits). This choice matters because if you try to do long division with Roman numerals; it is a <a href="https://leetcode.com/problems/roman-to-integer/">nightmare</a>, whereas it is easy with Arabic numerals.</p> <p>While writing this, I stumbled upon a paper titled <a href="https://arxiv.org/pdf/2004.05107">Levels of Analysis for Machine Learning</a> written by Google Deepmind submitted at “Bridging AI and Cognitive Science” ICLR 2020 workshop. It’s a perfect demonstration of putting Marr’s perspective (more than 50 years ago btw) to advocate a common conceptual framework that unify computer science, engineering and cognitive science into one single thread.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2026/01/1_696f4b48-43ba-4839-abfa-f432d6642e17_1768901448060-480.webp 480w,/assets/img/blog/2026/01/1_696f4b48-43ba-4839-abfa-f432d6642e17_1768901448060-800.webp 800w,/assets/img/blog/2026/01/1_696f4b48-43ba-4839-abfa-f432d6642e17_1768901448060-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2026/01/1_696f4b48-43ba-4839-abfa-f432d6642e17_1768901448060.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We can apply it to a standard Convolutional Neural Network (CNN) to see how it organizes our understanding:</p> <table> <thead> <tr> <th>Level</th> <th>Question</th> <th>Application to CNN (Vision)</th> </tr> </thead> <tbody> <tr> <td>Computational</td> <td>What is the goal?</td> <td>To map spatial data (images) to object categories (labels) in a way that is invariant to translation (moving the object shouldn’t change the label).</td> </tr> <tr> <td>Algorithmic</td> <td>How do we solve it?</td> <td>Using the Convolution operation, which enforces spatial locality and weight sharing. The representation is a hierarchy of feature maps.</td> </tr> <tr> <td>Implementation</td> <td>Physical realization?</td> <td>Executed on a GPU using CUDA kernels, or biological neurons firing in the visual cortex.</td> </tr> </tbody> </table> <p>Marr’s genius was realizing that these levels are loosely coupled. You can run the same algorithm (CNN) on different hardware (Implementation), or solve the same computational goal (Object Detection) with a different algorithm (Vision Transformers).</p> <p>However, as we will see, Marr missed out on crucial layers that define modern AI.</p> <hr/> <h1 id="3-missing-views-of-marrs-vision">3. Missing view(s) of Marr’s “Vision”</h1> <p>Marr’s three-level framework was groundbreaking, but it had a critical blind spot: it explained systems after they were built, not how they came to be. His levels (Computational, Algorithmic, Implementation) explain how a system works once it is finished, they fail to explain how the system got there. Marr’s original framework assumes a static, fully formed system. Poggio argues that for biological intelligence (and modern AI), the <em>process of acquiring</em> the algorithm is just as important as the algorithm itself, and even why there’s that process in the first place.</p> <p>In Tomaso 2012’s paper, <a href="https://www.mit.edu/~9.54/fall14/Classes/class01/Important_for_9S913/Poggio_LevelsOfunderstanding_Perceptionv41_Dec2012.pdf">The Levels of Understanding framework, revised</a>, he proposed the evolutionary and learning level on top of computational level.</p> <p><strong>1. Evolutionary Level:</strong> Why does this architecture exist? This level asks what environmental pressures shaped the system. Marr analyzed constraints like “surfaces are smooth” (for stereo vision). Poggio argues the ultimate constraint is deeper: <strong>“functions in nature are compositionally sparse.”</strong> The physical world is hierarchical; photons create edges, edges form shapes, shapes compose objects. Evolution selected brains with deep layered architectures because they mirror this hierarchical structure of reality itself. This explains why both cortical columns and convolutional neural networks share the same “deep hierarchy” design: they’re solving the same compositional problem.</p> <p><strong>2. Learning Level:</strong> How is the system constructed from data? This level examines the mathematical principles of <strong>generalization</strong>, how a system extracts universal rules (“objects are permanent,” “gravity pulls down”) from limited, noisy examples. For biological brains, this involves developmental plasticity and synaptic learning. For artificial networks, it involves optimization algorithms like Stochastic Gradient Descent (SGD). Poggio proposed that the learning algorithm itself acts as an implicit regularizer, favoring simple, generalizable solutions over complex memorization.</p> <p>The Extended Framework:</p> <ol> <li><strong>Evolutionary:</strong> Why this architecture? (Compositional structure of nature)</li> <li><strong>Learning:</strong> How is it acquired? (Generalization from data)</li> <li><strong>Computational:</strong> What goal does it solve? (Edge detection, face recognition)</li> <li><strong>Algorithmic:</strong> What representations/processes? (Convolutions, transformers)</li> <li><strong>Implementation:</strong> What hardware? (Neurons, GPUs)</li> </ol> <p>Defining these new levels hasn’t been straightforward. Poggio’s theories have sparked intense academic debates about whether his mathematical frameworks actually explain deep learning’s success, or merely describe it in elegant language.</p> <p>Poggio’s team argued that SGD implicitly regularizes networks, preventing overfitting even when models have billions of parameters. But in 2017, a landmark paper by Zhang et al. (<em>“Understanding Deep Learning Requires Rethinking Generalization”</em>) challenged this entire premise. They showed that deep networks can perfectly memorize datasets with completely random labels, where no pattern exists to learn. If Poggio were fully correct that SGD favors simplicity, networks shouldn’t be able to memorize pure noise so easily. Yet they <em>can</em> memorize noise (fitting random data perfectly) but <em>don’t</em> memorize real data (generalizing well instead). This suggests that “implicit regularization” alone might be insufficient and that something else determines when networks generalize versus memorize.</p> <p>Moreover, Poggio argues that deep learning works because it exploits the compositional structure of our universe. Critics, however, point to a critical gap: <em>representation ≠ learnability</em>. Just because a function <em>can</em> be represented compositionally doesn’t mean it’s learnable through gradient descent. Take cryptographic functions for example. They’re compositional (hierarchical combinations of simple operations). At its core, a cryptographic function (like AES or SHA-256) is just a composition of simple math operations. Layer 1: Take input bits, XOR them with a key. Layer 2: Shuffle the bits (permutation). Layer 3: Swap bits using a lookup table (substitution). Repeat 10-14 times. In terms of local, hierachical and compositional properties, DNNs should be able to work on AES but it doesn’t.</p> <p>Far from retreating, Poggio’s Center for Brains, Minds, and Machines has doubled down. In July 2025, his team published <em>“A Theory of Deep Learning Must Include Compositional Sparsity,”</em> mathematically linking compositionality to fundamental computability theory. They proved that any efficiently Turing-tractable function can be represented as compositionally sparse, suggesting deep learning works because it aligns with the deep structure of computation itself.</p> <p>So in conclusion, Marr gave us the language to describe intelligence. Poggio extended it to explain learning and evolution. But the full story of how blind optimization discovers intelligent structure is still being written. As we’ll see next, this expanded framework sets the stage for an entirely new field: <em>*Mechanistic Interpretability</em>, the discipline of reverse-engineering what modern AI systems have actually learned.</p> <hr/> <h1 id="4-the-birth-of-mechanistic-intepretability">4. The birth of Mechanistic Intepretability</h1> <p>Marr’s tri-level framework was designed to explain how systems <em>work once they are built</em>. But deep learning created a new challenge: systems that work brilliantly but remain opaque about what they’ve actually learned. A language model can translate poetry or a vision model can identify objects, yet neither we nor the researchers who built them can articulate which internal computations solve these tasks. In Marr’s terms, we have implementations (billions of neural parameters) but lack a clear map of the algorithmic level; the actual features, representations, and algorithms the network uses.</p> <p>So just what is Mechanistic Intepretability (Mech Interp), then? It was coined by Chris Olah, a technical staff at Anthropic (Awesome blog go check it out, <a href="https://colah.github.io/">Home - colah’s blog Colah’s Blog</a>). It’s established to reverse the state of ambiguity in Neural Nets into Marr’s framework. Rather than treating deep networks as black boxes, where we only care about inputs and outputs, mechanistic interpretability opens the box and asks: What algorithms have been learned? What features do neurons represent? How do these features compose into circuits that solve tasks. More onto that <a href="https://distill.pub/2017/feature-visualization">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2026/01/feature_visualization-480.webp 480w,/assets/img/blog/2026/01/feature_visualization-800.webp 800w,/assets/img/blog/2026/01/feature_visualization-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2026/01/feature_visualization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A figure from Chris Olah's distill.pub post about Feature Visualization. </div> <p>The field treats neural networks like compiled computer programs. Just as a programmer might decompile machine code to recover the original logic, mechanistic interpretability attempts to recover the <em>logic</em> learned by gradient descent from billions of weights. This means identifying two core structures:</p> <ol> <li><strong>Features</strong>: Interpretable concepts that neurons or groups of neurons encode. In a vision model, a feature might be “fur texture” or “curved edge.” In a language model, features represent abstract ideas like “is this referring to the subject?” or “does this context suggest deception?”</li> <li><strong>Circuits</strong>: Sparse subgraphs of the network, specific patterns of weighted connections, that implement algorithms on those features. Famous examples include “induction heads” in transformers, which enable models to perform in-context learning by recognizing repeated patterns and predicting what comes next.</li> </ol> <p>Mech Interp utilizes Marr’s framework in a way he never could with biological brains. Consider a GPT model detecting grammatical subject-verb agreement: * <strong>Computational Level:</strong> Identify the correct subject for a verb in complex sentences (why this matters: sentences with multiple potential subjects require disambiguation) * <strong>Algorithmic Level:</strong> The model uses an attention-based mechanism; specific attention heads track “name mentions,” and other heads suppress wrong subjects through mutual inhibition. * <strong>Implementation Level:</strong> These circuits run on GPU tensors. And for Poggio’s newly added layers, we could ask ourselves “Why does this circuit form? What inductive biases or compositional properties of the task make this algorithm a natural attractor for gradient descent?”</p> <p>But the fact is, this field are facing a scaling challenge. Researchers have successfully reverse-engineered circuits in small models (roughly 1M–410M parameters) with algorithmic tasks like modular arithmetic, greater-than comparisons, and even some natural language phenomena like subject-verb agreement. But current techniques struggle with larger models (billions of parameters) and messier, real-world tasks.</p> <p>Further details could be directly refered to Neel Nanda’s <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J">blogpost</a>.</p> <hr/> <h1 id="wrapping-up">Wrapping Up:</h1> <p>This post was written in memorial of David Marr. I respect the way that he devoted for science until the last breath. May his soul rest in peace.</p> <p>I wrote this blog post to anchor myself against the definition of current AI trends. I wanna understand how the brain works and how to derive useful mechanisms from it (as it’s the only source of general intelligence the world possesses currently). I hope many people will be more concerned on this matter in the future to push intelligence replication as one of 21st century profound inventions. Thank you for reading until the end:D</p> <hr/> <h1 id="appendix">Appendix</h1> <h2 id="a-chronological-biography-key-turning-points">A. Chronological Biography: Key Turning Points</h2> <p><strong>1945–1966: Early Formation</strong></p> <ul> <li> <p><strong>January 19, 1945:</strong> Born in Essex, England</p> </li> <li> <p><strong>1957–1963:</strong> Attended Rugby School on scholarship; developed interests in mathematics, physics, and neuroscience after reading Grey Walter’s <em>The Living Brain</em> (1953)</p> </li> <li> <p><strong>1963–1966:</strong> Undergraduate in mathematics at Trinity College, Cambridge (scholarship)</p> </li> </ul> <p><strong>1966–1971: The Neuronal Years</strong></p> <ul> <li> <p><strong>1966–1967:</strong> Intensive year of reading (advised by Giles Brindley) including Ramón y Cajal’s neuroanatomy and Eccles, Ito, and Szentágothai’s <em>The Cerebellum as a Neuronal Machine</em> (1967). This shaped his theoretical approach to neural circuits.[<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC2805361/">pmc.ncbi.nlm.nih</a>]​</p> </li> <li> <p><strong>1967–1971:</strong> Combined MA/PhD at Cambridge under Brindley, focusing on “How does the brain work?” Produced three landmark papers proposing computational theories of cerebellar cortex (1969), cerebral neocortex (1970), and hippocampus (archicortex, 1971).[<a href="https://www.thecognizer.com/post/on-the-shoulders-of-giants-David-%20Marr">thecognizer</a>]​</p> </li> </ul> <p><strong>1972–1973: The MIT Turning Point</strong></p> <ul> <li> <p><strong>May 24–26, 1972:</strong> Attended brain theory workshop at Boston University where he met Marvin Minsky and Seymour Papert. This encounter redirected his career toward AI.[<a href="https://academic.oup.com/book/338/chapter/134988181">academic.oup</a>]​</p> </li> <li> <p><strong>November 21, 1972:</strong> Received famous invitation letter from Minsky/Papert: “<em>This is a formal invitation for you to come for three or six months or more if you want. The salary is $1,250 a month. Requirement: Do something terrific!</em>”[<a href="https://www.thecognizer.com/post/on-the-shoulders-of-giants-David-%20Marr">thecognizer</a>]​</p> </li> <li> <p><strong>1972–1973:</strong> Brief stint at Francis Crick’s MRC Molecular Biology Laboratory (referred by Sydney Brenner)</p> </li> <li> <p><strong>1973:</strong> Moved to MIT AI Lab with KLUMSY proposal (robotic arm sensorimotor control); proposal rejected, interest shifted to vision[<a href="https://shimon-edelman.github.io/marr/marr.html">shimon-edelman.github</a>]​</p> </li> </ul> <p><strong>1973–1976: Birth of the Tri-Level Framework</strong></p> <ul> <li> <p><strong>1973:</strong> Met Tomaso Poggio at MIT; beginning of the “dynamic duo” collaboration.</p> </li> <li> <p><strong>1975:</strong> Accepted permanent faculty position in MIT Psychology Department</p> </li> <li> <p><strong>1976:</strong> Established “vision group” with Poggio, Shimon Ullman, Ellen Hildreth, and Keith Nishihara</p> </li> <li> <p><strong>1976:</strong> Published “<em>From Understanding Computation to Understanding Neural Circuitry</em>” with Poggio—first formal introduction of the tri-level framework[<a href="https://www.bionity.com/en/encyclopedia/David_Marr_\(neuroscientist\).html">bionity</a>]​</p> </li> </ul> <p><strong>1977–1980: Racing Against Time</strong></p> <ul> <li> <p><strong>December 2, 1977:</strong> Diagnosed with acute leukemia</p> </li> <li> <p><strong>1977:</strong> Promoted to tenured professor in MIT Psychology Department</p> </li> <li> <p><strong>Summer 1979:</strong> Completed manuscript for <em>Vision: A Computational Investigation</em></p> </li> <li> <p><strong>1979–1980:</strong> Despite illness, vision group published ~120 papers during this period[<a href="https://www.thecognizer.com/post/on-the-shoulders-of-giants-David-%20Marr">thecognizer</a>]​</p> </li> <li> <p><strong>November 17, 1980:</strong> Died in Cambridge, Massachusetts, age 35</p> </li> <li> <p><strong>1982:</strong> <em>Vision</em> published posthumously by MIT Press; became foundational text for computational neuroscience</p> </li> </ul> <hr/> <h2 id="b-selected-publications-chronological">B. Selected Publications (Chronological)</h2> <h2 id="early-neural-theories-19691971">Early Neural Theories (1969–1971)</h2> <ul> <li> <p><strong>Marr, D.</strong> (1969). A theory of cerebellar cortex. <em>Journal of Physiology</em>, 202, 437-470. [First proposal of motor learning theory, predating experimental confirmation of Hebbian plasticity]</p> </li> <li> <p><strong>Marr, D.</strong> (1970). A theory for cerebral neocortex. <em>Proceedings of the Royal Society B</em>, 176, 161-234.</p> </li> <li> <p><strong>Marr, D.</strong> (1970). How the cerebellum may be used. <em>Nature</em>, 227, 1224-1228. (With Stephen Blomfield)</p> </li> <li> <p><strong>Marr, D.</strong> (1971). Simple memory: a theory for archicortex. <em>Philosophical Transactions of the Royal Society B</em>, 262, 23-81.</p> </li> </ul> <h2 id="computational-vision-era-19741982">Computational Vision Era (1974–1982)</h2> <ul> <li> <p><strong>Marr, D.</strong> (1974). The computation of lightness by the primate retina. <em>Vision Research</em>, 14, 1377-1388.</p> </li> <li> <p><strong>Marr, D.</strong> (1975). Approaches to biological information processing. <em>Science</em>, 190, 875-876.</p> </li> <li> <p><strong>Marr, D.</strong> (1976). Early processing of visual information. <em>Philosophical Transactions of the Royal Society B</em>, 275, 483-519.</p> </li> <li> <p><strong>Marr, D., &amp; Poggio, T.</strong> (1976). Cooperative computation of stereo disparity. <em>Science</em>, 194, 283-287.</p> </li> <li> <p><strong>Marr, D.</strong> (1976). Analyzing natural images: a computational theory of texture vision. <em>Cold Spring Harbor Symposia on Quantitative Biology</em>, 40, 647-662.</p> </li> </ul> <h2 id="landmark-theoretical-papers">Landmark Theoretical Papers</h2> <ul> <li> <p><strong>Marr, D., &amp; Poggio, T.</strong> (1977). From understanding computation to understanding neural circuitry. <em>Neurosciences Research Program Bulletin</em>, 15, 470-488. [<strong>First formal presentation of tri-level framework</strong>]</p> </li> <li> <p><strong>Marr, D.</strong> (1977). Artificial intelligence: A personal view. <em>Artificial Intelligence</em>, 9, 37-48.</p> </li> <li> <p><strong>Marr, D., &amp; Nishihara, H. K.</strong> (1978). Representation and recognition of the spatial organization of three-dimensional shapes. <em>Proceedings of the Royal Society B</em>, 200, 269-294.</p> </li> <li> <p><strong>Marr, D., &amp; Poggio, T.</strong> (1979). A computational theory of human stereo vision. <em>Proceedings of the Royal Society B</em>, 204, 301-328.</p> </li> </ul> <h2 id="edge-detection-and-low-level-vision">Edge Detection and Low-Level Vision</h2> <ul> <li> <p><strong>Marr, D., &amp; Hildreth, E.</strong> (1980). Theory of edge detection. <em>Proceedings of the Royal Society B</em>, 207, 187-217.</p> </li> <li> <p><strong>Marr, D.</strong> (1980). Visual information processing: the structure and creation of visual representations. <em>Philosophical Transactions of the Royal Society B</em>, 290, 199-218.</p> </li> <li> <p><strong>Marr, D., &amp; Ullman, S.</strong> (1981). Directional selectivity and its use in early visual processing. <em>Proceedings of the Royal Society B</em>, 211, 151-180.</p> </li> </ul> <h2 id="the-magnum-opus">The Magnum Opus</h2> <ul> <li> <p><strong>Marr, D.</strong> (1982). <em>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</em>. W. H. Freeman. [Published posthumously; reissued by MIT Press, 2010]</p> </li> <li> <p><strong>Marr, D., &amp; Vaina, L. M.</strong> (1982). Representation and recognition of the movements of shapes. <em>Proceedings of the Royal Society B</em>, 214, 501-524.</p> </li> </ul> <hr/> <h2 id="c-intellectual-evolution">C. Intellectual Evolution</h2> <p><strong>Phase 1 (1966–1972): Neuronal Reductionism</strong> Marr believed “<em>truth was essentially neuronal</em>“—that understanding circuit wiring would explain function. His cerebellar theory exemplified this approach.[<a href="https://www.thecognizer.com/post/on-the-shoulders-of-giants-David-%20Marr">thecognizer</a>]​</p> <p><strong>Phase 2 (1972–1976): Computational Awakening</strong> The MIT environment and collaboration with Poggio shifted his focus from “<em>how neurons connect</em>” to “<em>what computational problems they solve</em>”. This led to the tri-level framework.</p> <p><strong>Phase 3 (1977–1980): Vision as Information Processing</strong> Diagnosed with leukemia, Marr raced to complete his comprehensive theory. <em>Vision</em> synthesized eight years of work into a unified framework treating perception as computation.</p>]]></content><author><name></name></author><category term="comp-neursci"/><category term="math"/><summary type="html"><![CDATA[Briefly written]]></summary></entry><entry xml:lang="en"><title type="html">Neuromatch NeuroAI course</title><link href="https://sabertoaster.github.io/vi/blog/2026/neuromatch-neuroai-course/" rel="alternate" type="text/html" title="Neuromatch NeuroAI course"/><published>2026-01-08T12:00:00+00:00</published><updated>2026-01-08T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2026/neuromatch-neuroai-course</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2026/neuromatch-neuroai-course/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/neuroai_navigation-480.webp 480w,/assets/img/blog/neuroai_navigation-800.webp 800w,/assets/img/blog/neuroai_navigation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/neuroai_navigation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A brief map on NeuroAI prominant topics, organizing the field along two primary axes: Applied-to-Neuro (vertical) and curiosity-driven foundations (horizontal). <a href="https://neuroai.neuromatch.io/tutorials/intro.html">Src: Neuromatch</a> </div> <details><summary>Brief on those topics</summary> <p>Curiosity-driven: It represents the “bottom-up” approach to science where researchers explore questions because they are fundamentally interesting or unknown. The NeuroAI sphere is traditionally divided into <strong>two primary directions</strong>—using neuroscience to build better AI, and using AI to understand the brain—intersecting with <strong>applied</strong> and <strong>theoretical</strong> goals.</p> <p>The map categorizes these topics into four quadrants:</p> <h3 id="1-applied-ai-neuro-to-ai">1. Applied AI (Neuro $\to$ AI)</h3> <p>This sector mines biological principles to create more robust and efficient artificial intelligence.</p> <ul> <li><strong>Biologically-Inspired Architectures:</strong> The field’s history is rooted here, from <strong>CNNs</strong> (mimicking the visual cortex) to <strong>Attention</strong> mechanisms (inspired by cognitive focus). Newer work includes <strong>Neural Style Transfer</strong> and <strong>Predictive Coding</strong>—architectures that anticipate inputs rather than just processing them.</li> <li><strong>Neurosymbolic AI:</strong> Approaches that combine the learning capability of neural networks with the logical reasoning of symbolic systems, aiming for “System 2” reasoning.</li> <li><strong>Geometric Deep Learning:</strong> Frameworks that process data on complex manifolds (like curved brain surfaces) rather than flat grids, helping models understand structure and invariances.</li> </ul> <h3 id="2-basic-neuroscience-ai-to-neuro">2. Basic Neuroscience (AI $\to$ Neuro)</h3> <p>This sector uses advanced AI as a “model organism” to test hypotheses about the brain.</p> <ul> <li><strong>AI as Brain Models:</strong> Specific AI architectures are used as functional proxies for brain regions. For example, <strong>CNNs</strong> model the ventral stream (object recognition), <strong>Transformers</strong> model the hippocampus (memory/spatial mapping), and <strong>Reinforcement Learning (RL)</strong> models the basal ganglia (reward processing).</li> <li><strong>Foundation Models for Neuro:</strong> Large-scale models (similar to GPT-4 but for neural data) trained on massive datasets of brain recordings (e.g., from thousands of neurons) to predict neural activity across different individuals and tasks.</li> <li><strong>AI Tooling:</strong> Using machine learning to automate tedious neuroscience tasks like <strong>spike sorting</strong> (classifying neuron firing) and <strong>computational ethology</strong> (tracking animal behavior).</li> </ul> <h3 id="3-theoretical-ai-curiosity-driven">3. Theoretical AI (Curiosity-Driven)</h3> <p>Research here focuses on the fundamental principles of intelligence, often abstracting away from immediate biological accuracy or commercial utility.</p> <ul> <li><strong>Spiking Neural Networks (SNNs):</strong> Neural networks that communicate via discrete “spikes” over time (like real neurons) rather than continuous values, offering extreme energy efficiency.</li> <li><strong>Animats &amp; Brain-AI Hybrids:</strong> “Animats” are simulated agents (or biological-synthetic hybrids) placed in virtual environments to evolve intelligence from scratch, often to study how physical embodiment affects learning.</li> <li><strong>Biologically Plausible Backprop:</strong> Searching for learning algorithms that real brains could actually support, as the brain likely does not use the standard “backpropagation” algorithm found in commercial AI.</li> </ul> <h3 id="4-clinical-neuroscience-applications">4. Clinical Neuroscience (Applications)</h3> <p>The direct medical application of NeuroAI technologies.</p> <ul> <li><strong>Digital Twins:</strong> Creating virtual replicas of a patient’s brain to simulate and optimize treatments before applying them physically.</li> <li><strong>Biomarkers:</strong> AI systems that detect subtle patterns in brain scans or behavior to diagnose conditions (like Alzheimer’s) earlier than human doctors can.</li> </ul> <h3 id="connecting-themes">Connecting Themes</h3> <p>At the center lie concepts like <strong>Curriculum Learning</strong> and <strong>Meta-Learning</strong>—techniques that help AI learn <em>how to learn</em> or learn in a structured sequence, mimicking human child development.</p> </details> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/neuroai_over_history-480.webp 480w,/assets/img/blog/neuroai_over_history-800.webp 800w,/assets/img/blog/neuroai_over_history-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/neuroai_over_history.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A figure of how topics evolved over time, from laying foundations to cutting-edge directions <a href="https://neuroai.neuromatch.io/tutorials/W1D1_Generalization/student/W1D1_Intro.html">Src: Neuromatch</a> </div> <hr/> <h2 id="what-is-intelligence">What is intelligence:</h2> <p>According to Patrick it was <code class="language-plaintext highlighter-rouge">Generalization ability</code>. It has multiple facets:</p> <ul> <li>Predict well in new circumstances</li> <li>Learn rapidly in new circumstances (sample complexity)</li> <li>Perform well in new circumstances</li> </ul> <details><summary>Here’s how predicting differs from performing</summary> <p>The first and last statements describe two distinct aspects of generalization in intelligence, differentiating between <strong>predictive accuracy</strong> and <strong>behavioral competence</strong>.</p> <h3 id="1-predict-well-in-new-circumstances-the-first-statement">1. Predict well in new circumstances (The First Statement)</h3> <p>This refers to the <strong>cognitive or informational</strong> aspect of intelligence. It is about the internal model’s ability to minimize surprise or error when facing novel data.</p> <ul> <li><strong>Focus:</strong> Accuracy of the internal model.</li> <li><strong>Mechanism:</strong> Recognizing patterns in unseen data (e.g., a “dog” seen from a new angle is still recognized as a “dog”).</li> <li><strong>Metric:</strong> Low prediction error or high classification accuracy on test sets.</li> </ul> <h3 id="2-perform-well-in-new-circumstances-the-last-statement">2. Perform well in new circumstances (The Last Statement)</h3> <p>This refers to the <strong>agentic or behavioral</strong> aspect of intelligence. It is about the ability to execute successful actions to achieve a goal in a novel environment.</p> <ul> <li><strong>Focus:</strong> Efficacy of actions and outcomes.</li> <li><strong>Mechanism:</strong> Adapting a policy or strategy to navigate a new situation (e.g., a robot walking on ice for the first time without falling).</li> <li><strong>Metric:</strong> High reward, survival, or task completion.</li> </ul> <p><strong>Summary of Difference:</strong> The first statement is about <strong>knowing</strong> what will happen (prediction), while the last statement is about <strong>doing</strong> the right thing (performance). You can predict well (know you are about to crash) without performing well (avoiding the crash). True general intelligence requires both.</p> </details> <hr/>]]></content><author><name></name></author><category term="computational-neuroscience"/><summary type="html"><![CDATA[In a nutshell]]></summary></entry><entry xml:lang="en"><title type="html">How to start Computational Neuroscience</title><link href="https://sabertoaster.github.io/vi/blog/2025/incf-open-neuroscience/" rel="alternate" type="text/html" title="How to start Computational Neuroscience"/><published>2025-08-01T12:00:00+00:00</published><updated>2025-08-01T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2025/incf-open-neuroscience</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2025/incf-open-neuroscience/"><![CDATA[<p>Disclaimer: I didn’t own any of the contents and it all belongs to the rightful owner credited/ mentioned/ referenced at the end of the post.</p> <blockquote> <p>Purpose of the study track is to provide a convenient and guided starting point to acquire the knowledge and skills required for open neuroscience.</p> </blockquote> <hr/> <h2 id="brief-introduction">Brief introduction</h2> <details><summary>Research in the neurosciences is becoming ever more demanding of a variety of sophisticated technical skills and computational competence, especially when one factors in the objective of making this science reproducible, open, and FAIR.</summary> <blockquote> <p>In collaboration with the INCF, the Canadian Open Neuroscience Platform (CONP) is assembling a curated set of international content that aims to provide guidance through the increasingly complex landscape of skills and tools required for open neuroscience research. Such initiatives are key to facilitating the acquisition of the skills and knowledge comprising open-science workflows (from ‘open-by-design’ experimental conception, through reproducible analysis, to safe data sharing). This is a living collection, with many materials to be added and updated still. <br/> The CONP is funded by a Brain Canada Platform Support Grant Competition Award, as well as funds and in-kind support from sponsor organizations. Please visit the <a href="https://conp.ca/">CONP</a> and <a href="https://braincanada.ca/">Brain Canada</a> websites linked below for more information.</p> </blockquote> </details> <p>The track is divided into 3 seperate parts:</p> <ul> <li><a href="https://training.incf.org/collection/data-science-tools-trade">Data Science - Tools of the Trade</a>: <br/> This collection looks to introduce neuroscience trainees to many of the basic tools and techniques essential for most computationally intensive neuroscience research environments.</li> <li><a href="https://training.incf.org/collection/statistics-machine-learning">Techniques on Statistics and Machine Learning</a>: <br/> These courses will introduce the basics of powerful machine learning techniques and the elements of traditional statistical approaches provide foundational knowledge for multivariate analyses.</li> <li><a href="https://training.incf.org/collection/standards-best-practices">Standard &amp; Best Practices</a>: <br/> This collection of courses and lessons intends to provide resources for standards and best practices in Open Science, Publishing, Ethics, and more.</li> </ul> <hr/> <h2 id="data-science---tools-of-the-trade"><a href="https://training.incf.org/collection/data-science-tools-trade">Data Science - Tools of the Trade</a></h2> <details><summary>Table of contents</summary> <ol> <li><a href="https://training.incf.org/course/conceptual-background-refreshers">Conceptual background &amp; refreshers</a> <ul> <li>Review of modelling</li> <li>Flash math refresher</li> <li>Models of neural function</li> <li>Overview of brain-imaging techniques</li> </ul> </li> <li><a href="https://training.incf.org/course/programming">Programming essentials</a> <ul> <li>Basics required for navigating command-line environments</li> <li>Python</li> <li>R</li> <li>Matlab/Octave</li> </ul> </li> <li><a href="https://training.incf.org/course/notebooks">Notebooks</a> <ul> <li>For teaching and learning</li> <li>As part of an everyday, scientific workflow</li> <li>As a complement to standard PDF publications</li> </ul> </li> <li><a href="https://training.incf.org/course/versioning-containerization">Versioning and containerization</a> <ul> <li>Overview</li> <li>Reproducibility</li> <li>Local execution</li> </ul> </li> <li><a href="https://training.incf.org/course/data-management-repositories-search-engines">Data Management, Repositories &amp; Search Engines</a> <ul> <li>The importance and utility of Research Data Management</li> <li>Sources and reuse of neuroscience data</li> </ul> </li> <li><a href="https://training.incf.org/course/data-management-repositories-search-engines">High-performance computing</a> <ul> <li>Case studies</li> <li>CLI environments (tools, scheduling, etc.)</li> <li>GUI environments</li> </ul> </li> </ol> </details> <h3 id="conceptual-background--refreshers">Conceptual background &amp; refreshers</h3> <hr/> <h2 id="statistics-and-machine-learning"><a href="https://training.incf.org/collection/statistics-machine-learning">Statistics and Machine Learning</a></h2> <details><summary>Table of contents</summary> <ol> <li><a href="https://training.incf.org/course/glm-regression-models-and-latent-variables">GLM, regression models, and latent variables</a> <ul> <li>Refresher for regressions models and GLM</li> <li>Addition of different noise distributions and advanced models</li> <li>Logistic regression</li> <li>Latent variables</li> </ul> </li> <li><a href="https://training.incf.org/course/machine-learning-conp">Machine learning</a> <ul> <li>Conceptual overview</li> <li>Hands-on application of simple machine learning to neuroscience data</li> <li>Advanced models</li> <li>Deep learning</li> <li>Caveats in deep learning applications to neuroscience</li> </ul> </li> <li><a href="https://training.incf.org/course/statistical-software">Statistical software</a> <ul> <li>scikit-learn</li> <li>nilearn</li> <li>JASP</li> </ul> </li> </ol> </details> <hr/> <h2 id="standard--best-practices"><a href="https://training.incf.org/collection/standards-best-practices">Standard &amp; Best Practices</a></h2> <details><summary>Table of contents</summary> <ol> <li><a href="https://training.incf.org/course/open-science-practices-and-policies">Open Science: Practices and Policies</a> <ul> <li>The Open Science Training Handbook</li> <li>Standards for Project Management and Organization</li> <li>Support Your Research With Data Management Planning!</li> </ul> </li> <li><a href="https://training.incf.org/course/ethics-and-governance">Ethics and Governance</a></li> <li><a href="https://training.incf.org/course/publishing">Publishing</a> (Still under development)</li> </ol> </details> <p>A few more things:</p> <ul> <li>From <a href="https://docs.neuromatch.io/p/CpPZ_P0Tl9vv6s/2025-Academy-Professional-Development">Neuromatch Academy 2025</a>: How to Build Data Pipelines for Neuroscience &amp; AI: https://youtu.be/PpDZgFyUlMQ</li> </ul> <hr/> <h2 id="references">References</h2> <p><a href="https://training.incf.org/studytrack/open-neuroscience-starter-kit">INCF Open Neuroscience Starter Kit</a></p>]]></content><author><name></name></author><category term="comp-neursci"/><summary type="html"><![CDATA[Notes and commentaries]]></summary></entry><entry xml:lang="en"><title type="html">Mathematics for Computational Neuroscience</title><link href="https://sabertoaster.github.io/vi/blog/2025/math-for-neuro/" rel="alternate" type="text/html" title="Mathematics for Computational Neuroscience"/><published>2025-08-01T12:00:00+00:00</published><updated>2025-08-01T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2025/math-for-neuro</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2025/math-for-neuro/"><![CDATA[<p>Hosted by Speaker: <a href="https://training.incf.org/taxonomy/term/160">Alex Williams</a></p> <h2 id="1-membrane-potential-introduction">1. <a href="https://training.incf.org/lesson/membrane-potential-introduction">Membrane Potential Introduction</a></h2> <details><summary>This lesson introduces the membrane potential equation</summary> <h4 id="what-is-membrane-potential">What is Membrane Potential?</h4> <p>The membrane potential ($V_m$) is the voltage difference across a neuron’s membrane, defined as $V_m = V_i - V_e$, where $V_i$ is the intracellular potential and $V_e$ is the extracellular potential.</p> <ul> <li>It’s measured in millivolts (mV), with the inside of the cell typically being negative relative to the outside (e.g., ~-70 mV at rest).</li> <li>This potential arises from the asymmetric distribution of ions like Sodium (Na+), Potassium (K+), and Chloride (Cl-) across the membrane, a state maintained by active ion pumps.</li> </ul> <h4 id="electrical-properties-of-the-membrane">Electrical Properties of the Membrane</h4> <p>To model the cell membrane, we can abstract its physical properties into an equivalent electrical circuit:</p> <ol> <li><strong>Capacitor (<code class="language-plaintext highlighter-rouge">insulator</code>)</strong>: The lipid bilayer separates charges, acting like a capacitor that can store charge. This is crucial for generating action potentials.</li> <li><strong>Resistors (<code class="language-plaintext highlighter-rouge">conductors</code>)</strong>: Ion channels allow ions to move across the membrane, functioning as resistors.</li> <li><strong>Voltage Source (Battery)</strong>: The concentration gradients of ions create an electrochemical potential, which acts as a battery.</li> </ol> <p>This leads to the simplest model of a cell membrane patch as an RC circuit:</p> <pre><code class="language-mermaid">---
title: Simplest Cellular Membrane Circuit
---
graph TD
    subgraph "Inside (V_i)"
        A --- C
    end
    subgraph "Outside (V_e)"
        B --- D
    end

    subgraph "Membrane"
        C -- C_m --&gt; D
        C -- R_m --&gt; E
        E -- E_L --&gt; D
    end

    A -- V_m --- B

    linkStyle 3 stroke-width:0px,fill:none;
</code></pre> <h4 id="governing-equations">Governing Equations</h4> <ul> <li><strong><a href="https://www.physiologyweb.com/calculators/nernst_potential_calculator.html">Nernst equation</a></strong>: Calculates the equilibrium potential for a single ion species.</li> <li><strong>Goldman equation</strong>: An extension of the Nernst equation for multiple ion species, used to calculate the resting membrane potential.</li> </ul> </details> <hr/> <h2 id="2-the-membrane-equation-passive-neuron">2. The Membrane Equation (Passive Neuron)</h2> <h2 id="3-separation-of-variables-solving-passive-membrane">3. Separation of Variables (Solving Passive Membrane)</h2> <p>Solving the passive membrane equation</p> <h2 id="4-injecting-current-into-a-passive-membrane">4. Injecting Current Into a Passive Membrane</h2> <p>Injecting current into a passive membrane</p> <h2 id="5-response-to-a-current-step">5. Response to a Current Step</h2> <p>Response to injected current</p> <h2 id="6-numerically-solving-the-membrane-equation">6. Numerically Solving the Membrane Equation</h2> <p>Explains the logic behind dealing with more complex currents by solving the membrane equation numerically.</p> <h2 id="7-intro-to-conductance-based-models">7. Intro to Conductance-Based Models</h2> <p>Introducing voltage-dependent ion channels into the passive membrane</p> <h2 id="8-hodgkin-huxley-channel-models">8. Hodgkin Huxley Channel Models</h2> <p>Introducing Hodgkin &amp; Huxley’s voltage dependent ion channel models, with emphasis on the sodium conductance</p> <h2 id="9-hodgkin-huxley-squid-axon-model">9. Hodgkin-Huxley Squid Axon Model</h2> <p>Introducing the classical Hodgkin &amp; Huxley squid axon model with sodium and potassium conductances</p> <h2 id="10-multi-compartment-conductance-based-models">10. Multi-Compartment Conductance-Based Models</h2> <p>This lesson extends the conductance-based model equation to multiple neuronal compartments, taking more complex morphology into account.</p> <h3 id="references">References</h3> <p><a href="https://training.incf.org/course/basic-mathematics-computational-neuroscience">INCF Basic Mathematics for Computational Neuroscience</a></p>]]></content><author><name></name></author><category term="comp-neursci"/><category term="math"/><summary type="html"><![CDATA[Notes and commentaries]]></summary></entry><entry xml:lang="en"><title type="html">Learning Resources</title><link href="https://sabertoaster.github.io/vi/blog/2025/awesome-learning-path/" rel="alternate" type="text/html" title="Learning Resources"/><published>2025-07-01T12:00:00+00:00</published><updated>2025-07-01T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2025/awesome-learning-path</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2025/awesome-learning-path/"><![CDATA[<blockquote> <p><em>An attempt of an undergrad to create a curated repository of computational neuroscience and brain-inspired artificial intelligence resources, which can also including Data Science, pure Math, Machine Learning, Deep Learning, and not constrictively limiting beyond those topics. Leave a star if you believe I can make it, or follow me if you don’t.</em></p> </blockquote> <div style="float: right"> SaberToaster, 2025 </div> <p><br/></p> <h2 id="learning-resources">Learning Resources</h2> <details><summary>MOOCs &amp; Online Courses</summary> <table> <thead> <tr> <th>Course</th> <th>Provider/Author</th> <th>Focus</th> <th>Best For</th> <th>Access</th> </tr> </thead> <tbody> <tr> <td><strong>Computational Neuroscience</strong></td> <td>Coursera</td> <td>Mathematical foundations of neural computation</td> <td>CS students transitioning to neuro</td> <td><a href="https://www.coursera.org/learn/computational-neuroscience">Course</a></td> </tr> <tr> <td><strong>Neuroscience for Machine Learners</strong></td> <td>Neuromatch Academy</td> <td>CS-friendly intro to neuroscience</td> <td>ML practitioners</td> <td><a href="https://neuro4ml.github.io/">Course</a> • <a href="https://youtube.com/playlist?list=PL09WqqDbQWHErc8xOyWdKpNEk78Jjk0EL">Videos</a></td> </tr> <tr> <td><strong>Machine Learning Specialization</strong></td> <td>Coursera</td> <td>Standard ML foundations</td> <td>Beginners</td> <td><a href="https://www.coursera.org/specializations/machine-learning-introduction">Course</a></td> </tr> <tr> <td><strong>Deep Learning Specialization</strong></td> <td>Coursera</td> <td>Neural networks and architectures</td> <td>Intermediate practitioners</td> <td><a href="https://www.coursera.org/specializations/deep-learning">Course</a></td> </tr> <tr> <td><strong>Predictive Brain Lab Resources</strong></td> <td>MIT</td> <td>Heavy NeuroAI focus</td> <td>Advanced researchers</td> <td><a href="https://predictive-brain-lab.github.io/Lab-Wiki/documents/neuro-ai/neuroai-resources.html">Resources</a></td> </tr> <tr> <td><strong>Computational Neuroscience 2020</strong></td> <td>Michelle R. Greene, Ph.D</td> <td>Self-paced neuroscience curriculum</td> <td>Self-directed learners</td> <td><a href="https://mrgreene09.github.io/compNeuro2020/">Course</a></td> </tr> </tbody> </table> </details> <details><summary>Essential Books &amp; Textbooks</summary> <h5 id="mathematical-foundations">Mathematical Foundations</h5> <table> <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>Focus</th> <th>Best For</th> <th>Access</th> </tr> </thead> <tbody> <tr> <td><strong>Neuronal Dynamics</strong></td> <td>Wulfram Gerstner et al</td> <td>2014</td> <td>Mathematical models of neural dynamics</td> <td>Physics background</td> <td><a href="https://neuronaldynamics.epfl.ch/">Free Online</a></td> </tr> <tr> <td><strong>Theoretical Neuroscience</strong></td> <td>Dayan &amp; Abbott</td> <td>2001</td> <td>Mathematical framework for computation</td> <td>CS + Physics students</td> <td>Essential textbook</td> </tr> <tr> <td><strong>Dynamical Systems in Neuroscience</strong></td> <td>Izhikevich</td> <td>2007</td> <td>Mathematical models of neuronal behavior</td> <td>Advanced mathematical focus</td> <td>Standard reference</td> </tr> <tr> <td><strong>Neural Engineering</strong></td> <td>Eliasmith &amp; Anderson</td> <td>2003</td> <td>Neural representation principles (NEF)</td> <td>Engineering approach</td> <td>NEF methodology foundation</td> </tr> </tbody> </table> <h5 id="aiml-references">AI/ML References</h5> <table> <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>Focus</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><strong>Deep Learning</strong></td> <td>Goodfellow et al.</td> <td>2016</td> <td>Comprehensive DL textbook</td> <td>Standard reference</td> </tr> <tr> <td><strong>Attention Is All You Need</strong></td> <td>Vaswani et al.</td> <td>2017</td> <td>Transformer architecture</td> <td>Revolutionary paper</td> </tr> </tbody> </table> </details> <details><summary>Blogs &amp; Personal Resources</summary> <table> <thead> <tr> <th>Author</th> <th>Affiliation</th> <th>Focus</th> <th>Why Follow</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>Christopher Olah</strong></td> <td>Anthropic AI co-founder</td> <td>Neural network interpretability</td> <td>Clear explanations of complex concepts</td> <td><a href="https://colah.github.io/">Blog</a></td> </tr> <tr> <td><strong>Aman Chadha</strong></td> <td>AWS GenAI Chief Research Scientist</td> <td>AI research and applications</td> <td>Industry perspective on cutting-edge research</td> <td><a href="https://aman.ai/">Homepage</a></td> </tr> <tr> <td><strong>Charles Frye</strong></td> <td>Helen Wills Neuroscience Institute</td> <td>Computational neuroscience</td> <td>Academic insights bridging theory and practice</td> <td><a href="https://charlesfrye.github.io/about/">Homepage</a> • <a href="https://charlesfrye.github.io/FoundationalNeuroscience/">CNS</a></td> </tr> </tbody> </table> </details> <details><summary>YouTube Channels</summary> <table> <thead> <tr> <th>Channel</th> <th>Creator</th> <th>Focus</th> <th>Why Watch</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>3Blue1Brown</strong></td> <td>Grant Sanderson</td> <td>Mathematical visualizations</td> <td>Best math/ML concept explanations</td> <td><a href="https://www.youtube.com/@3blue1brown">Channel</a></td> </tr> <tr> <td><strong>Artem Kirsanov</strong></td> <td>Artem Kirsanov</td> <td>Computational neuroscience animations</td> <td>Covers most CNS concepts with clear animations</td> <td><a href="https://www.youtube.com/@ArtemKirsanov">Channel</a></td> </tr> <tr> <td><strong>Deepia</strong></td> <td>Various</td> <td>AI/ML visualizations</td> <td>Cool visualization techniques</td> <td><a href="https://www.youtube.com/@Deepia-ls2fo">Channel</a></td> </tr> <tr> <td><strong>Computerphile</strong></td> <td>University team</td> <td>Computer science concepts</td> <td>CS fundamentals relevant to neurocomputation</td> <td><a href="https://www.youtube.com/@Computerphile">Channel</a></td> </tr> </tbody> </table> </details> <hr/> <h2 id="research-literature">Research Literature</h2> <details><summary>Mathematical &amp; Computational Foundations</summary> <table> <thead> <tr> <th>Paper/Book</th> <th>Authors</th> <th>Year</th> <th>Key Contribution</th> <th>Impact</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>What the Frog’s Eye Tells the Frog’s Brain</strong></td> <td>Lettvin et al.</td> <td>1959</td> <td>Feature detection in visual system</td> <td>Classic computational neuro</td> <td> </td> </tr> <tr> <td><strong>Computational neuroscience: a frontier</strong></td> <td>Multiple</td> <td>2020</td> <td>State of the field overview</td> <td>Recent comprehensive review</td> <td><a href="https://academic.oup.com/nsr/article/7/9/1418/5856589">PDF</a></td> </tr> </tbody> </table> <p><em>Note: Mathematical foundations table to be expanded with specific papers on neural dynamics, information theory, and computational methods.</em></p> </details> <details><summary>NeuroAI Integration Papers</summary> <table> <thead> <tr> <th>Paper</th> <th>Authors</th> <th>Year</th> <th>Key Contribution</th> <th>Research Area</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><strong>The Genomic Bottleneck</strong></td> <td>Zador</td> <td>2019</td> <td>Constraints on innate vs learned computation</td> <td>Evolutionary computation</td> <td>Influential perspective paper</td> </tr> <tr> <td><strong>Brain-Inspired AI</strong></td> <td>Hassabis et al.</td> <td>2017</td> <td>Neuroscience-AI bidirectional influence</td> <td>AI methodology</td> <td>DeepMind perspective</td> </tr> <tr> <td><strong>Surrogate Gradient Learning in SNN</strong></td> <td>Neftci et al.</td> <td>2019</td> <td>Training spiking neural networks</td> <td>SNN algorithms</td> <td>Key training methodology</td> </tr> <tr> <td><strong>Deep Learning with Spiking Neurons</strong></td> <td>Pfeiffer &amp; Pfeil</td> <td>2018</td> <td>Neuromorphic deep learning approaches</td> <td>Hardware-software co-design</td> <td>Practical implementation focus</td> </tr> <tr> <td><strong>Towards Spike-Based Machine Intelligence</strong></td> <td>Roy et al.</td> <td>2019</td> <td>Comprehensive SNN survey</td> <td>SNN overview</td> <td>State-of-the-art review</td> </tr> </tbody> </table> </details> <details><summary>Specialized Topics</summary> <h5 id="spiking-neural-networks">Spiking Neural Networks</h5> <p><em>Papers on temporal dynamics, energy efficiency, and biological realism</em></p> <h5 id="hopfield-networks">Hopfield Networks</h5> <p><em>Classic associative memory models and modern variants</em></p> <h5 id="attention-mechanisms">Attention Mechanisms</h5> <p><em>Biological inspiration and computational implementations</em></p> <p><em>These sections to be populated with specific paper recommendations</em></p> </details> <hr/> <h2 id="academic--research-community">Academic &amp; Research Community</h2> <details><summary>Research Institutions &amp; Labs</summary> <h5 id="north-america">North America</h5> <table> <thead> <tr> <th>Institution</th> <th>Key Researchers</th> <th>Research Focus</th> <th>Location</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>MIT McGovern Institute</strong></td> <td>Multiple PIs</td> <td>Computational neuroscience</td> <td>Cambridge, MA</td> <td> </td> </tr> <tr> <td><strong>Stanford Wu Tsai</strong></td> <td>Multiple PIs</td> <td>Neural computation and AI</td> <td>Stanford, CA</td> <td> </td> </tr> <tr> <td><strong>Allen Institute</strong></td> <td>Christof Koch</td> <td>Large-scale brain mapping</td> <td>Seattle, WA</td> <td> </td> </tr> <tr> <td><strong>HHMI Janelia</strong></td> <td>Multiple PIs</td> <td>Neural circuits and computation</td> <td>Ashburn, VA</td> <td> </td> </tr> <tr> <td><strong>Cold Spring Harbor Laboratory</strong></td> <td>Multiple PIs</td> <td>NeuroAI internships + postdocs</td> <td>NYC, NY</td> <td><a href="https://www.cshl.edu/research/neuroscience/neuroai/">NeuroAI Program</a></td> </tr> </tbody> </table> <h5 id="europe">Europe</h5> <table> <thead> <tr> <th>Institution</th> <th>Key Researchers</th> <th>Research Focus</th> <th>Location</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>EPFL (Computational Neuroscience)</strong></td> <td>Wulfram Gerstner</td> <td>Mathematical neuroscience</td> <td>Switzerland</td> <td><a href="https://www.epfl.ch/labs/lcn/">LCN Lab</a></td> </tr> <tr> <td><strong>EPFL (NeuroAI)</strong></td> <td>Martin Schrimpf</td> <td>Brain-AI alignment</td> <td>Switzerland</td> <td><a href="https://www.epfl.ch/labs/schrimpflab/">Schrimpf Lab</a></td> </tr> <tr> <td><strong>ETH Zurich INI</strong></td> <td>Multiple PIs</td> <td>Neuromorphic engineering</td> <td>Switzerland</td> <td> </td> </tr> <tr> <td><strong>DeepMind Neuroscience</strong></td> <td>Multiple researchers</td> <td>Brain-inspired AI</td> <td>London, UK</td> <td> </td> </tr> <tr> <td><strong>IT:U</strong></td> <td>Jie Mei</td> <td>Neuromodulatory mechanisms</td> <td>Austria</td> <td><a href="https://it-u.at/en/research/research-groups/computational-neuroscience/">Computational Neuroscience</a></td> </tr> </tbody> </table> <h5 id="asia">Asia</h5> <table> <thead> <tr> <th>Institution</th> <th>Key Researchers</th> <th>Research Focus</th> <th>Location</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>KWANGWOON University</strong></td> <td>Young-Seok Choi</td> <td>Neuroengineering and AI</td> <td>Korea</td> <td><a href="https://sites.google.com/view/neuroailab/home">NeuroAI Lab</a></td> </tr> <tr> <td><strong>Shanghai Jiao Tong University</strong></td> <td>Ru-Yuan Zhang</td> <td>Cognitive computational neuroscience</td> <td>Shanghai, China</td> <td><a href="https://ruyuanzhang.github.io/index.html">Zhang Lab</a></td> </tr> </tbody> </table> </details> <details><summary>Journals &amp; Conferences</summary> <h5 id="high-impact-journals">High-Impact Journals</h5> <table> <thead> <tr> <th>Journal</th> <th>Type</th> <th>Focus</th> <th>Impact Factor</th> <th>Submission Focus</th> </tr> </thead> <tbody> <tr> <td><strong>Nature Neuroscience</strong></td> <td>Journal</td> <td>High-impact neuroscience research</td> <td>&gt;20</td> <td>Breakthrough discoveries</td> </tr> <tr> <td><strong>Neuron</strong></td> <td>Journal</td> <td>Cellular and systems neuroscience</td> <td>&gt;15</td> <td>Mechanistic insights</td> </tr> <tr> <td><strong>Neural Computation</strong></td> <td>Journal</td> <td>Computational theory</td> <td>~3</td> <td>Mathematical models</td> </tr> </tbody> </table> <h5 id="open-access-venues">Open-Access Venues</h5> <table> <thead> <tr> <th>Venue</th> <th>Type</th> <th>Focus</th> <th>Impact Factor</th> <th>Why Submit Here</th> </tr> </thead> <tbody> <tr> <td><strong>PLoS Computational Biology</strong></td> <td>Journal</td> <td>Computational biology</td> <td>~4</td> <td>Open access, broad reach</td> </tr> <tr> <td><strong>Frontiers in Computational Neuroscience</strong></td> <td>Journal</td> <td>Computational approaches</td> <td>~3</td> <td>Fast review, open access</td> </tr> </tbody> </table> <h5 id="key-conferences">Key Conferences</h5> <table> <thead> <tr> <th>Conference</th> <th>Full Name</th> <th>Focus</th> <th>Venue Type</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><strong>NeurIPS</strong></td> <td>Neural Information Processing Systems</td> <td>ML/AI with neuroscience connections</td> <td>Annual</td> <td>Premier ML conference</td> </tr> <tr> <td><strong>COSYNE</strong></td> <td>Computational and Systems Neuroscience</td> <td>Computational neuroscience</td> <td>Annual</td> <td>Theory-focused</td> </tr> <tr> <td><strong>ICLR</strong></td> <td>International Conference on Learning Representations</td> <td>Representation learning</td> <td>Annual</td> <td>Rising importance in AI</td> </tr> <tr> <td><strong>CNS</strong></td> <td>Cognitive Neuroscience Society</td> <td>Cognitive neuroscience</td> <td>Annual</td> <td>Experimental focus</td> </tr> <tr> <td><strong>ICMNAI</strong></td> <td>International Conference on Mathematics of Neuroscience and AI</td> <td>Mathematics intersection</td> <td>Annual</td> <td><a href="https://www.neuromonster.org/contact">Website</a></td> </tr> </tbody> </table> </details> <hr/> <h2 id="computational-tools--software">Computational Tools &amp; Software</h2> <details><summary>Neural Simulation Platforms</summary> <table> <thead> <tr> <th>Tool</th> <th>Category</th> <th>Description</th> <th>Language</th> <th>Best For</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>Brian2</strong></td> <td>SNN Simulation</td> <td>Clock-driven neural network simulator</td> <td>Python</td> <td>Research, education</td> <td> </td> </tr> <tr> <td><strong>NEURON</strong></td> <td>Detailed Modeling</td> <td>Biophysically detailed neuron models</td> <td>Python/C++</td> <td>Detailed biophysical models</td> <td> </td> </tr> <tr> <td><strong>STEPS</strong></td> <td>Spatial Modeling</td> <td>Spatial stochastic simulation</td> <td>Python/C++</td> <td>Subcellular processes</td> <td> </td> </tr> </tbody> </table> </details> <details><summary>Analysis &amp; Visualization Tools</summary> <table> <thead> <tr> <th>Tool</th> <th>Category</th> <th>Description</th> <th>Language</th> <th>Best For</th> <th>URL</th> </tr> </thead> <tbody> <tr> <td><strong>DeepLabCut</strong></td> <td>Behavioral Analysis</td> <td>Markerless pose estimation</td> <td>Python</td> <td>Animal behavior tracking</td> <td> </td> </tr> <tr> <td><strong>PyTorch</strong></td> <td>Deep Learning</td> <td>Research-focused ML framework</td> <td>Python</td> <td>NeuroAI model development</td> <td> </td> </tr> <tr> <td><strong>TensorFlow</strong></td> <td>Deep Learning</td> <td>Production-focused ML framework</td> <td>Python</td> <td>Deployment and large-scale apps</td> <td> </td> </tr> </tbody> </table> </details> <hr/> <p><em>This resource hub is community-maintained. Feel free to contribute through PRs or suggestions for additional resources, corrections, or organizational improvements.</em></p>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="computational-neuroscience"/><category term="mathematics"/><summary type="html"><![CDATA[AI, Math and Neuroscience]]></summary></entry><entry xml:lang="en"><title type="html">Harvard CS197 AI Research Experiences</title><link href="https://sabertoaster.github.io/vi/blog/2025/harvard-cs197/" rel="alternate" type="text/html" title="Harvard CS197 AI Research Experiences"/><published>2025-07-01T12:00:00+00:00</published><updated>2025-07-01T12:00:00+00:00</updated><id>https://sabertoaster.github.io/blog/2025/harvard-cs197</id><content type="html" xml:base="https://sabertoaster.github.io/blog/2025/harvard-cs197/"><![CDATA[<p>Disclaimer: I didn’t own any of the contents and it all belongs to the rightful owner credited/ mentioned/ referenced at the end of the post.</p> <h2 id="brief-introduction">Brief introduction</h2> <p>This is a course ran by Harvard, instructed by Professor <a href="https://pranavrajpurkar.com/">Pranav Rajpurkar</a>.</p> <blockquote> <p>Dive into cutting-edge development tools like PyTorch, Lightning, and Hugging Face, and streamline your workflow with VSCode, Git, and Conda. You’ll learn how to harness the power of the cloud with AWS and Colab to train massive deep learning models with lightning-fast GPU acceleration. Plus, you’ll master best practices for managing a large number of experiments with Weights and Biases. And that’s just the beginning! This course will also teach you how to systematically read research papers, generate new ideas, and present them in slides or papers. You’ll even learn valuable project management and team communication techniques used by top AI researchers. Don’t miss out on this opportunity to level up your AI skills.</p> </blockquote> <hr/> <h2 id="chapter-1--2-introduction-to-ai-and-setting-up-environment">Chapter <a href="https://docs.google.com/document/u/0/d/1FHnGGGhTTarovEAVzSfELlNvxhXFJV4DkpuGgMKaEGw/edit">1</a> &amp; <a href="https://docs.google.com/document/u/0/d/1z5ELxpTw_U01jUB6-D6ILqHRPg6SSiLE7VFQryH3LPU/edit">2</a>: Introduction to AI and Setting up environment</h2> <p>Typical, intuitive guides to interact with AI and its research + development. Working environment requires source-code version control (Git), environment control (Conda) and editors (VSCode).</p> <hr/> <h2 id="chapter-3-reading-ai-research-papers"><a href="https://docs.google.com/document/d/1bPhwNdCCKkm1_adD0rx1YV6r2JG98qYmTxutT5gdAdQ/edit?tab=t.0">Chapter 3: Reading AI Research Papers</a></h2> <h3 id="ch3-objectives"><u>Objectives</u>:</h3> <ul> <li>Conduct a literature search to identify papers relevant to a topic of interest</li> <li>Read a machine learning research paper and summarize its contributions</li> </ul> <h3 id="ch3-content"><u>Chapter's content</u>:</h3> <details><summary>TLDR: Read wide for learning, read deep for improving. Take incremental approach.</summary> <blockquote> <p>I’m going to break down the process of reading AI research papers into two pieces: reading wide, and reading deep. <em><span style="color:red">When you start learning about a new topic, you typically get more out of reading wide</span></em>: this means navigating through literature reading small amounts of individual research papers. Our goal when reading wide is to build and improve our mental model of a research topic. <em><span style="color:red">Once you have identified key works that you want to understand well in the first step, you will want to read deep</span></em>: here, you are trying to read individual papers in depth. Both reading wide and deep are necessary and complimentary, especially when you’re getting started.</p> </blockquote> <h4 id="read-wide">Read wide:</h4> <p>Paperswithcode (AI/ML) | Google scholar | ACM Digital Lib &gt; Taking notes &gt; Check benchmark with recent submissions &gt; Check SOTA &gt; Check dataset (in any order) &gt; Read em and take notes. <br/> Results: Notes condensed with information, may take up to 2-3 hours doing this.</p> <blockquote> <p>At this point, I find myself particularly intrigued by the SOTA methods: Why are they achieving high performance? According to the review paper, it looks like “training strategies using pre-training” have been an advance. Maybe that’s worth keeping an eye out for!</p> </blockquote> <h4 id="read-deep">Read deep:</h4> <blockquote> <p>So I would like you to take an incremental approach here. Understand that, in your first pass, you will not understand more than 10% of the research paper. The paper may require us to read another more fundamental paper (which might require reading a third paper and so on; it could be turtles all the way down)! Then, in your second pass, you might understand 20% of the paper. Understanding 100% of a paper might require a significant leap, maybe because it’s poorly written, insufficiently detailed, or simply too technically/mathematically advanced. We thus want to aim to build up to understanding as much of the paper as possible – I’ll bet that 70-80% of the paper is a good target.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec3-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec3-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025/07/harvard-cs197-lec3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Define your own highlight strategy, for example:</p> <ul> <li>The <span style="color:yellow">yellow highlights</span> are the problems/challenges</li> <li>The <span style="color:pink">pink highlights</span> are the solutions to the challenges</li> <li>The <span style="color:orange">orange highlights</span> are the main contributions of the work we’re reading.</li> </ul> <h3 id="ch3-tips"><u>Some useful tips</u>:</h3> <ul> <li> <p>Read deep: do a <a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">three-pass approach</a>. <br/><br/> First, take a quick scan through the title, put some brief thought into it. Read the abstract, introduction, section headings. <strong>Identify the research question, the thematics, the papers’ contribution.</strong> Look through the figures for visual details. Glance through <strong>discussion</strong> if exists. Read the conclusion and see if the research question have been solved (?) <br/><br/> Second, focus on the introduction content, preliminary. Grasp the underlying content in <strong>methodology</strong> section and the <strong>results</strong>. Take notes on key points. Skip math terms if too heavy. The goal is to understand the paper’s content without getting bogged down in details. <br/><br/> This pass is a thorough, line-by-line reading with the aim of understanding every detail and challenging the author’s assumptions and claims. This pass is particularly useful for papers that require a deep understanding or are central. <strong>Re-implement</strong> the paper’s work/ <strong>identify areas for improvement</strong>.</p> </li> <li> <p>Read wide: If <strong>doing a literature review/ search, just do the first pass</strong> then look for referenced sources, mentioned SOTA methods, prominent proposed dataset/ benchmark, …</p> </li> <li> <blockquote> <p>You would have thus created a list of concepts you need to learn about, and the relevant paper for each, if the paper specifies any.</p> </blockquote> </li> </ul> </details> <hr/> <h2 id="chapter-4-fine-tuning-a-language-model-using-hugging-face"><a href="https://docs.google.com/document/d/18mTeEk1zfJDz-oY48oB96ryZRuiAWRbCLOMYKap8eXk/edit?tab=t.0">Chapter 4: Fine-tuning a Language Model using Hugging Face</a></h2> <h3 id="ch4-objectives"><u>Objectives</u>:</h3> <ul> <li>Load up and process a natural language processing dataset using the datasets library.</li> <li>Tokenize a text sequence, and understand the steps used in tokenization.</li> <li>Construct a dataset and training step for causal language modeling.</li> </ul> <h3 id="ch4-content"><u>Chapter's content</u>:</h3> <details><summary>TLDR: HuggingFace’s Dataloader -&gt; Tokenize -&gt; Train/Test split -&gt; Train and Evaluate -&gt; Upload to model hub</summary> <h4 id="huggingface">HuggingFace</h4> <ul> <li>Community/ Data science center for building, training and deploying ML models based on open src software.</li> <li><a href="https://huggingface.co/docs/transformers/tasks/language_modeling">Guide 1: Causal language mdeoling</a> and <a href="https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb">Guide 2: Code </a> on how to fine tune.</li> </ul> <h4 id="load-up-a-dataset">Load up a dataset</h4> <p>HF’s <code class="language-plaintext highlighter-rouge">Datasets library</code>’s 3 main feats:</p> <ul> <li>Load/ process data from CSV/JSON/text or python dict/pandas.DataFrame</li> <li>Access/ share datasets (through HF’s hub)</li> <li>Can be used with DL frameworks (pandas, numpy, pytorch/ tensorflow) (interoperable - new vocab hehe)</li> </ul> <p><a href="https://huggingface.co/datasets/rajpurkar/squad">SQuAD</a> (Stanford Question Answering Dataset) dataset:</p> <ul> <li>Reading comprehension</li> <li>Questions posed by crowdworkers on a set of Wikipedia articles</li> <li>Answer is a segment of text, span from corresponding reading passage; or unanswerable.</li> </ul> <h4 id="tokenize">Tokenize</h4> <p>A tokenization pipeline in huggingface comprises several <a href="https://huggingface.co/course/chapter6/8?fw=pt">steps</a>:</p> <ul> <li>(1) Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)</li> <li>(2) Pre-tokenization (splitting the input into words)</li> <li>(3) Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)</li> <li>(4) Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs).</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>The above steps show how we can go from text into tokens. There are multiple rules that govern the process that are specific to certain models. For tokenization, there are three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others); we won’t go into any of these, but if you’re curious, you can learn about them <a href="https://huggingface.co/course/chapter6/4?fw=pt#algorithm-overview">here</a>.</p> </blockquote> <h4 id="data-processing">Data Processing</h4> <blockquote> <p>For causal language modeling (CLM), one of the data preparation steps often used is to concatenate the different examples together, and then split them into chunks of equal size. This is so that we can have a common length across all examples without needing to pad.</p> </blockquote> <p>From <br/>[<code class="language-plaintext highlighter-rouge">"I went to the yard.&lt;|endoftext|&gt;"</code>,<code class="language-plaintext highlighter-rouge">"You came here a long time ago from the west coast.&lt;|endoftext|&gt;"</code>], <br/>we might change this to: <br/>[<code class="language-plaintext highlighter-rouge">"I went to the yard.&lt;|endoftext|&gt;You came here"</code>, <code class="language-plaintext highlighter-rouge">"a long time ago from the west coast.&lt;|endoftext|&gt;"</code>]</p> <h4 id="finetuning-and-setup-trainer">Finetuning and setup <a href="https://huggingface.co/docs/transformers/v4.21.3/en/main_classes/trainer#transformers.Trainer">Trainer</a></h4> <p>Code sample:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">distilgpt2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">small_train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">small_eval_dataset</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
</code></pre></div></div> <h4 id="model-evaluation">Model Evaluation</h4> <blockquote> <p>Because we want our model to assign high probabilities to sentences that are real, and low probabilities to fake sentences, we seek a model that assigns the highest probability to the test set. The metric we use is <a href="##" title="Investigate this more">‘perplexity’</a>, which we can think of as the inverse probability of the test set normalized by the number of words in the test set. Therefore, a lower perplexity is better.</p> </blockquote> <h4 id="generation-with-our-fine-tuned-model">Generation with our fine-tuned model</h4> <p>Code sample:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rajpurkar/</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rajpurkar/</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">)</span>
<span class="n">start_text</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">A speedrun is a playthrough of a video game, </span><span class="se">\
</span><span class="s">or section of a video game, with the goal of </span><span class="se">\
</span><span class="s">completing it as fast as possible. Speedruns </span><span class="se">\
</span><span class="s">often follow planned routes, which may incorporate sequence </span><span class="se">\
</span><span class="s">breaking, and might exploit glitches that allow sections to </span><span class="se">\
</span><span class="s">be skipped or completed more quickly than intended. </span><span class="sh">"</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is the</span><span class="sh">"</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
     <span class="n">start_text</span> <span class="o">+</span> <span class="n">prompt</span><span class="p">,</span>
     <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
     <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>

<span class="n">prompt_length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
     <span class="n">inputs</span><span class="p">,</span>
     <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
     <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
     <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
     <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
     <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
     <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">prompt_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>

<span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div> <p>Results:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec4-result-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec4-result-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec4-result-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/2025/07/harvard-cs197-lec4-result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </details> <hr/> <h2 id="chapter-5-fine-tuning-a-vision-transformer-using-lightning"><a href="https://docs.google.com/document/d/1VnNYGEmVgvl5p8w2xzypGySajaRv6qvzqw7E7LEwQKI/edit?tab=t.0">Chapter 5: Fine-tuning a Vision Transformer using Lightning</a></h2> <h3 id="ch5-objectives"><u>Objectives</u>:</h3> <details><summary>Learning outcomes: <ul> <li>Interact with code to explore data loading and tokenization of images for Vision Transformers.</li> <li>Parse code for PyTorch architecture and modules for building a Vision Transformer.</li> <li>Get acquainted with an example training workflow with PyTorch Lightning.</li> </ul></summary> <blockquote> <p>Reading code is often an effective way of learning. Today we will step through an image classification workflow with Vision transformers. We will parse code to process a computer vision dataset, tokenize inputs for vision transformers, and build a training workflow using the Lightning (PyTorch Lightning) framework. You might be used to learning about a new AI framework with simple tutorials first that build in complexity. However, in research settings, you’ll often be faced with using codebases that use unfamiliar frameworks. Our lecture today reflects this very setting, and is thus structured as a walkthrough where you will be exposed to code that uses Pytorch Lightning and then proceed to understand parts of it.</p> </blockquote> </details> <p>Example notebook:</p> <ul> <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/11-vision-transformer.html">Tutorial</a></li> <li><a href="https://github.com/rajpurkar/cs197-lec5">Finetuning a Vision Transformer using Lightning</a></li> </ul> <h3 id="ch5-content"><u>Chapter's content</u>:</h3> <details><summary>TLDR: Data Loading</summary> <h4 id="lightning">Lightning</h4> <ul> <li>Lightning is a framework for PyTorch that provides a high-level interface for training models.</li> <li>It allows you to write less boilerplate code and focus on the model architecture and training logic</li> </ul> <details><summary><a href="https://lightning.ai/docs/pytorch/stable/levels/core_skills.html#basic-skills">Basic skills in Lightning:</a></summary> <ol> <li>Train a model</li> <li>Add validation and test</li> <li>Use pretrained models</li> <li>Enable script parameters</li> <li>Understand and visualize your model</li> <li>Predict with your model</li> </ol> </details> <h4 id="pipeline">Pipeline</h4> </details> <hr/> <h2 id="chapter-6--7-solidifying-pytorch-fundamentals"><a href="https://docs.google.com/document/d/1dA8KmOTZePMRl3MixxM6Fb0H8IJhIyn_g-LUXbRHeqU/edit?tab=t.0">Chapter 6 &amp; 7: Solidifying PyTorch Fundamentals</a></h2> <hr/> <h2 id="chapter-8--9-organizing-model-training-with-weights--biases-and-hydra"><a href="https://docs.google.com/document/d/1kZCrACh8wHFFAinscHpbaHqMBKeErjOgXMVqKSUZIMU/edit?tab=t.0">Chapter 8 &amp; 9: Organizing Model Training with Weights &amp; Biases and Hydra</a></h2> <hr/> <h2 id="chapter-10--11-a-framework-for-generating-research-ideas"><a href="https://docs.google.com/document/d/15pnUpD47S6mAM-g4fwQvc2klYIb-GKgWex1oOlmNjvg/edit?tab=t.0#heading=h.puuns7thj5bs">Chapter 10 &amp; 11: A Framework for Generating Research Ideas</a></h2> <p>Chapter’s featured papers:</p> <ul> <li><a href="https://www.nature.com/articles/s41551-022-00936-9">Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning</a></li> <li><a href="https://arxiv.org/pdf/2103.00020.pdf">CLIP (Learning Transferable Visual Models From Natural Language Supervision)</a></li> </ul> <h3 id="ch10-objectives"><u>Objectives</u>:</h3> <ul> <li><strong>Identify gaps in a research paper</strong>, including in the research question, experimental setup, and findings.</li> <li><strong>Generate ideas to build on a research paper</strong>, thinking about the elements of the task of interest, evaluation strategy and the proposed method.</li> <li>Iterate on your ideas to improve their quality.</li> </ul> <h3 id="ch10-content"><u>Chapter's content</u>:</h3> <p>/** I’ll save this for my AIC task **/</p> <hr/> <h2 id="chapter-12--13-structuring-a-research-paper">Chapter 12 &amp; 13: Structuring a Research Paper</h2> <hr/> <h2 id="chapter-14--15-aws-ec2-for-deep-learning-setup-optimization-and-hands-on-training-with-chexzero">Chapter 14 &amp; 15: AWS EC2 for Deep Learning: Setup, Optimization, and Hands-on Training with CheXzero</h2> <hr/> <h2 id="chapter-16--17-fine-tuning-your-stable-diffusion-model">Chapter 16 &amp; 17: Fine-Tuning Your Stable Diffusion Model</h2> <hr/> <h2 id="chapter-18-tips-to-manage-your-time-and-efforts">Chapter 18: Tips to Manage Your Time and Efforts</h2> <hr/> <h2 id="chapter-19-making-progress-and-impact-in-ai-research">Chapter 19: Making Progress and Impact in AI Research</h2> <hr/> <h2 id="chapter-20-tips-for-creating-high-quality-slides">Chapter 20: Tips for Creating High-Quality Slides</h2> <hr/> <h2 id="chapter-21-statistical-testing-to-compare-model-performances">Chapter 21: Statistical Testing to Compare Model Performances</h2> <hr/> <h2 id="exercises">Exercises:</h2> <h3 id="assignments">Assignments:</h3> <p>Assignment 1: The Language of Code <br/> Assignment 2: First Dive in AI <br/> Assignment 3: Torched <br/> Assignment 4: Spark Joy <br/> Assignment 5: Ideation and Organization <br/> Assignment 6: Stable Diffusion and Research Operations</p> <hr/> <h3 id="course-project">Course Project:</h3> <p>[Redacted]</p> <hr/> <h2 id="references">References:</h2> <p><br/> <a href="https://www.cs197.seas.harvard.edu/">Course Homepage</a> <br/> <a href="https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit?tab=t.0#heading=h.act903jwq03w">Docs Content</a></p>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="research-methodology"/><summary type="html"><![CDATA[Notes and commentaries]]></summary></entry></feed>