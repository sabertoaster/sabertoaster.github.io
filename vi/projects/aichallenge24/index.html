<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Known Item Search for Video Retrieval | Huy Mai </title> <meta name="author" content="Huy Mai"> <meta name="description" content="A text-to-video retrieval system developed for HCMC AI Challenge 2024"> <meta name="keywords" content="deep-learning, artificial-intelligence, neuroscience"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%91%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sabertoaster.github.io/projects/aichallenge24/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/vi/"> <span class="font-weight-bold">Huy</span> Mai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/vi/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/vi/projects/">Works <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/news/">Logs </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Known Item Search for Video Retrieval</h1> <p class="post-description">A text-to-video retrieval system developed for HCMC AI Challenge 2024</p> </header> <article> <h1 id="hcmc-ai-challenge-2024-known-item-search-for-video-retrieval">HCMC AI Challenge 2024: Known Item Search for Video Retrieval</h1> <h2 id="overview">Overview</h2> <p>This project implements a text-to-video retrieval system that allows users to search for specific video frames using natural language descriptions. Developed for the HCMC AI Challenge 2024, the system uses advanced CLIP models to bridge the semantic gap between textual queries and visual content.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/aic2024/overview-480.webp 480w,/assets/img/projects/aic2024/overview-800.webp 800w,/assets/img/projects/aic2024/overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/aic2024/overview.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Overview" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> System architecture overview: From natural language query to finding the exact frame in a large video database. </div> <h2 id="key-features">Key Features</h2> <ul> <li> <strong>Multi-language support</strong>: Accepts queries in various languages with automatic translation to English</li> <li> <strong>Multiple CLIP models</strong>: Supports ViT-L/14, ViT-B/32, and ViT-B/16 CLIP models for different accuracy-speed tradeoffs</li> <li> <strong>Vector search optimization</strong>: Uses Qdrant vector database and FAISS for efficient similarity search</li> <li> <strong>Rich visualization</strong>: Interactive UI allowing direct playback of matched video segments</li> <li> <strong>Batch processing</strong>: Supports uploading multiple query files for large-scale analysis</li> </ul> <h2 id="technical-implementation">Technical Implementation</h2> <h3 id="system-architecture">System Architecture</h3> <p>The system is built around three main components:</p> <ol> <li> <strong>Embedding generation pipeline</strong>: Processes videos by extracting keyframes and computing CLIP embeddings</li> <li> <strong>Vector database</strong>: Stores and indexes frame embeddings for efficient search</li> <li> <strong>Search API and UI</strong>: Connects user queries to the backend retrieval system</li> </ol> <div class="row"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/aic2024-architecture-480.webp 480w,/assets/img/projects/aic2024-architecture-800.webp 800w,/assets/img/projects/aic2024-architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/aic2024-architecture.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Technical Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/aic2024-demo-480.webp 480w,/assets/img/projects/aic2024-demo-800.webp 800w,/assets/img/projects/aic2024-demo-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/aic2024-demo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Demo" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: The technical architecture showing data flow through the system. Right: Screenshot of the system's user interface. </div> <h3 id="clip-models">CLIP Models</h3> <p>The system offers three CLIP model variants:</p> <ul> <li> <strong>ViT-L/14@336px</strong>: Higher accuracy but slower inference (768-dimensional embeddings)</li> <li> <strong>ViT-B/16</strong>: Balanced performance (512-dimensional embeddings)</li> <li> <strong>ViT-B/32</strong>: Faster inference with slightly lower accuracy (512-dimensional embeddings)</li> </ul> <p>These models convert both images and text into the same embedding space, allowing for direct similarity comparison between queries and video frames.</p> <h3 id="preprocessing-pipeline">Preprocessing Pipeline</h3> <p>Video processing follows these steps:</p> <ol> <li> <strong>Scene Detection</strong>: Videos are analyzed to identify meaningful shots and transitions</li> <li> <strong>Keyframe Extraction</strong>: Representative frames are extracted from each scene</li> <li> <strong>Feature Embedding</strong>: CLIP models generate vector embeddings for each keyframe</li> <li> <strong>Indexing</strong>: Embeddings are stored in Qdrant with metadata linking back to source videos</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of embedding generation with CLIP
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">(),</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">():</span>
    <span class="n">text_vector</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode_text</span><span class="p">(</span><span class="n">text_inputs</span><span class="p">)</span>
<span class="n">text_vector</span> <span class="o">/=</span> <span class="n">text_vector</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="vector-database">Vector Database</h3> <p>We use a combination of FAISS and Qdrant:</p> <ul> <li> <strong>FAISS</strong>: Optimized for high-dimensional vector search with efficient indexing</li> <li> <strong>Qdrant</strong>: Provides persistence, metadata storage, and production-ready APIs</li> </ul> <p>This hybrid approach allows for both performance and flexibility when searching through thousands of video frames.</p> <h2 id="user-interface">User Interface</h2> <p>The web interface offers:</p> <ul> <li>Text input for natural language queries</li> <li>Model selection for different accuracy/speed preferences</li> <li>Adjustable number of results</li> <li>CSV export for batch processing results</li> <li>Direct YouTube video playback at matched timestamps</li> <li>Drag-and-drop reordering of results</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/aic2024-results-480.webp 480w,/assets/img/projects/aic2024-results-800.webp 800w,/assets/img/projects/aic2024-results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/aic2024-results.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Example results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example search results for the query "A boat moving on ice" showing matched frames with corresponding video information. </div> <h2 id="challenges--solutions">Challenges &amp; Solutions</h2> <h3 id="translation-quality">Translation Quality</h3> <p><strong>Challenge</strong>: Queries in non-English languages sometimes lost nuance after translation.</p> <p><strong>Solution</strong>: Implemented a specialized translation pipeline optimized for retaining visual descriptors.</p> <h3 id="vector-search-performance">Vector Search Performance</h3> <p><strong>Challenge</strong>: Initial vector search was too slow for interactive use.</p> <p><strong>Solution</strong>: Added FAISS indexes and implemented a hybrid search approach that balances accuracy and speed.</p> <h3 id="time-complexity">Time Complexity</h3> <p><strong>Challenge</strong>: Processing large video datasets required significant computing resources.</p> <p><strong>Solution</strong>: Developed a selective keyframe extraction approach that reduced the number of frames to process while maintaining result quality.</p> <h2 id="conclusion--future-work">Conclusion &amp; Future Work</h2> <p>The system demonstrates effective cross-modal search abilities, allowing users to find specific video moments using natural language. In the HCMC AI Challenge 2024, our approach achieved competitive results with particularly strong performance on fine-grained descriptive queries.</p> <p>Future improvements could include:</p> <ul> <li>Temporal awareness for better understanding of actions and events</li> <li>Integration of additional modalities like audio analysis</li> <li>More advanced vector quantization for handling larger datasets</li> <li>Zero-shot learning capabilities for handling novel query types</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/aic2024-team-480.webp 480w,/assets/img/projects/aic2024-team-800.webp 800w,/assets/img/projects/aic2024-team-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/aic2024-team.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Team photo" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Our team at the HCMC AI Challenge 2024 competition. </div> <h2 id="acknowledgments">Acknowledgments</h2> <p>We would like to thank the organizers of HCMC AI Challenge 2024 for providing the dataset and evaluation framework. Special thanks to our advisors for their guidance throughout the competition.</p> <p>The code for this project is available in our <a href="https://github.com/sabertoaster/AIChallengeHCMC2024" rel="external nofollow noopener" target="_blank">GitHub repository</a>.</p> </article> <div id="giscus_thread"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'sabertoaster/sabertoaster.github.io',
        'data-repo-id': 'R_kgDONJ-QGg',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDONJ-QGs4CnyrA',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'top',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Huy Mai. al-folio theme </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>