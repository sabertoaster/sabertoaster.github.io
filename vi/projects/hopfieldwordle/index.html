<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hopfield Network as Wordle Solver | Huy Mai </title> <meta name="author" content="Huy Mai"> <meta name="description" content="Biologically Plausible Associative Memory for Recalling Missing Word Patterns."> <meta name="keywords" content="deep-learning, artificial-intelligence, neuroscience"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%91%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sabertoaster.github.io/vi/projects/hopfieldwordle/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/vi/"> <span class="font-weight-bold">Huy</span> Mai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/vi/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/vi/projects/">Works <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Hopfield Network as Wordle Solver</h1> <p class="post-description">Biologically Plausible Associative Memory for Recalling Missing Word Patterns.</p> </header> <article> <h2 id="motivation">Motivation</h2> <p>What happens when you encounter a partial word like “d__ty” in your mind? Your brain doesn’t randomly guess—it systematically explores possibilities like “deity,” “dusty,” “dirty,” each triggered by spreading activation through your memory network. This cognitive process inspired me to explore whether <strong>modern Hopfield networks</strong> could model human-like word completion.</p> <p>While existing Wordle solvers achieve 95%+ success rates using information theory or reinforcement learning, they lack <strong>biological plausibility</strong>. Can we build a system that solves Wordle the way humans might—through <strong>associative memory retrieval</strong> rather than exhaustive optimization?</p> <hr> <h2 id="background">Background</h2> <h3 id="1-wordle-and-the-mathematical-approach">1. Wordle and the Mathematical Approach.</h3> <p>Skip to <a href="#2-brief-background-on-classical-hopfield-networks">Hopfield part</a> if you already know what is a Wordle and Information Theory application for it.</p> <blockquote> <p>“Wordle is a web-based word game created and developed by the Welsh software engineer <a href="https://en.wikipedia.org/wiki/Josh_Wardle" rel="external nofollow noopener" target="_blank">Josh Wardle</a>. <mark>In the game, players have six attempts to guess a five-letter word, receiving feedback through colored tiles that indicate correct letters and their placement.</mark> A single puzzle is released daily, with all players attempting to solve the same word. It was inspired by word games like <a href="https://en.wikipedia.org/wiki/Jotto" rel="external nofollow noopener" target="_blank">Jotto</a> and the game show <a href="https://en.wikipedia.org/wiki/Lingo_(American_game_show)" rel="external nofollow noopener" target="_blank">Lingo</a>. Bought and hosted by <a href="https://www.nytimes.com" rel="external nofollow noopener" target="_blank">The NY Times</a>.” <br>- From Wikipedia.</p> </blockquote> <div class="caption"> A summary of Wordle mechanism. Few exceptions like double word rule can be found <a href="https://www.reddit.com/r/wordle/comments/ry49ne/illustration_of_what_happens_when_your_guess_has/" rel="external nofollow noopener" target="_blank">here</a>. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/hopfieldwordle/wordle_game-480.webp 480w,/assets/img/projects/hopfieldwordle/wordle_game-800.webp 800w,/assets/img/projects/hopfieldwordle/wordle_game-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/hopfieldwordle/wordle_game.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><br> <br></p> <hr> <p>Information Theory - How it’s used in solving Wordle: (<a href="https://www.youtube.com/watch?v=v68zYyaEmEA" rel="external nofollow noopener" target="_blank">3blue1brown explanation</a>)</p> <p>Briefly, the optimal Wordle strategy maximizes <strong>information gain</strong> per guess. Each guess partitions the solution space, with entropy measuring uncertainty:</p> \[E = -\sum_{i} p_i \log_2 p_i\] <p><strong>Example:</strong> Starting with “SALET” provides ~5.9 bits of information, reducing 2,315 possible answers to ~60 on average. However, this approach requires:</p> <ul> <li>Perfect knowledge of word frequencies</li> <li>Exhaustive similarity computations</li> <li>No memory or attention limitations</li> </ul> <hr> <p>Rank One Approximation - from the paper <a href="https://arxiv.org/abs/2204.06324" rel="external nofollow noopener" target="_blank">Rank One Approximation as a Strategy for Wordle</a></p> <p><a href="##" title="Cited from the paper"><strong>Motivation:</strong></a></p> <blockquote> <p>“The motivation behind a low rank approximation comes from determining the dominant direction of a matrix. In statistics this approach is referred to as “Principal Component Analysis” and is used to <strong>determine a line of best fit</strong> through a set of data. The principal direction can be helpful in data analysis and making predictive models. In the context of Wordle, the principal component of a set of words will be found and interpreted as the best representation of the set. By then finding the word closest to the principal component it can be interpreted that this word is ‘most representative’ of the set. This method will find a representative word without needing to consider letter frequency.”</p> </blockquote> <p><a href="##" title="Cited from the paper"><strong>Theory</strong></a> <br> A rank one approximation begins with applying single value decomposition to a matrix \(A\). Single value decomposition is a way of factoring a matrix \(A\) into the following form:</p> \[A=USV^T\] <p>For an \(m×n\) matrix the factors \(U,S\) and \(V\) are defined as follows:</p> <p>\(U := m×m \text{ matrix whose } \textbf{columns} \text{ are the left singular vectors } u_i\) \(S := m×n \text{ matrix whose } \textbf{diagonal entries} \text{ are the singular values } \lambda_i\) \(V := n×n \text{ matrix whose } \textbf{rows} \text{ are the right singular vectors } v_i\)</p> <p>where:</p> <p>\(u_i := \text{the eigenvectors of } AA^T\) <br> \(\lambda_i := \text{the eigenvalues of } AA^T \text{ (or } A^TA\text{)}\) <br> \(v_i := \text{the eigenvectors of } A^TA\)</p> <p>The eigenvectors for both \(U,S\) and \(V\) are arranged in descending order according to their corresponding eigenvalue. If $m=n$ that is to say the following:</p> \[A =\begin{bmatrix} u_{11} &amp; \dots &amp; u_{1m} \\ \vdots &amp; &amp; \vdots \\ u_{m1} &amp; \dots &amp; u_{mm} \end{bmatrix} \begin{bmatrix} \lambda_{1} &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; \lambda_{2} &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \lambda_{m} \end{bmatrix} \begin{bmatrix} v_{11} &amp; \dots &amp; v_{1n} \\ \vdots &amp; &amp; \vdots \\ v_{n1} &amp; \dots &amp; v_{nn} \end{bmatrix}\] <p>Where:</p> \[\lambda_1&gt;\lambda_2...&gt;\lambda_m\] <p>and</p> \[u_i = \begin{bmatrix} u_{1i} \\ u_{2i} \\ \vdots \\ u_{mi} \end{bmatrix} \text{ and } v_i^T=\begin{bmatrix} v_{i1} &amp; v_{i2} &amp; ... &amp; v_{in} \end{bmatrix} \text{ correspond to } \lambda_i\] <p>Factoring \(A\) into the components \(U,S\) and \(V\) is known as <em>Single Value Decomposition</em> of the matrix and is a question of determining eigenvectors and eigenvalues of the two matrix products and placing the results in their corresponding index. From here, a rank one approximation of \(A\) considers <strong><em>the singular value \(\lambda_1\) and the corresponding vectors \(u_1\) and \(v_1\) to approximate the matrix \(A\).</em></strong> The method proposes that these two column vectors and eigenvalue is the best approximation of the matrix \(A\).</p> \[A ≈u_1\lambda_1 v_1^T\] <hr> <p><strong>References:</strong></p> <ul> <li>Sauer, Timothy. “Eigenvalues and Singular Values.” Numerical Analysis, Pearson, Boston, MA, 2006, pp. 566–579.</li> <li>James, David, et al. “Singular Vectors’ Subtle Secrets.” The College Mathematics Journal, vol. 42, no. 2, 2011, pp. 86–95., https://doi.org/10.4169/college.math.j.42.2.086.</li> </ul> <p>In this work, I applied LORA based on the fact that it can applied on a retrieved set of words (fitting for my initial idea).</p> <h3 id="2-brief-background-on-classical-hopfield-networks">2. Brief background on Classical Hopfield Networks</h3> <p><strong>Associative memories</strong> are one of the earliest artificial neural models dating back to the 1960s and 1970s. Best known are Hopfield Networks, presented by John Hopfield in 1982. As the name suggests, the main purpose of associative memory networks is to associate an input with its most similar pattern. <mark>In other words, the purpose is to store and retrieve patterns</mark>.</p> <p>Think of a Hopfield Network as a <strong>magical photo album</strong> where showing a <strong>torn photograph</strong> automatically reconstructs the <strong>complete original image</strong>. This is exactly what these networks do—they’re <strong>associative memories</strong> that can recall complete patterns from partial or noisy inputs. In our case, they can complete “D__TY” → “DEITY” by associative recall rather than exhaustive search.</p> <div class="caption"> Classical Hopfield Network: Fully connected neurons with symmetric weights </div> <div class="row mt-3 justify-content-center"> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/hopfieldwordle/hopfield_net-480.webp 480w,/assets/img/projects/hopfieldwordle/hopfield_net-800.webp 800w,/assets/img/projects/hopfieldwordle/hopfield_net-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/hopfieldwordle/hopfield_net.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The components typically includes:</p> <ul> <li> <strong>Symmetric weights</strong> which can be represented as an undirected graph (the connections from neuron i to neuron j is the same weight vice versa)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>: \(W_{ij} = W_{ji}\)</li> <li> <strong>Binary neuron states</strong>: 0/1 (binary), -1/+1 (bipolar)</li> <li> <strong>No self-connections</strong>: \(W_{ii} = 0\) (neurons don’t connect to themselves)</li> </ul> <p><strong>Example:</strong> To store the word “DEITY”, we encode it as a bipolar vector where each position represents a letter:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"DEITY" → [+1, +1, +1, +1, +1] (simplified 5-dimensional representation)
</code></pre></div></div> <p>The network creates connections between neurons such that this pattern becomes a <strong>stable memory state</strong>.</p> <h4 id="mathematical-formulation-for-word-completion">Mathematical formulation for Word Completion</h4> <blockquote> <p><strong>Given:</strong> N word patterns to recall: \(\{x_i\}_{i=1}^N\) (our vocabulary)</p> <p><strong>State:</strong> Current word configuration: \(\xi \in \{-1,+1\}^d\)</p> <p><strong>Energy function:</strong> \(E = -\frac{1}{2}\sum_i \sum_j W_{ij}\xi_i\xi_j + \sum_i b_i \xi_i\)</p> <p>where \(b_i\) are bias terms (thresholds for each neuron).</p> </blockquote> <p><strong>Key Insight:</strong> Each polar/binary patterns are present on a energy landscape which is given by the weights. The model works by creating transition from one local state to the next. The weights ensure that there are clear wells corresponding to learned patterns which attract surrounding patterns.</p> \[\boxed{\text{Partial word input} \rightarrow \text{Energy minimization} \rightarrow \text{Complete word retrieval}}\] <h4 id="the-functions-of-the-model">The functions of the model:</h4> <p><strong>Learning the patterns:</strong></p> <ol> <li> <p><strong>Direct weight setting (Hebbian Learning):</strong></p> \[W = \sum_{i=1}^N x_{i} \cdot x_{i}^T\] <ul> <li> <strong>Pros:</strong> Simple, biologically inspired, fast storage</li> <li> <strong>Cons:</strong> Limited capacity (~0.14N patterns), interference between similar words</li> <li> <strong>Example:</strong> Storing [“DEITY”, “DUSTY”] might create spurious states like “DAATY”</li> </ul> </li> <li> <p><strong>Gradient-based learning (Modern approach):</strong> Uses backpropagation to optimize storage capacity and reduce interference.</p> <ul> <li> <strong>Pros:</strong> Higher capacity, fewer spurious states, better retrieval</li> <li> <strong>Cons:</strong> More computationally expensive, requires training data</li> <li> <strong>Example:</strong> Can cleanly separate similar words like “DUSTY” vs “DIRTY”</li> </ul> </li> </ol> <p><strong>Retrieving patterns (Update rules):</strong></p> <ol> <li> <p><strong>Asynchronous updates (Classical):</strong></p> \[\xi_{i}^{(new)} = \text{sign}\Big(\sum_j W_{ij} \xi_j - b \Big) \text{(one neuron at a time)}\] <ul> <li> <strong>Pros:</strong> Guaranteed convergence, biologically realistic<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> </li> <li> <strong>Cons:</strong> Slower convergence, order-dependent results</li> <li> <strong>Use case:</strong> When you want guaranteed stability for word completion</li> </ul> </li> <li> <p><strong>Synchronous updates (Parallel):</strong></p> \[\xi^{(new)} = \text{sign} \big( W \cdot \xi \big) \text{(all neurons simultaneously)}\] <ul> <li> <strong>Pros:</strong> Faster computation due to <a href="https://www.geeksforgeeks.org/python/vectorization-in-python/" rel="external nofollow noopener" target="_blank">vectorization</a>.</li> <li> <strong>Cons:</strong> Can oscillate, might not converge</li> <li> <strong>Use case:</strong> When speed matters and patterns are well-separated</li> </ul> </li> </ol> <h4 id="classical-limitations-setting-up-the-need-for-modern-hopfield">Classical Limitations (Setting up the need for Modern Hopfield)</h4> <p><strong>The fundamental problems:</strong></p> <ul> <li> <strong>Limited capacity:</strong> Can only reliably store ~0.14N patterns</li> <li> <strong>Spurious attractors:</strong> Network might converge to “DAATY” instead of “DEITY”</li> <li> <strong>Interference:</strong> Similar words like “DUSTY” and “DIRTY” confuse each other</li> </ul> <p>These limitations motivated the development of <strong>Modern Hopfield Networks</strong>, which solve these issues while preserving the elegant associative memory properties.</p> <p>Some side related topics which are also very important:</p> <ul> <li> <strong>Free Energy Principle:</strong> How biological systems minimize surprise through prediction. <blockquote> <p>Imagine you’re trying to catch a ball. You make predictions about its trajectory (perception) and then take actions to move towards it (action). The FEP suggests that you’re constantly minimizing the “surprise” of catching the ball by updating your predictions and refining your movements.</p> </blockquote> </li> <li> <strong>Hebbian Learning:</strong> <blockquote> <p>“Neurons that fire together, wire together” - the biological basis for associative memory.</p> </blockquote> </li> </ul> <h3 id="3-modern-hopfield-networks-exponential-capacity-and-continuous-states">3. Modern Hopfield Networks: Exponential Capacity and Continuous States</h3> <p>The classical Hopfield networks suffer from fundamental limitations: spurious attractors, limited storage capacity (~0.14N), and poor retrieval for similar patterns. Modern Hopfield Networks, introduced by Krotov &amp; Hopfield (2016) and generalized by Ramsauer et al. (2020), address these issues through <strong>continuous states</strong> and <strong>exponential interaction functions</strong>.</p> <h4 id="energy-function-with-interaction-functions">Energy Function with Interaction Functions</h4> <p>The key innovation lies in replacing the quadratic energy function with more general <strong>interaction functions</strong>. The modern energy function is:</p> \[E = -\sum_{i=1}^N F\left(\frac{1}{d}(\mathbf{x}_i)^T \boldsymbol{\xi}\right)\] <p>where:</p> <ul> <li>\(\mathbf{x}_i \in \mathbb{R}^d\) are the stored patterns</li> <li>\(\boldsymbol{\xi} \in \mathbb{R}^d\) is the current state</li> <li>\(F: \mathbb{R} \rightarrow \mathbb{R}\) is the <strong>interaction function</strong> </li> <li>\(d\) is the pattern dimension</li> </ul> <p><strong>Common Interaction Functions:</strong></p> <ol> <li> <strong>Exponential</strong>: \(F(x) = \exp(x)\) <ul> <li>Provides exponential storage capacity</li> <li>Most commonly used in practice</li> </ul> </li> <li> <strong>Polynomial</strong>: \(F(x) = x^n\) for \(n \geq 2\) <ul> <li>Polynomial storage capacity scaling</li> <li>Computational simplicity</li> </ul> </li> <li> <strong>Custom functions</strong>: Any concave function satisfying convergence conditions</li> </ol> <p>The <strong>exponential interaction function</strong> is particularly powerful because it creates a <strong>log-sum-exp</strong> energy landscape that naturally separates stored patterns.</p> <h4 id="storage-capacity">Storage Capacity</h4> <p><strong>Classical Hopfield</strong>: Capacity scales as \(C_\text{classical} \sim 0.14N\) patterns.</p> <p><strong>Modern Hopfield</strong>: With exponential interaction functions, capacity scales <strong>exponentially</strong> with dimension: \(C_{modern} \sim \exp(\alpha d)\)</p> <p>where \(\alpha\) depends on the specific interaction function and pattern distribution.</p> <p><strong>Practical Implication</strong>: A 130-dimensional Modern Hopfield Network (5×26 for our word encoding) can store thousands of patterns without significant interference, compared to ~18 patterns for classical networks.</p> <h4 id="the-beta-parameter">The Beta Parameter</h4> <p>The <strong>inverse temperature</strong> parameter \(\beta\) controls the <strong>sharpness</strong> of the energy landscape:</p> \[E_{\beta} = -\frac{1}{\beta} \log \sum_{i=1}^N \exp\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)\] <p><strong>Effects of \(\beta\):</strong></p> <ul> <li> <strong>\(\beta \rightarrow 0\)</strong>: Uniform distribution over all patterns (high temperature)</li> <li> <strong>\(\beta \rightarrow \infty\)</strong>: Sharp focus on most similar pattern (low temperature)</li> <li> <strong>\(\beta = 1\)</strong>: Balanced retrieval dynamics</li> </ul> <p>For word completion, higher \(\beta\) values (\(\beta \geq 2\)) provide more decisive pattern completion.</p> <h4 id="update-rule-derivation">Update Rule Derivation</h4> <p>The update rule emerges from <strong>energy minimization</strong> using the concave-convex procedure. Starting with the energy:</p> \[E = -\frac{1}{\beta} \log \sum_{i=1}^N \exp\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)\] <p><strong>Step 1</strong>: Apply concave-convex decomposition to handle the log-sum-exp structure.</p> <p><strong>Step 2</strong>: The gradient with respect to \(\boldsymbol{\xi}\) yields: \(\frac{\partial E}{\partial \boldsymbol{\xi}} = -\frac{1}{d} \sum_{i=1}^N \frac{\exp\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)}{\sum_{j=1}^N \exp\left(\beta \frac{(\mathbf{x}^{(j)})^T \boldsymbol{\xi}}{d}\right)} \mathbf{x}_i\)</p> <p><strong>Step 3</strong>: Recognizing the softmax structure: \(\boldsymbol{\xi}^{new} = \sum_{i=1}^N \text{softmax}\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)_i \mathbf{x}_i\)</p> <p><strong>Step 4</strong>: In matrix notation, with \(\mathbf{X} = [\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(N)}]\):</p> \[\boxed{\boldsymbol{\xi}^{new} = \mathbf{X} \cdot \text{softmax}\left(\beta \mathbf{X}^T \boldsymbol{\xi}\right)}\] <h4 id="connection-to-transformer-self-attention">Connection to Transformer Self-Attention</h4> <p>The update rule is <strong>mathematically equivalent</strong> to the self-attention mechanism in transformer networks:</p> <p><strong>Hopfield Update:</strong> \(\boldsymbol{\xi}^{new} = \mathbf{X} \cdot \text{softmax}(\beta \mathbf{X}^T \boldsymbol{\xi})\)</p> <p><strong>Self-Attention:</strong> \(\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \mathbf{V} \cdot \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\)</p> <p><strong>Correspondence:</strong></p> <ul> <li>\(\mathbf{V} = \mathbf{K} = \mathbf{X}\) (stored patterns)</li> <li>\(\mathbf{Q} = \boldsymbol{\xi}^T\) (query state)</li> <li>\(\beta = 1/\sqrt{d_k}\) (temperature scaling)</li> </ul> <p>This equivalence reveals that <strong>attention mechanisms are performing associative memory retrieval</strong> in the continuous state space.</p> <hr> <h2 id="what-i-learned-from-trying-and-failing-to-solve-wordle-with-hopfield-networks">What I Learned from Trying (and Failing) to Solve Wordle with Hopfield Networks</h2> <p><em>A brutally honest account of why biological inspiration doesn’t always translate to working code</em></p> <hr> <h3 id="what-i-thought-would-happen">What I Thought Would Happen</h3> <p>I was convinced that solving Wordle would be a breeze—just build a working MVP since I initially thought solving Wordle was a <strong>recalling game</strong> plus <strong>optimally choosing the retrieved answer</strong>. The logic seemed bulletproof:</p> <ol> <li>Humans see “d__ty” and recall words like “deity,” “dusty,” “dirty”</li> <li>This looks exactly like <strong>associative memory retrieval</strong> </li> <li>Modern Hopfield networks are basically attention mechanisms that do associative memory</li> <li>Therefore: <strong>Hopfield + some word selection strategy = Human-like Wordle solver</strong> </li> </ol> <p>I figured I’d model the recall part using Hopfield networks, then use some optimization method (LoRA-based eigenvector alignment) to pick the best candidate. Easy, right?</p> <p><strong>Spoiler alert</strong>: It wasn’t.</p> <hr> <h3 id="what-i-actually-built">What I Actually Built</h3> <p>Let me walk through what I created:</p> <h4 id="the-core-architecture">The Core Architecture</h4> <p>I implemented a <code class="language-plaintext highlighter-rouge">WordHopfieldNetwork</code> that:</p> <ul> <li> <strong>One-hot encodes</strong> 5-letter words into 130-dimensional vectors (5 positions × 26 letters)</li> <li>Uses <strong>modern Hopfield dynamics</strong> with softmax attention: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">similarities</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memories</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">effective_query</span><span class="p">)</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">similarities</span><span class="p">)</span>
<span class="n">retrieved</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memories</span> <span class="o">@</span> <span class="n">attention_weights</span>
</code></pre></div> </div> </li> <li>Applies <strong>winner-take-all</strong> decoding to get valid words back</li> </ul> <h4 id="the-memory-deletion-catastrophe">The Memory Deletion Catastrophe</h4> <p>Here’s where things got creative (read: desperate). When the network kept suggesting the same words, I implemented a “suppression mechanism”:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">retrieve_possible_words</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">partial_word</span><span class="p">,</span> <span class="n">unknown_char</span><span class="o">=</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">completed_word</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">complete_word</span><span class="p">(</span><span class="n">partial_word</span><span class="p">,</span> <span class="n">unknown_char</span><span class="p">)</span>
        <span class="c1"># Find the memory pattern for this word
</span>        <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">all</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memories</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode_word</span><span class="p">(</span>
            <span class="n">completed_word</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">position</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">res</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">completed_word</span><span class="p">)</span>
            <span class="c1"># Delete the memory permanently
</span>            <span class="n">self</span><span class="p">.</span><span class="n">memories</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memories</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">res</span>
</code></pre></div></div> <p>Yes, I literally <strong>deleted patterns from memory</strong> after retrieving them. What could go wrong?</p> <h4 id="the-lora-band-aid">The LoRA Band-Aid</h4> <p>To pick the “best” word from retrieved candidates, I bolted on a <strong>Low-Rank Approximation</strong> method that:</p> <ul> <li>Converts candidate words to letter frequency vectors</li> <li>Finds the dominant eigenvector of the word-word similarity matrix</li> <li>Picks the word with the smallest angle to this eigenvector</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">LSI</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">possible_words</span><span class="p">,</span> <span class="n">must_haves</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">word_to_vector</span><span class="p">(</span><span class="n">possible_words</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">possible_words</span><span class="p">:</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">A</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">word_to_vector</span><span class="p">(</span><span class="n">word</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="n">AA</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">matrix</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigh</span><span class="p">(</span><span class="n">AA</span><span class="p">)</span>
    <span class="n">dom1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">v</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Dominant eigenvector
</span>    
    <span class="c1"># Find word with smallest angle to dominant direction
</span>    <span class="n">angle_min</span> <span class="o">=</span> <span class="mi">90</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">possible_words</span><span class="p">)):</span>
        <span class="n">angle1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arccos</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">dom1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])))</span>
        <span class="c1"># ... pick best_word based on angle
</span></code></pre></div></div> <p>This whole pipeline was held together with digital duct tape and wishful thinking.</p> <hr> <h3 id="why-it-failed-spectacularly">Why It Failed Spectacularly</h3> <h4 id="1-sub-optimal-energy-minima">1. <strong>Sub-optimal Energy Minima</strong> </h4> <p>The network would converge to <strong>nonsensical letter combinations</strong> like “DAAEZ” or “SUUUU” because:</p> <ul> <li>One-hot encoding creates <strong>sparse, high-dimensional vectors</strong> </li> <li>Similar words (like “DUSTY”, “DIRTY”, “DEITY”) create <strong>overlapping attraction basins</strong> </li> <li>The energy landscape was littered with <strong>spurious attractors</strong> that maximized pattern overlap without forming valid English words</li> </ul> <h4 id="2-pattern-completion--constraint-satisfaction">2. <strong>Pattern Completion ≠ Constraint Satisfaction</strong> </h4> <p>This was the fundamental conceptual error. Wordle isn’t about recalling complete patterns—it’s about <strong>satisfying logical constraints</strong>:</p> <ul> <li>🟩 Green: Letter X <strong>must</strong> be in position Y</li> <li>🟨 Yellow: Letter X <strong>must</strong> be in the word but <strong>not</strong> in position Y</li> <li>⬜ Gray: Letter X <strong>must not</strong> be anywhere in the word</li> </ul> <p>My Hopfield network optimized for: \(\text{argmax}_{\text{word}} \; \text{similarity}(\text{word}, \text{partial\_pattern})\)</p> <p>But Wordle requires: \(\text{argmax}_{\text{word}} \; \mathbb{I}[\text{word satisfies all constraints}]\)</p> <p>These are <strong>fundamentally different objective functions</strong>.</p> <h4 id="3-suppression-mechanism-caused-chaos">3. <strong>Suppression Mechanism Caused Chaos</strong> </h4> <p>Deleting memories from the Hopfield network was like <strong>removing neurons from a brain mid-thought</strong>:</p> <ul> <li> <strong>Catastrophic interference</strong>: Removing one pattern corrupted the retrieval of similar patterns</li> <li> <strong>Cascade failures</strong>: After suppressing ~10 words, the network couldn’t retrieve anything sensible</li> <li> <strong>Memory degradation</strong>: The attention weights became unstable as the memory matrix shrank</li> </ul> <p>The network went from confidently suggesting “DEITY” to outputting gibberish like “” within a few deletions. Also <a href="##" title="Maybe it's a programming skill issues">a weird bug happened</a>, the network keeps on converging to deleted patterns although it’s not presented in the memory matrix anymore.</p> <h4 id="4-wrong-tool-for-the-job">4. <strong>Wrong Tool for the Job</strong> </h4> <p>I was essentially using a <strong>hammer to perform surgery</strong>. Hopfield networks excel at:</p> <ul> <li> <strong>Denoising</strong> corrupted patterns</li> <li> <strong>Completing</strong> missing parts of stored memories</li> <li> <strong>Associative recall</strong> from partial cues</li> </ul> <p>But Wordle needs:</p> <ul> <li> <strong>Logical reasoning</strong> about constraints</li> <li> <strong>Search</strong> through valid word space</li> <li> <strong>Information-theoretic optimization</strong> for guess selection</li> </ul> <hr> <h3 id="what-i-learned">What I Learned</h3> <p>The biggest lesson? I was solving the wrong problem entirely. I thought Wordle was about pattern completion when it’s actually about constraint satisfaction. Hopfield networks excel at recalling corrupted memories, but Wordle doesn’t give you corrupted patterns—it gives you logical rules that must be satisfied. When humans see “d__ty” and think “deity,” they’re not just doing associative recall. They’re simultaneously tracking constraints (no repeated letters from previous guesses, letter positions that are forbidden, etc.) while accessing word memory. I tried to cram this entire cognitive pipeline into a single associative memory mechanism, which was fundamentally naive. Seeing memory deletion cascade into complete network breakdown showed me why you can’t just hack biological systems with engineering tricks. Most importantly: <mark>biological inspiration doesn't automatically translate to good algorithms. Just because the brain does something doesn't mean copying that mechanism will work in silicon.</mark> The gap between cognitive plausibility and computational efficiency is huge, and I underestimated it completely.</p> <hr> <h3 id="if-i-were-to-do-this-again">If I Were to Do This Again…</h3> <h4 id="constraint-aware-energy-function">Constraint-Aware Energy Function</h4> <p>Instead of pure pattern completion, I’d formulate this as <strong>constraint satisfaction with memory bias</strong>:</p> \[E(x) = E_{\text{valid}}(x) + \lambda_G E_{\text{green}}(x, G) + \lambda_Y E_{\text{yellow}}(x, Y) + \lambda_B E_{\text{black}}(x, B) + E_{\text{memory}}(x)\] <p>Where:</p> <ul> <li>\(E_{\text{green}}(x, G) = \sum_{(i,c) \in G} (1 - x_{i,c})\) penalizes violating green constraints</li> <li>\(E_{\text{yellow}}(x, Y)\) ensures yellow letters appear elsewhere</li> <li>\(E_{\text{black}}(x, B)\) forbids gray letters entirely</li> <li>\(E_{\text{memory}}(x) = -\log\sum_{\mu} \exp(\beta x^T \xi^{\mu})\) biases toward real words</li> </ul> <p>This wouldn’t be “pure” Hopfield anymore—it’s more like <strong>constrained optimization inspired by Hopfield</strong>. But at least it might work.</p> <h4 id="inhibitory-fields-instead-of-memory-deletion">Inhibitory Fields Instead of Memory Deletion</h4> <p>Rather than destroying memories, research more and implement some forms of memory suppressions:</p> <ul> <li><a href="https://onlinelibrary.wiley.com/doi/10.1002/jocb.680" rel="external nofollow noopener" target="_blank">Do Not Let the Beginning Trap you! On Inhibition, Associative Creative Chains, and Hopfield Neural Networks</a></li> </ul> <p>This approach might preserves network stability while avoiding repetition.</p> <h4 id="dense-distributed-representations">Dense Distributed Representations</h4> <p>The <strong>sparse one-hot encoding</strong> was problematic because:</p> <ul> <li>No <strong>semantic relationships</strong> between similar words</li> <li> <strong>High dimensionality</strong> with <strong>low information density</strong> </li> <li>Lots of <strong>local minima</strong> without meaningful structure</li> </ul> <p>Better approaches:</p> <ul> <li> <strong>Letter n-gram embeddings</strong> (capture “TH”, “CH”, “ST” patterns)</li> <li> <strong>Phonetic representations</strong> (similar-sounding words cluster together)</li> <li> <strong>Pre-trained word embeddings</strong> (semantic similarity built-in)</li> </ul> <p>For example, “DUSTY” and “DIRTY” should be closer in embedding space than “DUSTY” and “ZEBRA”.</p> <hr> <h2 id="conclusion">Conclusion:</h2> <p>This project was a total failure in terms of building a working Wordle solver. The success rate was probably worse than random guessing. Sometimes the most valuable research is the kind that <strong>doesn’t work</strong>—because it teaches you <strong>why</strong> it doesn’t work, and <strong>what to try instead</strong>.</p> <p>Next up: Maybe I’ll try to solve Wordle with constraint satisfaction properly, or better yet, find a problem that’s actually suited for Hopfield networks.</p> <hr> <p><em>Code available on <a href="https://github.com/sabertoaster/HopfieldWordle" rel="external nofollow noopener" target="_blank">GitHub</a></em></p> <hr> <h3 id="references">References</h3> <ol> <li>The paper I chose to dive deep into hopfield: <a href="https://ml-jku.github.io/hopfield-layers/" rel="external nofollow noopener" target="_blank">Hopfield Networks is All You Need</a> </li> <li>Video that sparked my interest in doing Hopfield Net: <a href="https://youtu.be/piF6D6CQxUw" rel="external nofollow noopener" target="_blank">Hopfield network: How are memories stored in neural networks? #SoME2</a> </li> <li>Daily Wordle: <a href="https://www.nytimes.com/games/wordle/index.html" rel="external nofollow noopener" target="_blank">NY Times</a> </li> </ol> <hr> <p>Acknowledgement:</p> <ul> <li>Thanks my dear teammates @ HCMUS for agreeing to do my initial idea on Hopfield Networks, hope we can colab again in the future.</li> <li>Thanks Dr. Tam Nguyen @ EPFL who checked on our drafts and be my trusty information source.</li> <li>Everyone that supported me emotionally during this.</li> </ul> <h3 id="disclaimer">Disclaimer</h3> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>This is proved to be inaccurate to depict how neurons works irl, example when you sit down/ stand up, there’ll be 2 seperate pathways of neurons to strengthen. For further reading, please consider <a href="https://www.youtube.com/watch?v=Ay3_D7VgzZs" rel="external nofollow noopener" target="_blank">a explanation video of Artem Kirsanov on “Brain’s Hidden Learning Limits”</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>The fact that neurons don’t wait for other neurons to fire at the same time as they’re constantly firing and transmiting information in realtime. The scope of temporal encoding unfortunately isn’t implemented in this project tho. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Huy Mai. al-folio theme </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-works",title:"Works",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-tr\u01b0\u1ee3t-d\u1ed1c-\u0111\u1eebng-xu\u1ed1ng-h\u1ed1",title:"Tr\u01b0\u1ee3t d\u1ed1c \u0111\u1eebng xu\u1ed1ng h\u1ed1.",description:"H\xe0nh tr\xecnh x\xe2y d\u1ef1ng th\u1ebf gi\u1edbi c\u1ee7a t\xf4i",section:"Posts",handler:()=>{window.location.href="/blog/2025/downfall/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-known-item-search-for-video-retrieval",title:"Known Item Search for Video Retrieval",description:"A text-to-video retrieval system developed for HCMC AI Challenge 2024",section:"Projects",handler:()=>{window.location.href="/projects/aichallenge24/"}},{id:"projects-heineken-beer-detection",title:"Heineken Beer Detection",description:"(AngelHack 2024) An AI-powered system that analyzes images to detect Heineken products, count customers, and extract marketing insights",section:"Projects",handler:()=>{window.location.href="/projects/angelhack24/"}},{id:"projects-hopfield-network-as-wordle-solver",title:"Hopfield Network as Wordle Solver",description:"Biologically Plausible Associative Memory for Recalling Missing Word Patterns.",section:"Projects",handler:()=>{window.location.href="/projects/hopfieldwordle/"}},{id:"projects-mazegame",title:"MazeGame",description:"(CSC10010) A Python-based maze game featuring multiple pathfinding algorithms with interactive visualization",section:"Projects",handler:()=>{window.location.href="/projects/mazegame/"}},{id:"projects-llm-based-document-similarity-detection",title:"LLM-Based Document Similarity Detection",description:"(EVN) Leveraging PhoBERT and Longformer for Vietnamese text duplicate detection",section:"Projects",handler:()=>{window.location.href="/projects/phobertEVN/"}},{id:"projects-remote-control-via-email",title:"Remote Control Via Email",description:"(CSC10008) DHCP",section:"Projects",handler:()=>{window.location.href="/projects/remotecontrolviaemail/"}},{id:"projects-flower-classification",title:"Flower Classification",description:"(Fellowship.AI) Fine-tuning ResNet50 with contrastive learning for the 102 Category Flower Dataset",section:"Projects",handler:()=>{window.location.href="/projects/resnet50fellowship/"}},{id:"projects-tictactoe",title:"TicTacToe",description:"(CSC10001) A feature-rich console-based game with AI algorithms and animations",section:"Projects",handler:()=>{window.location.href="/projects/tictactoe/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%69%6E%68%68%75%79%6D%61%69%64%75%63@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0005-2711-5320","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/sabertoaster","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/sabertoaster","_blank")}},{id:"socials-discord",title:"Discord",section:"Socials",handler:()=>{window.open("https://discord.com/users/483486384510861312","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>