<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Harvard CS197 AI Research Experiences | Huy Mai </title> <meta name="author" content="Huy Mai"> <meta name="description" content="Notes and commentaries"> <meta name="keywords" content="deep-learning, artificial-intelligence, neuroscience"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%91%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sabertoaster.github.io/blog/2025/harvard-cs197/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/vi/"> <span class="font-weight-bold">Huy</span> Mai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/vi/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/vi/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/projects/">Works </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/news/">Logs </a> </li> <li class="nav-item "> <a class="nav-link" href="/vi/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <div class="language-switcher"> </div> <h1 class="post-title">Harvard CS197 AI Research Experiences</h1> <p class="post-meta"> Created on July 01, 2025 </p> <p class="post-tags"> <a href="/vi/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="notation-convention">Notation Convention:</h2> <blockquote> <p>This is a quote from the course or other references.</p> </blockquote> <p><em><span style="color:red">This is an important highlight.</span></em></p> <h2 id="brief-introduction">Brief introduction</h2> <p>This is a course ran by Harvard, instructed by Professor <a href="https://pranavrajpurkar.com/" rel="external nofollow noopener" target="_blank">Pranav Rajpurkar</a>.</p> <blockquote> <p>Dive into cutting-edge development tools like PyTorch, Lightning, and Hugging Face, and streamline your workflow with VSCode, Git, and Conda. You’ll learn how to harness the power of the cloud with AWS and Colab to train massive deep learning models with lightning-fast GPU acceleration. Plus, you’ll master best practices for managing a large number of experiments with Weights and Biases. And that’s just the beginning! This course will also teach you how to systematically read research papers, generate new ideas, and present them in slides or papers. You’ll even learn valuable project management and team communication techniques used by top AI researchers. Don’t miss out on this opportunity to level up your AI skills.</p> </blockquote> <hr> <h2 id="chapter-1--2-introduction-to-ai-and-setting-up-environment">Chapter <a href="https://docs.google.com/document/u/0/d/1FHnGGGhTTarovEAVzSfELlNvxhXFJV4DkpuGgMKaEGw/edit" rel="external nofollow noopener" target="_blank">1</a> &amp; <a href="https://docs.google.com/document/u/0/d/1z5ELxpTw_U01jUB6-D6ILqHRPg6SSiLE7VFQryH3LPU/edit" rel="external nofollow noopener" target="_blank">2</a>: Introduction to AI and Setting up environment</h2> <p>Typical, intuitive guides to interact with AI and its research + development. Working environment requires source-code version control (Git), environment control (Conda) and editors (VSCode).</p> <hr> <h2 id="chapter-3-reading-ai-research-papers"><a href="https://docs.google.com/document/d/1bPhwNdCCKkm1_adD0rx1YV6r2JG98qYmTxutT5gdAdQ/edit?tab=t.0" rel="external nofollow noopener" target="_blank">Chapter 3: Reading AI Research Papers</a></h2> <details><summary>TLDR: Read wide for learning, read deep for improving. Take incremental approach.</summary> <h3 id="ch3-objectives"> <u>Objectives</u>:</h3> <ul> <li>Conduct a literature search to identify papers relevant to a topic of interest</li> <li>Read a machine learning research paper and summarize its contributions</li> </ul> <h3 id="ch3-content"> <u>Chapter's content</u>:</h3> <blockquote> <p>I’m going to break down the process of reading AI research papers into two pieces: reading wide, and reading deep. <em><span style="color:red">When you start learning about a new topic, you typically get more out of reading wide</span></em>: this means navigating through literature reading small amounts of individual research papers. Our goal when reading wide is to build and improve our mental model of a research topic. <em><span style="color:red">Once you have identified key works that you want to understand well in the first step, you will want to read deep</span></em>: here, you are trying to read individual papers in depth. Both reading wide and deep are necessary and complimentary, especially when you’re getting started.</p> </blockquote> <h4 id="read-wide">Read wide:</h4> <p>Paperswithcode (AI/ML) | Google scholar | ACM Digital Lib &gt; Taking notes &gt; Check benchmark with recent submissions &gt; Check SOTA &gt; Check dataset (in any order) &gt; Read em and take notes. <br> Results: Notes condensed with information, may take up to 2-3 hours doing this.</p> <blockquote> <p>At this point, I find myself particularly intrigued by the SOTA methods: Why are they achieving high performance? According to the review paper, it looks like “training strategies using pre-training” have been an advance. Maybe that’s worth keeping an eye out for!</p> </blockquote> <h4 id="read-deep">Read deep:</h4> <blockquote> <p>So I would like you to take an incremental approach here. Understand that, in your first pass, you will not understand more than 10% of the research paper. The paper may require us to read another more fundamental paper (which might require reading a third paper and so on; it could be turtles all the way down)! Then, in your second pass, you might understand 20% of the paper. Understanding 100% of a paper might require a significant leap, maybe because it’s poorly written, insufficiently detailed, or simply too technically/mathematically advanced. We thus want to aim to build up to understanding as much of the paper as possible – I’ll bet that 70-80% of the paper is a good target.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec3-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec3-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025/07/harvard-cs197-lec3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Define your own highlight strategy, for example:</p> <ul> <li>The <span style="color:yellow">yellow highlights</span> are the problems/challenges</li> <li>The <span style="color:pink">pink highlights</span> are the solutions to the challenges</li> <li>The <span style="color:orange">orange highlights</span> are the main contributions of the work we’re reading.</li> </ul> <h3 id="ch3-tips"> <u>Some useful tips</u>:</h3> <ul> <li> <p>Read deep: do a <a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf" rel="external nofollow noopener" target="_blank">three-pass approach</a>. <br><br> First, take a quick scan through the title, put some brief thought into it. Read the abstract, introduction, section headings. <strong>Identify the research question, the thematics, the papers’ contribution.</strong> Look through the figures for visual details. Glance through <strong>discussion</strong> if exists. Read the conclusion and see if the research question have been solved (?) <br><br> Second, focus on the introduction content, preliminary. Grasp the underlying content in <strong>methodology</strong> section and the <strong>results</strong>. Take notes on key points. Skip math terms if too heavy. The goal is to understand the paper’s content without getting bogged down in details. <br><br> This pass is a thorough, line-by-line reading with the aim of understanding every detail and challenging the author’s assumptions and claims. This pass is particularly useful for papers that require a deep understanding or are central. <strong>Re-implement</strong> the paper’s work/ <strong>identify areas for improvement</strong>.</p> </li> <li> <p>Read wide: If <strong>doing a literature review/ search, just do the first pass</strong> then look for referenced sources, mentioned SOTA methods, prominent proposed dataset/ benchmark, …</p> </li> <li> <blockquote> <p>You would have thus created a list of concepts you need to learn about, and the relevant paper for each, if the paper specifies any.</p> </blockquote> </li> </ul> </details> <hr> <h2 id="chapter-4-fine-tuning-a-language-model-using-hugging-face"><a href="https://docs.google.com/document/d/18mTeEk1zfJDz-oY48oB96ryZRuiAWRbCLOMYKap8eXk/edit?tab=t.0" rel="external nofollow noopener" target="_blank">Chapter 4: Fine-tuning a Language Model using Hugging Face</a></h2> <h3 id="ch4-objectives"> <u>Objectives</u>:</h3> <ul> <li>Load up and process a natural language processing dataset using the datasets library.</li> <li>Tokenize a text sequence, and understand the steps used in tokenization.</li> <li>Construct a dataset and training step for causal language modeling.</li> </ul> <h3 id="ch4-content"> <u>Chapter's content</u>:</h3> <details><summary>TLDR: HuggingFace’s Dataloader -&gt; Tokenize -&gt; Train/Test split -&gt; Train and Evaluate -&gt; Upload to model hub</summary> <h4 id="huggingface">HuggingFace</h4> <ul> <li>Community/ Data science center for building, training and deploying ML models based on open src software.</li> <li> <a href="https://huggingface.co/docs/transformers/tasks/language_modeling" rel="external nofollow noopener" target="_blank">Guide 1: Causal language mdeoling</a> and <a href="https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb" rel="external nofollow noopener" target="_blank">Guide 2: Code </a> on how to fine tune.</li> </ul> <h4 id="load-up-a-dataset">Load up a dataset</h4> <p>HF’s <code class="language-plaintext highlighter-rouge">Datasets library</code>’s 3 main feats:</p> <ul> <li>Load/ process data from CSV/JSON/text or python dict/pandas.DataFrame</li> <li>Access/ share datasets (through HF’s hub)</li> <li>Can be used with DL frameworks (pandas, numpy, pytorch/ tensorflow) (interoperable - new vocab hehe)</li> </ul> <p><a href="https://huggingface.co/datasets/rajpurkar/squad" rel="external nofollow noopener" target="_blank">SQuAD</a> (Stanford Question Answering Dataset) dataset:</p> <ul> <li>Reading comprehension</li> <li>Questions posed by crowdworkers on a set of Wikipedia articles</li> <li>Answer is a segment of text, span from corresponding reading passage; or unanswerable.</li> </ul> <h4 id="tokenize">Tokenize</h4> <p>A tokenization pipeline in huggingface comprises several <a href="https://huggingface.co/course/chapter6/8?fw=pt" rel="external nofollow noopener" target="_blank">steps</a>:</p> <ul> <li>(1) Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)</li> <li>(2) Pre-tokenization (splitting the input into words)</li> <li>(3) Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)</li> <li>(4) Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs).</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025/07/harvard-cs197-lec4-pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <blockquote> <p>The above steps show how we can go from text into tokens. There are multiple rules that govern the process that are specific to certain models. For tokenization, there are three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others); we won’t go into any of these, but if you’re curious, you can learn about them <a href="https://huggingface.co/course/chapter6/4?fw=pt#algorithm-overview" rel="external nofollow noopener" target="_blank">here</a>.</p> </blockquote> <h4 id="data-processing">Data Processing</h4> <blockquote> <p>For causal language modeling (CLM), one of the data preparation steps often used is to concatenate the different examples together, and then split them into chunks of equal size. This is so that we can have a common length across all examples without needing to pad.</p> </blockquote> <p>From <br>[<code class="language-plaintext highlighter-rouge">"I went to the yard.&lt;|endoftext|&gt;"</code>,<code class="language-plaintext highlighter-rouge">"You came here a long time ago from the west coast.&lt;|endoftext|&gt;"</code>], <br>we might change this to: <br>[<code class="language-plaintext highlighter-rouge">"I went to the yard.&lt;|endoftext|&gt;You came here"</code>, <code class="language-plaintext highlighter-rouge">"a long time ago from the west coast.&lt;|endoftext|&gt;"</code>]</p> <h4 id="finetuning-and-setup-trainer">Finetuning and setup <a href="https://huggingface.co/docs/transformers/v4.21.3/en/main_classes/trainer#transformers.Trainer" rel="external nofollow noopener" target="_blank">Trainer</a> </h4> <p>Code sample:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">distilgpt2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">small_train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">small_eval_dataset</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
</code></pre></div></div> <h4 id="model-evaluation">Model Evaluation</h4> <blockquote> <p>Because we want our model to assign high probabilities to sentences that are real, and low probabilities to fake sentences, we seek a model that assigns the highest probability to the test set. The metric we use is <a href="##" title="Investigate this more">‘perplexity’</a>, which we can think of as the inverse probability of the test set normalized by the number of words in the test set. Therefore, a lower perplexity is better.</p> </blockquote> <h4 id="generation-with-our-fine-tuned-model">Generation with our fine-tuned model</h4> <p>Code sample:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rajpurkar/</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rajpurkar/</span><span class="si">{</span><span class="n">model_checkpoint</span><span class="si">}</span><span class="s">-squad</span><span class="sh">"</span><span class="p">)</span>
<span class="n">start_text</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">A speedrun is a playthrough of a video game, </span><span class="se">\
</span><span class="s">or section of a video game, with the goal of </span><span class="se">\
</span><span class="s">completing it as fast as possible. Speedruns </span><span class="se">\
</span><span class="s">often follow planned routes, which may incorporate sequence </span><span class="se">\
</span><span class="s">breaking, and might exploit glitches that allow sections to </span><span class="se">\
</span><span class="s">be skipped or completed more quickly than intended. </span><span class="sh">"</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is the</span><span class="sh">"</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
     <span class="n">start_text</span> <span class="o">+</span> <span class="n">prompt</span><span class="p">,</span>
     <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
     <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>

<span class="n">prompt_length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
     <span class="n">inputs</span><span class="p">,</span>
     <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
     <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
     <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
     <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
     <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
     <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">prompt_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>

<span class="nf">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div> <p>Results:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2025/07/harvard-cs197-lec4-result-480.webp 480w,/assets/img/blog/2025/07/harvard-cs197-lec4-result-800.webp 800w,/assets/img/blog/2025/07/harvard-cs197-lec4-result-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/2025/07/harvard-cs197-lec4-result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </details> <hr> <h2 id="chapter-5-fine-tuning-a-vision-transformer-using-lightning"><a href="https://docs.google.com/document/d/1VnNYGEmVgvl5p8w2xzypGySajaRv6qvzqw7E7LEwQKI/edit?tab=t.0" rel="external nofollow noopener" target="_blank">Chapter 5: Fine-tuning a Vision Transformer using Lightning</a></h2> <hr> <h2 id="chapter-6--7-solidifying-pytorch-fundamentals"><a href="https://docs.google.com/document/d/1dA8KmOTZePMRl3MixxM6Fb0H8IJhIyn_g-LUXbRHeqU/edit?tab=t.0" rel="external nofollow noopener" target="_blank">Chapter 6 &amp; 7: Solidifying PyTorch Fundamentals</a></h2> <hr> <h2 id="chapter-8--9-organizing-model-training-with-weights--biases-and-hydra"><a href="https://docs.google.com/document/d/1kZCrACh8wHFFAinscHpbaHqMBKeErjOgXMVqKSUZIMU/edit?tab=t.0" rel="external nofollow noopener" target="_blank">Chapter 8 &amp; 9: Organizing Model Training with Weights &amp; Biases and Hydra</a></h2> <hr> <h2 id="chapter-10--11-a-framework-for-generating-research-ideas">Chapter 10 &amp; 11: A Framework for Generating Research Ideas</h2> <hr> <h2 id="chapter-12--13-structuring-a-research-paper">Chapter 12 &amp; 13: Structuring a Research Paper</h2> <hr> <h2 id="chapter-14--15-aws-ec2-for-deep-learning-setup-optimization-and-hands-on-training-with-chexzero">Chapter 14 &amp; 15: AWS EC2 for Deep Learning: Setup, Optimization, and Hands-on Training with CheXzero</h2> <hr> <h2 id="chapter-16--17-fine-tuning-your-stable-diffusion-model">Chapter 16 &amp; 17: Fine-Tuning Your Stable Diffusion Model</h2> <hr> <h2 id="chapter-18-tips-to-manage-your-time-and-efforts">Chapter 18: Tips to Manage Your Time and Efforts</h2> <hr> <h2 id="chapter-19-making-progress-and-impact-in-ai-research">Chapter 19: Making Progress and Impact in AI Research</h2> <hr> <h2 id="chapter-20-tips-for-creating-high-quality-slides">Chapter 20: Tips for Creating High-Quality Slides</h2> <hr> <h2 id="chapter-21-statistical-testing-to-compare-model-performances">Chapter 21: Statistical Testing to Compare Model Performances</h2> <hr> <h2 id="exercises">Exercises:</h2> <h3 id="assignments">Assignments:</h3> <p>Assignment 1: The Language of Code <br> Assignment 2: First Dive in AI <br> Assignment 3: Torched <br> Assignment 4: Spark Joy <br> Assignment 5: Ideation and Organization <br> Assignment 6: Stable Diffusion and Research Operations</p> <hr> <h3 id="course-project">Course Project:</h3> <p>[Redacted]</p> <hr> <h2 id="references">References:</h2> <p><br> <a href="https://www.cs197.seas.harvard.edu/" rel="external nofollow noopener" target="_blank">Course Homepage</a> <br> <a href="https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit?tab=t.0#heading=h.act903jwq03w" rel="external nofollow noopener" target="_blank">Docs Content</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/vi/blog/2025/awesome-learning-path/">Learning Resources</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/vi/blog/2025/downfall/">Trượt dốc đừng xuống hố.</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'sabertoaster/sabertoaster.github.io',
        'data-repo-id': 'R_kgDONJ-QGg',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDONJ-QGs4CnyrA',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'top',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Huy Mai. al-folio theme </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>