<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hopfield Network as Wordle Solver | Huy Mai </title> <meta name="author" content="Huy Mai"> <meta name="description" content="Biologically Plausible Associative Memory for Recalling Missing Word Patterns."> <meta name="keywords" content="deep-learning, artificial-intelligence, neuroscience"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%91%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sabertoaster.github.io/projects/hopfieldwordle/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Huy</span> Mai </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Works <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Hopfield Network as Wordle Solver</h1> <p class="post-description">Biologically Plausible Associative Memory for Recalling Missing Word Patterns.</p> </header> <article> <h2 id="motivation">Motivation</h2> <p>What happens when you encounter a partial word like “d__ty” in your mind? Your brain doesn’t randomly guess—it systematically explores possibilities like “deity,” “dusty,” “dirty,” each triggered by spreading activation through your memory network. This cognitive process inspired me to explore whether <strong>modern Hopfield networks</strong> could model human-like word completion.</p> <p>While existing Wordle solvers achieve 95%+ success rates using information theory or reinforcement learning, they lack <strong>biological plausibility</strong>. Can we build a system that solves Wordle the way humans might—through <strong>associative memory retrieval</strong> rather than exhaustive optimization?</p> <hr> <h2 id="background">Background</h2> <h3 id="1-wordle-and-information-theory">1. Wordle and Information Theory.</h3> <p>Skip to <a href="#2-brief-background-on-classical-hopfield-networks">Hopfield part</a> if you already know what is a Wordle and Information Theory application for it.</p> <blockquote> <p>“Wordle is a web-based word game created and developed by the Welsh software engineer <a href="https://en.wikipedia.org/wiki/Josh_Wardle" rel="external nofollow noopener" target="_blank">Josh Wardle</a>. <mark>In the game, players have six attempts to guess a five-letter word, receiving feedback through colored tiles that indicate correct letters and their placement.</mark> A single puzzle is released daily, with all players attempting to solve the same word. It was inspired by word games like <a href="https://en.wikipedia.org/wiki/Jotto" rel="external nofollow noopener" target="_blank">Jotto</a> and the game show <a href="https://en.wikipedia.org/wiki/Lingo_(American_game_show)" rel="external nofollow noopener" target="_blank">Lingo</a>. Bought and hosted by <a href="https://www.nytimes.com" rel="external nofollow noopener" target="_blank">The NY Times</a>.” <br>- From Wikipedia.</p> </blockquote> <div class="caption"> A summary of Wordle mechanism. Few exceptions like double word rule can be found <a href="https://www.reddit.com/r/wordle/comments/ry49ne/illustration_of_what_happens_when_your_guess_has/" rel="external nofollow noopener" target="_blank">here</a>. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/hopfieldwordle/wordle_game-480.webp 480w,/assets/img/projects/hopfieldwordle/wordle_game-800.webp 800w,/assets/img/projects/hopfieldwordle/wordle_game-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/hopfieldwordle/wordle_game.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><br> <br> Information Theory - How it’s used in solving Wordle: (<a href="https://www.youtube.com/watch?v=v68zYyaEmEA" rel="external nofollow noopener" target="_blank">3blue1brown explanation</a>)</p> <p>Briefly, the optimal Wordle strategy maximizes <strong>information gain</strong> per guess. Each guess partitions the solution space, with entropy measuring uncertainty:</p> \[E = -\sum_{i} p_i \log_2 p_i\] <p><strong>Example:</strong> Starting with “SALET” provides ~5.9 bits of information, reducing 2,315 possible answers to ~60 on average. However, this approach requires:</p> <ul> <li>Perfect knowledge of word frequencies</li> <li>Exhaustive similarity computations</li> <li>No memory or attention limitations</li> </ul> <p><strong>The Cognitive Gap:</strong> Humans don’t optimize information theory—we use <strong>bounded memory retrieval</strong> and <strong>associative reasoning</strong>.</p> <h3 id="2-brief-background-on-classical-hopfield-networks">2. Brief background on Classical Hopfield Networks</h3> <p><strong>Associative memories</strong> are one of the earliest artificial neural models dating back to the 1960s and 1970s. Best known are Hopfield Networks, presented by John Hopfield in 1982. As the name suggests, the main purpose of associative memory networks is to associate an input with its most similar pattern. <mark>In other words, the purpose is to store and retrieve patterns</mark>.</p> <p>Think of a Hopfield Network as a <strong>magical photo album</strong> where showing a <strong>torn photograph</strong> automatically reconstructs the <strong>complete original image</strong>. This is exactly what these networks do—they’re <strong>associative memories</strong> that can recall complete patterns from partial or noisy inputs. In our case, they can complete “D__TY” → “DEITY” by associative recall rather than exhaustive search.</p> <div class="caption"> Classical Hopfield Network: Fully connected neurons with symmetric weights </div> <div class="row mt-3 justify-content-center"> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/hopfieldwordle/hopfield_net-480.webp 480w,/assets/img/projects/hopfieldwordle/hopfield_net-800.webp 800w,/assets/img/projects/hopfieldwordle/hopfield_net-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/hopfieldwordle/hopfield_net.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The components typically includes:</p> <ul> <li> <strong>Symmetric weights</strong> which can be represented as an undirected graph (the connections from neuron i to neuron j is the same weight vice versa)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>: \(W_{ij} = W_{ji}\)</li> <li> <strong>Binary neuron states</strong>: 0/1 (binary), -1/+1 (bipolar)</li> <li> <strong>No self-connections</strong>: \(W_{ii} = 0\) (neurons don’t connect to themselves)</li> </ul> <p><strong>Example:</strong> To store the word “DEITY”, we encode it as a bipolar vector where each position represents a letter:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"DEITY" → [+1, +1, +1, +1, +1] (simplified 5-dimensional representation)
</code></pre></div></div> <p>The network creates connections between neurons such that this pattern becomes a <strong>stable memory state</strong>.</p> <h4 id="mathematical-formulation-for-word-completion">Mathematical formulation for Word Completion</h4> <blockquote> <p><strong>Given:</strong> N word patterns to recall: \(\{x_i\}_{i=1}^N\) (our vocabulary)</p> <p><strong>State:</strong> Current word configuration: \(\xi \in \{-1,+1\}^d\)</p> <p><strong>Energy function:</strong> \(E = -\frac{1}{2}\sum_i \sum_j W_{ij}\xi_i\xi_j + \sum_i b_i \xi_i\)</p> <p>where \(b_i\) are bias terms (thresholds for each neuron).</p> </blockquote> <p><strong>Key Insight:</strong> Each polar/binary patterns are present on a energy landscape which is given by the weights. The model works by creating transition from one local state to the next. The weights ensure that there are clear wells corresponding to learned patterns which attract surrounding patterns.</p> \[\boxed{\text{Partial word input} \rightarrow \text{Energy minimization} \rightarrow \text{Complete word retrieval}}\] <h4 id="the-functions-of-the-model">The functions of the model:</h4> <p><strong>Learning the patterns:</strong></p> <ol> <li> <p><strong>Direct weight setting (Hebbian Learning):</strong></p> \[W = \sum_{i=1}^N x_{i} \cdot x_{i}^T\] <ul> <li> <strong>Pros:</strong> Simple, biologically inspired, fast storage</li> <li> <strong>Cons:</strong> Limited capacity (~0.14N patterns), interference between similar words</li> <li> <strong>Example:</strong> Storing [“DEITY”, “DUSTY”] might create spurious states like “DAATY”</li> </ul> </li> <li> <p><strong>Gradient-based learning (Modern approach):</strong> Uses backpropagation to optimize storage capacity and reduce interference.</p> <ul> <li> <strong>Pros:</strong> Higher capacity, fewer spurious states, better retrieval</li> <li> <strong>Cons:</strong> More computationally expensive, requires training data</li> <li> <strong>Example:</strong> Can cleanly separate similar words like “DUSTY” vs “DIRTY”</li> </ul> </li> </ol> <p><strong>Retrieving patterns (Update rules):</strong></p> <ol> <li> <p><strong>Asynchronous updates (Classical):</strong></p> \[\xi_{i}^{(new)} = \text{sign}\Big(\sum_j W_{ij} \xi_j - b \Big) \text{(one neuron at a time)}\] <ul> <li> <strong>Pros:</strong> Guaranteed convergence, biologically realistic<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> </li> <li> <strong>Cons:</strong> Slower convergence, order-dependent results</li> <li> <strong>Use case:</strong> When you want guaranteed stability for word completion</li> </ul> </li> <li> <p><strong>Synchronous updates (Parallel):</strong></p> \[\xi^{(new)} = \text{sign} \big( W \cdot \xi \big) \text{(all neurons simultaneously)}\] <ul> <li> <strong>Pros:</strong> Faster computation due to <a href="https://www.geeksforgeeks.org/python/vectorization-in-python/" rel="external nofollow noopener" target="_blank">vectorization</a>.</li> <li> <strong>Cons:</strong> Can oscillate, might not converge</li> <li> <strong>Use case:</strong> When speed matters and patterns are well-separated</li> </ul> </li> </ol> <h4 id="classical-limitations-setting-up-the-need-for-modern-hopfield">Classical Limitations (Setting up the need for Modern Hopfield)</h4> <p><strong>The fundamental problems:</strong></p> <ul> <li> <strong>Limited capacity:</strong> Can only reliably store ~0.14N patterns</li> <li> <strong>Spurious attractors:</strong> Network might converge to “DAATY” instead of “DEITY”</li> <li> <strong>Interference:</strong> Similar words like “DUSTY” and “DIRTY” confuse each other</li> </ul> <p>These limitations motivated the development of <strong>Modern Hopfield Networks</strong>, which solve these issues while preserving the elegant associative memory properties.</p> <p>Some side related topics which are very important:</p> <ul> <li> <strong>Free Energy Principle:</strong> How biological systems minimize surprise through prediction. <blockquote> <p>Imagine you’re trying to catch a ball. You make predictions about its trajectory (perception) and then take actions to move towards it (action). The FEP suggests that you’re constantly minimizing the “surprise” of catching the ball by updating your predictions and refining your movements.</p> </blockquote> </li> <li> <strong>Hebbian Learning:</strong> “Neurons that fire together, wire together” - the biological basis for associative memory. <blockquote> <p>And neurons that fire out of sync, lose their link.</p> </blockquote> </li> </ul> <h3 id="3-modern-hopfield-networks-exponential-capacity-and-continuous-states">3. Modern Hopfield Networks: Exponential Capacity and Continuous States</h3> <p>The classical Hopfield networks suffer from fundamental limitations: spurious attractors, limited storage capacity (~0.14N), and poor retrieval for similar patterns. Modern Hopfield Networks, introduced by Krotov &amp; Hopfield (2016) and generalized by Ramsauer et al. (2020), address these issues through <strong>continuous states</strong> and <strong>exponential interaction functions</strong>.</p> <h4 id="energy-function-with-interaction-functions">Energy Function with Interaction Functions</h4> <p>The key innovation lies in replacing the quadratic energy function with more general <strong>interaction functions</strong>. The modern energy function is:</p> \[E = -\sum_{i=1}^N F\left(\frac{1}{d}(\mathbf{x}_i)^T \boldsymbol{\xi}\right)\] <p>where:</p> <ul> <li>\(\mathbf{x}_i \in \mathbb{R}^d\) are the stored patterns</li> <li>\(\boldsymbol{\xi} \in \mathbb{R}^d\) is the current state</li> <li>\(F: \mathbb{R} \rightarrow \mathbb{R}\) is the <strong>interaction function</strong> </li> <li>\(d\) is the pattern dimension</li> </ul> <p><strong>Common Interaction Functions:</strong></p> <ol> <li> <strong>Exponential</strong>: \(F(x) = \exp(x)\) <ul> <li>Provides exponential storage capacity</li> <li>Most commonly used in practice</li> </ul> </li> <li> <strong>Polynomial</strong>: \(F(x) = x^n\) for \(n \geq 2\) <ul> <li>Polynomial storage capacity scaling</li> <li>Computational simplicity</li> </ul> </li> <li> <strong>Custom functions</strong>: Any concave function satisfying convergence conditions</li> </ol> <p>The <strong>exponential interaction function</strong> is particularly powerful because it creates a <strong>log-sum-exp</strong> energy landscape that naturally separates stored patterns.</p> <h4 id="storage-capacity">Storage Capacity</h4> <p><strong>Classical Hopfield</strong>: Capacity scales as \(C_\text{classical} \sim 0.14N\) patterns.</p> <p><strong>Modern Hopfield</strong>: With exponential interaction functions, capacity scales <strong>exponentially</strong> with dimension: \(C_{modern} \sim \exp(\alpha d)\)</p> <p>where \(\alpha\) depends on the specific interaction function and pattern distribution.</p> <p><strong>Practical Implication</strong>: A 130-dimensional Modern Hopfield Network (5×26 for our word encoding) can store thousands of patterns without significant interference, compared to ~18 patterns for classical networks.</p> <h4 id="the-beta-parameter">The Beta Parameter</h4> <p>The <strong>inverse temperature</strong> parameter \(\beta\) controls the <strong>sharpness</strong> of the energy landscape:</p> \[E_{\beta} = -\frac{1}{\beta} \log \sum_{i=1}^N \exp\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)\] <p><strong>Effects of \(\beta\):</strong></p> <ul> <li> <strong>\(\beta \rightarrow 0\)</strong>: Uniform distribution over all patterns (high temperature)</li> <li> <strong>\(\beta \rightarrow \infty\)</strong>: Sharp focus on most similar pattern (low temperature)</li> <li> <strong>\(\beta = 1\)</strong>: Balanced retrieval dynamics</li> </ul> <p>For word completion, higher \(\beta\) values (\(\beta \geq 2\)) provide more decisive pattern completion.</p> <h4 id="update-rule-derivation">Update Rule Derivation</h4> <p>The update rule emerges from <strong>energy minimization</strong> using the concave-convex procedure. Starting with the energy:</p> \[E = -\frac{1}{\beta} \log \sum_{i=1}^N \exp\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)\] <p><strong>Step 1</strong>: Apply concave-convex decomposition to handle the log-sum-exp structure.</p> <p><strong>Step 2</strong>: The gradient with respect to \(\boldsymbol{\xi}\) yields: \(\frac{\partial E}{\partial \boldsymbol{\xi}} = -\frac{1}{d} \sum_{i=1}^N \frac{\exp\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)}{\sum_{j=1}^N \exp\left(\beta \frac{(\mathbf{x}^{(j)})^T \boldsymbol{\xi}}{d}\right)} \mathbf{x}_i\)</p> <p><strong>Step 3</strong>: Recognizing the softmax structure: \(\boldsymbol{\xi}^{new} = \sum_{i=1}^N \text{softmax}\left(\beta \frac{(\mathbf{x}_i)^T \boldsymbol{\xi}}{d}\right)_i \mathbf{x}_i\)</p> <p><strong>Step 4</strong>: In matrix notation, with \(\mathbf{X} = [\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(N)}]\):</p> \[\boxed{\boldsymbol{\xi}^{new} = \mathbf{X} \cdot \text{softmax}\left(\beta \mathbf{X}^T \boldsymbol{\xi}\right)}\] <h4 id="connection-to-transformer-self-attention">Connection to Transformer Self-Attention</h4> <p>The update rule is <strong>mathematically equivalent</strong> to the self-attention mechanism in transformer networks:</p> <p><strong>Hopfield Update:</strong> \(\boldsymbol{\xi}^{new} = \mathbf{X} \cdot \text{softmax}(\beta \mathbf{X}^T \boldsymbol{\xi})\)</p> <p><strong>Self-Attention:</strong> \(\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \mathbf{V} \cdot \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\)</p> <p><strong>Correspondence:</strong></p> <ul> <li>\(\mathbf{V} = \mathbf{K} = \mathbf{X}\) (stored patterns)</li> <li>\(\mathbf{Q} = \boldsymbol{\xi}^T\) (query state)</li> <li>\(\beta = 1/\sqrt{d_k}\) (temperature scaling)</li> </ul> <p>This equivalence reveals that <strong>attention mechanisms are performing associative memory retrieval</strong> in the continuous state space.</p> <h2 id="what-did-i-do-met-qua-mai-chay-thi-nghiem-gio-di-ngu-cai">What did I do (met qua mai chay thi nghiem gio di ngu cai)?</h2> <h3 id="techniques">Techniques:</h3> <ul> <li>Onehot encoding</li> <li>Feedback loop</li> </ul> <h3 id="emergent-problems">Emergent Problems:</h3> <ul> <li>Sub-optimal energy minimal.</li> <li>Temporary suppress mechanism for newly recalled words.</li> </ul> <h2 id="conclusion">Conclusion</h2> <h3 id="references">References</h3> <h3 id="disclaimer">Disclaimer</h3> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>This is proved to be inaccurate to depict how neurons works irl, example when you sit down/ stand up, there’ll be 2 seperate pathways of neurons to strengthen. For further reading, please consider <a href="https://www.youtube.com/watch?v=Ay3_D7VgzZs" rel="external nofollow noopener" target="_blank">a explanation video of Artem Kirsanov on “Brain’s Hidden Learning Limits”</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>The fact that neurons don’t wait for other neurons to fire at the same time as they’re constantly firing and transmiting information in realtime. The scope of temporal encoding unfortunately isn’t implemented in this project tho. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Huy Mai. al-folio theme </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-works",title:"Works",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-downfall-don-39-t-fall-down",title:"Downfall don't fall down.",description:"My worldbuilding experience.",section:"Posts",handler:()=>{window.location.href="/blog/2025/downfall/"}},{id:"post-downfall",title:"Downfall",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/downfall/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-known-item-search-for-video-retrieval",title:"Known Item Search for Video Retrieval",description:"A text-to-video retrieval system developed for HCMC AI Challenge 2024",section:"Projects",handler:()=>{window.location.href="/projects/aichallenge24/"}},{id:"projects-heineken-beer-detection",title:"Heineken Beer Detection",description:"(AngelHack 2024) An AI-powered system that analyzes images to detect Heineken products, count customers, and extract marketing insights",section:"Projects",handler:()=>{window.location.href="/projects/angelhack24/"}},{id:"projects-hopfield-network-as-wordle-solver",title:"Hopfield Network as Wordle Solver",description:"Biologically Plausible Associative Memory for Recalling Missing Word Patterns.",section:"Projects",handler:()=>{window.location.href="/projects/hopfieldwordle/"}},{id:"projects-mazegame",title:"MazeGame",description:"(CSC10010) A Python-based maze game featuring multiple pathfinding algorithms with interactive visualization",section:"Projects",handler:()=>{window.location.href="/projects/mazegame/"}},{id:"projects-llm-based-document-similarity-detection",title:"LLM-Based Document Similarity Detection",description:"(EVN) Leveraging PhoBERT and Longformer for Vietnamese text duplicate detection",section:"Projects",handler:()=>{window.location.href="/projects/phobertEVN/"}},{id:"projects-remote-control-via-email",title:"Remote Control Via Email",description:"(CSC10008) DHCP",section:"Projects",handler:()=>{window.location.href="/projects/remotecontrolviaemail/"}},{id:"projects-flower-classification",title:"Flower Classification",description:"(Fellowship.AI) Fine-tuning ResNet50 with contrastive learning for the 102 Category Flower Dataset",section:"Projects",handler:()=>{window.location.href="/projects/resnet50fellowship/"}},{id:"projects-tictactoe",title:"TicTacToe",description:"(CSC10001) A feature-rich console-based game with AI algorithms and animations",section:"Projects",handler:()=>{window.location.href="/projects/tictactoe/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%69%6E%68%68%75%79%6D%61%69%64%75%63@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0005-2711-5320","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/sabertoaster","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/sabertoaster","_blank")}},{id:"socials-discord",title:"Discord",section:"Socials",handler:()=>{window.open("https://discord.com/users/483486384510861312","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>